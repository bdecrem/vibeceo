{
  "fetch_date": "2025-10-21 11:01:31 UTC",
  "target_date": "2025-10-20",
  "total_papers": 270,
  "categories": [
    "cs.AI",
    "cs.LG",
    "cs.CV",
    "cs.CL",
    "stat.ML"
  ],
  "papers": [
    {
      "arxiv_id": "2510.17803v1",
      "title": "ConsistEdit: Highly Consistent and Precise Training-free Visual Editing",
      "abstract": "Recent advances in training-free attention control methods have enabled\nflexible and efficient text-guided editing capabilities for existing generation\nmodels. However, current approaches struggle to simultaneously deliver strong\nediting strength while preserving consistency with the source. This limitation\nbecomes particularly critical in multi-round and video editing, where visual\nerrors can accumulate over time. Moreover, most existing methods enforce global\nconsistency, which limits their ability to modify individual attributes such as\ntexture while preserving others, thereby hindering fine-grained editing.\nRecently, the architectural shift from U-Net to MM-DiT has brought significant\nimprovements in generative performance and introduced a novel mechanism for\nintegrating text and vision modalities. These advancements pave the way for\novercoming challenges that previous methods failed to resolve. Through an\nin-depth analysis of MM-DiT, we identify three key insights into its attention\nmechanisms. Building on these, we propose ConsistEdit, a novel attention\ncontrol method specifically tailored for MM-DiT. ConsistEdit incorporates\nvision-only attention control, mask-guided pre-attention fusion, and\ndifferentiated manipulation of the query, key, and value tokens to produce\nconsistent, prompt-aligned edits. Extensive experiments demonstrate that\nConsistEdit achieves state-of-the-art performance across a wide range of image\nand video editing tasks, including both structure-consistent and\nstructure-inconsistent scenarios. Unlike prior methods, it is the first\napproach to perform editing across all inference steps and attention layers\nwithout handcraft, significantly enhancing reliability and consistency, which\nenables robust multi-round and multi-region editing. Furthermore, it supports\nprogressive adjustment of structural consistency, enabling finer control.",
      "authors": [
        {
          "name": "Zixin Yin",
          "affiliation": null
        },
        {
          "name": "Ling-Hao Chen",
          "affiliation": null
        },
        {
          "name": "Lionel Ni",
          "affiliation": null
        },
        {
          "name": "Xili Dai",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17803v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17803v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17802v1",
      "title": "Unbiased Gradient Low-Rank Projection",
      "abstract": "Memory-efficient optimization is critical for training increasingly large\nlanguage models (LLMs). A popular strategy involves gradient low-rank\nprojection, storing only the projected optimizer states, with GaLore being a\nrepresentative example. However, a significant drawback of many such methods is\ntheir lack of convergence guarantees, as various low-rank projection approaches\nintroduce inherent biases relative to the original optimization algorithms,\nwhich contribute to performance gaps compared to full-parameter training.\nAiming to tackle this problem, this paper investigates the layerwise sampling\ntechnique for debiasing low-rank projection mechanisms. In particular, an\ninstantiation of the paradigm gives rise to a novel and unbiased low-rank\noptimization method built upon GaLore's mechanism and the Muon algorithm, named\nGaLore Unbiased with Muon (GUM). We theoretically prove our method matches the\nconvergence guarantees of the base Muon algorithm while preserving the memory\nefficiency of low-rank techniques. Empirical experiments on LLM fine-tuning and\npretraining also demonstrate non-trivial improvements over GaLore and even\nbetter performance than full-parameter training. Further investigation shows\nthat the improvement of this technique comes from a more uniform distribution\nof knowledge inside layers, leading to more efficient utilization of the model\nparameter space and better memorization.",
      "authors": [
        {
          "name": "Rui Pan",
          "affiliation": null
        },
        {
          "name": "Yang Luo",
          "affiliation": null
        },
        {
          "name": "Yuxing Liu",
          "affiliation": null
        },
        {
          "name": "Yang You",
          "affiliation": null
        },
        {
          "name": "Tong Zhang",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17802v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17802v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17800v1",
      "title": "Glyph: Scaling Context Windows via Visual-Text Compression",
      "abstract": "Large language models (LLMs) increasingly rely on long-context modeling for\ntasks such as document understanding, code analysis, and multi-step reasoning.\nHowever, scaling context windows to the million-token level brings prohibitive\ncomputational and memory costs, limiting the practicality of long-context LLMs.\nIn this work, we take a different perspective-visual context scaling-to tackle\nthis challenge. Instead of extending token-based sequences, we propose Glyph, a\nframework that renders long texts into images and processes them with\nvision-language models (VLMs). This approach substantially compresses textual\ninput while preserving semantic information, and we further design an\nLLM-driven genetic search to identify optimal visual rendering configurations\nfor balancing accuracy and compression. Through extensive experiments, we\ndemonstrate that our method achieves 3-4x token compression while maintaining\naccuracy comparable to leading LLMs such as Qwen3-8B on various long-context\nbenchmarks. This compression also leads to around 4x faster prefilling and\ndecoding, and approximately 2x faster SFT training. Furthermore, under extreme\ncompression, a 128K-context VLM could scale to handle 1M-token-level text\ntasks. In addition, the rendered text data benefits real-world multimodal\ntasks, such as document understanding. Our code and model are released at\nhttps://github.com/thu-coai/Glyph.",
      "authors": [
        {
          "name": "Jiale Cheng",
          "affiliation": null
        },
        {
          "name": "Yusen Liu",
          "affiliation": null
        },
        {
          "name": "Xinyu Zhang",
          "affiliation": null
        },
        {
          "name": "Yulin Fei",
          "affiliation": null
        },
        {
          "name": "Wenyi Hong",
          "affiliation": null
        },
        {
          "name": "Ruiliang Lyu",
          "affiliation": null
        },
        {
          "name": "Weihan Wang",
          "affiliation": null
        },
        {
          "name": "Zhe Su",
          "affiliation": null
        },
        {
          "name": "Xiaotao Gu",
          "affiliation": null
        },
        {
          "name": "Xiao Liu",
          "affiliation": null
        },
        {
          "name": "Yushi Bai",
          "affiliation": null
        },
        {
          "name": "Jie Tang",
          "affiliation": null
        },
        {
          "name": "Hongning Wang",
          "affiliation": null
        },
        {
          "name": "Minlie Huang",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "cs.CL",
        "cs.LG"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17800v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17800v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17797v1",
      "title": "Enterprise Deep Research: Steerable Multi-Agent Deep Research for Enterprise Analytics",
      "abstract": "As information grows exponentially, enterprises face increasing pressure to\ntransform unstructured data into coherent, actionable insights. While\nautonomous agents show promise, they often struggle with domain-specific\nnuances, intent alignment, and enterprise integration. We present Enterprise\nDeep Research (EDR), a multi-agent system that integrates (1) a Master Planning\nAgent for adaptive query decomposition, (2) four specialized search agents\n(General, Academic, GitHub, LinkedIn), (3) an extensible MCP-based tool\necosystem supporting NL2SQL, file analysis, and enterprise workflows, (4) a\nVisualization Agent for data-driven insights, and (5) a reflection mechanism\nthat detects knowledge gaps and updates research direction with optional\nhuman-in-the-loop steering guidance. These components enable automated report\ngeneration, real-time streaming, and seamless enterprise deployment, as\nvalidated on internal datasets. On open-ended benchmarks including DeepResearch\nBench and DeepConsult, EDR outperforms state-of-the-art agentic systems without\nany human steering. We release the EDR framework and benchmark trajectories to\nadvance research on multi-agent reasoning applications.\n  Code at https://github.com/SalesforceAIResearch/enterprise-deep-research and\nDataset at https://huggingface.co/datasets/Salesforce/EDR-200",
      "authors": [
        {
          "name": "Akshara Prabhakar",
          "affiliation": null
        },
        {
          "name": "Roshan Ram",
          "affiliation": null
        },
        {
          "name": "Zixiang Chen",
          "affiliation": null
        },
        {
          "name": "Silvio Savarese",
          "affiliation": null
        },
        {
          "name": "Frank Wang",
          "affiliation": null
        },
        {
          "name": "Caiming Xiong",
          "affiliation": null
        },
        {
          "name": "Huan Wang",
          "affiliation": null
        },
        {
          "name": "Weiran Yao",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17797v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17797v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17795v1",
      "title": "Executable Knowledge Graphs for Replicating AI Research",
      "abstract": "Replicating AI research is a crucial yet challenging task for large language\nmodel (LLM) agents. Existing approaches often struggle to generate executable\ncode, primarily due to insufficient background knowledge and the limitations of\nretrieval-augmented generation (RAG) methods, which fail to capture latent\ntechnical details hidden in referenced papers. Furthermore, previous approaches\ntend to overlook valuable implementation-level code signals and lack structured\nknowledge representations that support multi-granular retrieval and reuse. To\novercome these challenges, we propose Executable Knowledge Graphs (xKG), a\nmodular and pluggable knowledge base that automatically integrates technical\ninsights, code snippets, and domain-specific knowledge extracted from\nscientific literature. When integrated into three agent frameworks with two\ndifferent LLMs, xKG shows substantial performance gains (10.9% with o3-mini) on\nPaperBench, demonstrating its effectiveness as a general and extensible\nsolution for automated AI research replication. Code will released at\nhttps://github.com/zjunlp/xKG.",
      "authors": [
        {
          "name": "Yujie Luo",
          "affiliation": null
        },
        {
          "name": "Zhuoyun Yu",
          "affiliation": null
        },
        {
          "name": "Xuehai Wang",
          "affiliation": null
        },
        {
          "name": "Yuqi Zhu",
          "affiliation": null
        },
        {
          "name": "Ningyu Zhang",
          "affiliation": null
        },
        {
          "name": "Lanning Wei",
          "affiliation": null
        },
        {
          "name": "Lun Du",
          "affiliation": null
        },
        {
          "name": "Da Zheng",
          "affiliation": null
        },
        {
          "name": "Huajun Chen",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.MA",
        "cs.SE"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17795v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17795v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17794v1",
      "title": "Functional Distribution Networks (FDN)",
      "abstract": "Modern probabilistic regressors often remain overconfident under distribution\nshift. We present Functional Distribution Networks (FDN), an input-conditioned\ndistribution over network weights that induces predictive mixtures whose\ndispersion adapts to the input. FDN is trained with a beta-ELBO and Monte Carlo\nsampling. We further propose an evaluation protocol that cleanly separates\ninterpolation from extrapolation and stresses OOD sanity checks (e.g., that\npredictive likelihood degrades under shift while in-distribution accuracy and\ncalibration are maintained). On standard regression tasks, we benchmark against\nstrong Bayesian, ensemble, dropout, and hypernetwork baselines under matched\nparameter and update budgets, and assess accuracy, calibration, and\nshift-awareness with standard diagnostics. Together, the framework and protocol\naim to make OOD-aware, well-calibrated neural regression practical and modular.",
      "authors": [
        {
          "name": "Omer Haq",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17794v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17794v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17793v1",
      "title": "Foundational Automatic Evaluators: Scaling Multi-Task Generative Evaluator Training for Reasoning-Centric Domains",
      "abstract": "Finetuning specialized generative evaluators has emerged as a popular\nparadigm to meet the increasing demand for scalable evaluation during both\ntraining and test-time. However, recent work has largely focused on applying\nnew methodology, such as reinforcement learning (RL), to training evaluators,\nshying away from large-scale, data-driven development. In this work, we focus\non data scaling, curating a set of 2.5M samples spanning five unique evaluation\ntasks (pairwise, step-level, reference-free and reference-based verification,\nand single rating) and multiple domains focused on reasoning evaluation. With\nour data, we train Foundational Automatic Reasoning Evaluators (FARE), a family\nof 8B and 20B (with 3.6B active) parameter evaluators, with a simple iterative\nrejection-sampling supervised finetuning (SFT) approach. FARE-8B challenges\nlarger specialized RL-trained evaluators and FARE-20B sets the new standard for\nopen-source evaluators, surpassing specialized 70B+ evaluators. Beyond static\nbenchmarks, we evaluate FARE in real-world tasks: As inference-time rerankers,\nFARE-20B achieves near-oracle performance on MATH. As verifiers in RL training,\nFARE improves the downstream RL-trained model performance by up to 14.1% vs.\nstring-matching verifiers. When initialized from FARE, a continually-finetuned\nFARE-Code outperforms gpt-oss-20B by 65% on evaluating test-case quality.",
      "authors": [
        {
          "name": "Austin Xu",
          "affiliation": null
        },
        {
          "name": "Xuan-Phi Nguyen",
          "affiliation": null
        },
        {
          "name": "Yilun Zhou",
          "affiliation": null
        },
        {
          "name": "Chien-Sheng Wu",
          "affiliation": null
        },
        {
          "name": "Caiming Xiong",
          "affiliation": null
        },
        {
          "name": "Shafiq Joty",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17793v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17793v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17792v1",
      "title": "SoftMimic: Learning Compliant Whole-body Control from Examples",
      "abstract": "We introduce SoftMimic, a framework for learning compliant whole-body control\npolicies for humanoid robots from example motions. Imitating human motions with\nreinforcement learning allows humanoids to quickly learn new skills, but\nexisting methods incentivize stiff control that aggressively corrects\ndeviations from a reference motion, leading to brittle and unsafe behavior when\nthe robot encounters unexpected contacts. In contrast, SoftMimic enables robots\nto respond compliantly to external forces while maintaining balance and\nposture. Our approach leverages an inverse kinematics solver to generate an\naugmented dataset of feasible compliant motions, which we use to train a\nreinforcement learning policy. By rewarding the policy for matching compliant\nresponses rather than rigidly tracking the reference motion, SoftMimic learns\nto absorb disturbances and generalize to varied tasks from a single motion\nclip. We validate our method through simulations and real-world experiments,\ndemonstrating safe and effective interaction with the environment.",
      "authors": [
        {
          "name": "Gabriel B. Margolis",
          "affiliation": null
        },
        {
          "name": "Michelle Wang",
          "affiliation": null
        },
        {
          "name": "Nolan Fey",
          "affiliation": null
        },
        {
          "name": "Pulkit Agrawal",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17792v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17792v1",
      "primary_category": "cs.RO",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17790v1",
      "title": "UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action",
      "abstract": "Multimodal agents for computer use rely exclusively on primitive actions\n(click, type, scroll) that require accurate visual grounding and lengthy\nexecution chains, leading to cascading failures and performance bottlenecks.\nWhile other agents leverage rich programmatic interfaces (APIs, MCP servers,\ntools), computer-use agents (CUAs) remain isolated from these capabilities. We\npresent UltraCUA, a foundation model that bridges this gap through hybrid\naction -- seamlessly integrating GUI primitives with high-level programmatic\ntool calls. To achieve this, our approach comprises four key components: (1) an\nautomated pipeline that scales programmatic tools from software documentation,\nopen-source repositories, and code generation; (2) a synthetic data engine\nproducing over 17,000 verifiable tasks spanning real-world computer-use\nscenarios; (3) a large-scale high-quality hybrid action trajectory collection\nwith both low-level GUI actions and high-level programmatic tool calls; and (4)\na two-stage training pipeline combining supervised fine-tuning with online\nreinforcement learning, enabling strategic alternation between low-level and\nhigh-level actions. Experiments with our 7B and 32B models demonstrate\nsubstantial improvements over state-of-the-art agents. On OSWorld, UltraCUA\nmodels achieve an average 22% relative improvement over base models, while\nbeing 11% faster in terms of steps. Out-of-domain evaluation on\nWindowsAgentArena shows our model reaches 21.7% success rate, outperforming\nbaselines trained on Windows data. The hybrid action mechanism proves critical,\nreducing error propagation while maintaining execution efficiency.",
      "authors": [
        {
          "name": "Yuhao Yang",
          "affiliation": null
        },
        {
          "name": "Zhen Yang",
          "affiliation": null
        },
        {
          "name": "Zi-Yi Dou",
          "affiliation": null
        },
        {
          "name": "Anh Nguyen",
          "affiliation": null
        },
        {
          "name": "Keen You",
          "affiliation": null
        },
        {
          "name": "Omar Attia",
          "affiliation": null
        },
        {
          "name": "Andrew Szot",
          "affiliation": null
        },
        {
          "name": "Michael Feng",
          "affiliation": null
        },
        {
          "name": "Ram Ramrakhya",
          "affiliation": null
        },
        {
          "name": "Alexander Toshev",
          "affiliation": null
        },
        {
          "name": "Chao Huang",
          "affiliation": null
        },
        {
          "name": "Yinfei Yang",
          "affiliation": null
        },
        {
          "name": "Zhe Gan",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "cs.CL"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17790v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17790v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17786v1",
      "title": "Inference-Time Compute Scaling For Flow Matching",
      "abstract": "Allocating extra computation at inference time has recently improved sample\nquality in large language models and diffusion-based image generation. In\nparallel, Flow Matching (FM) has gained traction in language, vision, and\nscientific domains, but inference-time scaling methods for it remain\nunder-explored. Concurrently, Kim et al., 2025 approach this problem but\nreplace the linear interpolant with a non-linear variance-preserving (VP)\ninterpolant at inference, sacrificing FM's efficient and straight sampling.\nAdditionally, inference-time compute scaling for flow matching has only been\napplied to visual tasks, like image generation. We introduce novel\ninference-time scaling procedures for FM that preserve the linear interpolant\nduring sampling. Evaluations of our method on image generation, and for the\nfirst time (to the best of our knowledge), unconditional protein generation,\nshow that I) sample quality consistently improves as inference compute\nincreases, and II) flow matching inference-time scaling can be applied to\nscientific domains.",
      "authors": [
        {
          "name": "Adam Stecklov",
          "affiliation": null
        },
        {
          "name": "Noah El Rimawi-Fine",
          "affiliation": null
        },
        {
          "name": "Mathieu Blanchette",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17786v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17786v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17783v1",
      "title": "Botany-Bot: Digital Twin Monitoring of Occluded and Underleaf Plant Structures with Gaussian Splats",
      "abstract": "Commercial plant phenotyping systems using fixed cameras cannot perceive many\nplant details due to leaf occlusion. In this paper, we present Botany-Bot, a\nsystem for building detailed \"annotated digital twins\" of living plants using\ntwo stereo cameras, a digital turntable inside a lightbox, an industrial robot\narm, and 3D segmentated Gaussian Splat models. We also present robot algorithms\nfor manipulating leaves to take high-resolution indexable images of occluded\ndetails such as stem buds and the underside/topside of leaves. Results from\nexperiments suggest that Botany-Bot can segment leaves with 90.8% accuracy,\ndetect leaves with 86.2% accuracy, lift/push leaves with 77.9% accuracy, and\ntake detailed overside/underside images with 77.3% accuracy. Code, videos, and\ndatasets are available at https://berkeleyautomation.github.io/Botany-Bot/.",
      "authors": [
        {
          "name": "Simeon Adebola",
          "affiliation": null
        },
        {
          "name": "Chung Min Kim",
          "affiliation": null
        },
        {
          "name": "Justin Kerr",
          "affiliation": null
        },
        {
          "name": "Shuangyu Xie",
          "affiliation": null
        },
        {
          "name": "Prithvi Akella",
          "affiliation": null
        },
        {
          "name": "Jose Luis Susa Rincon",
          "affiliation": null
        },
        {
          "name": "Eugen Solowjow",
          "affiliation": null
        },
        {
          "name": "Ken Goldberg",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17783v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17783v1",
      "primary_category": "cs.RO",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17776v1",
      "title": "Mapping Post-Training Forgetting in Language Models at Scale",
      "abstract": "Scaled post-training now drives many of the largest capability gains in\nlanguage models (LMs), yet its effect on pretrained knowledge remains poorly\nunderstood. Not all forgetting is equal: Forgetting one fact (e.g., a U.S.\npresident or an API call) does not \"average out\" by recalling another. Hence,\nwe propose a sample-wise paradigm to measure what is forgotten and when\nbackward transfer occurs. Our metric counts 1->0 transitions (correct before\npost-training, incorrect after) to quantify forgetting and 0->1 transitions to\nquantify backward transfer. Traditional task averages conflate these effects\nand obscure large changes. For multiple-choice benchmarks, we add\nchance-adjusted variants that subtract the expected contribution of random\nguessing from pre- and post-training accuracies. We apply this framework across\npost-training stages, model sizes, and data scales. Our large-scale analysis\nshows that: (1) Domain-continual pretraining induces moderate forgetting with\nlow-to-moderate backward transfer; (2) RL/SFT post-training applied to base\nmodels and Instruction tuning yields moderate-to-large backward transfer on\nmath and logic with overall low-to-moderate forgetting; (3) Applying RL/SFT to\ninstruction-tuned models is sensitive on data scale: at small scales, both\nforgetting and backward transfer are small; at larger scales, effects are mixed\nand warrant further study with better controls; (4) Model merging does not\nreliably mitigate forgetting. Overall, our framework offers a practical\nyardstick for mapping how post-training alters pretrained knowledge at scale --\nenabling progress towards generally capable AI systems.",
      "authors": [
        {
          "name": "Jackson Harmon",
          "affiliation": null
        },
        {
          "name": "Andreas Hochlehnert",
          "affiliation": null
        },
        {
          "name": "Matthias Bethge",
          "affiliation": null
        },
        {
          "name": "Ameya Prabhu",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17776v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17776v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17777v1",
      "title": "SparseVILA: Decoupling Visual Sparsity for Efficient VLM Inference",
      "abstract": "Vision Language Models (VLMs) have rapidly advanced in integrating visual and\ntextual reasoning, powering applications across high-resolution image\nunderstanding, long-video analysis, and multi-turn conversation. However, their\nscalability remains limited by the growing number of visual tokens that\ndominate inference latency. We present SparseVILA, a new paradigm for efficient\nVLM inference that decouples visual sparsity across the prefilling and decoding\nstages. SparseVILA distributes sparsity across stages by pruning redundant\nvisual tokens during prefill and retrieving only query-relevant tokens during\ndecoding. This decoupled design matches leading prefill pruning methods while\npreserving multi-turn fidelity by retaining most of the visual cache so that\nquery-aware tokens can be retrieved at each conversation round. Built on an\nAWQ-optimized inference pipeline, SparseVILA achieves up to 4.0 times faster\nprefilling, 2.5 times faster decoding, and an overall 2.6 times end-to-end\nspeedup on long-context video tasks -- while improving accuracy on\ndocument-understanding and reasoning tasks. By decoupling query-agnostic\npruning and query-aware retrieval, SparseVILA establishes a new direction for\nefficient multimodal inference, offering a training-free, architecture-agnostic\nframework for accelerating large VLMs without sacrificing capability.",
      "authors": [
        {
          "name": "Samir Khaki",
          "affiliation": null
        },
        {
          "name": "Junxian Guo",
          "affiliation": null
        },
        {
          "name": "Jiaming Tang",
          "affiliation": null
        },
        {
          "name": "Shang Yang",
          "affiliation": null
        },
        {
          "name": "Yukang Chen",
          "affiliation": null
        },
        {
          "name": "Konstantinos N. Plataniotis",
          "affiliation": null
        },
        {
          "name": "Yao Lu",
          "affiliation": null
        },
        {
          "name": "Song Han",
          "affiliation": null
        },
        {
          "name": "Zhijian Liu",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17777v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17777v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17773v1",
      "title": "Towards Explainable Skin Cancer Classification: A Dual-Network Attention Model with Lesion Segmentation and Clinical Metadata Fusion",
      "abstract": "Skin cancer is a life-threatening disease where early detection significantly\nimproves patient outcomes. Automated diagnosis from dermoscopic images is\nchallenging due to high intra-class variability and subtle inter-class\ndifferences. Many deep learning models operate as \"black boxes,\" limiting\nclinical trust. In this work, we propose a dual-encoder attention-based\nframework that leverages both segmented lesions and clinical metadata to\nenhance skin lesion classification in terms of both accuracy and\ninterpretability. A novel Deep-UNet architecture with Dual Attention Gates\n(DAG) and Atrous Spatial Pyramid Pooling (ASPP) is first employed to segment\nlesions. The classification stage uses two DenseNet201 encoders-one on the\noriginal image and another on the segmented lesion whose features are fused via\nmulti-head cross-attention. This dual-input design guides the model to focus on\nsalient pathological regions. In addition, a transformer-based module\nincorporates patient metadata (age, sex, lesion site) into the prediction. We\nevaluate our approach on the HAM10000 dataset and the ISIC 2018 and 2019\nchallenges. The proposed method achieves state-of-the-art segmentation\nperformance and significantly improves classification accuracy and average AUC\ncompared to baseline models. To validate our model's reliability, we use\nGradient-weighted Class Activation Mapping (Grad-CAM) to generate heatmaps.\nThese visualizations confirm that our model's predictions are based on the\nlesion area, unlike models that rely on spurious background features. These\nresults demonstrate that integrating precise lesion segmentation and clinical\ndata with attention-based fusion leads to a more accurate and interpretable\nskin cancer classification model.",
      "authors": [
        {
          "name": "Md. Enamul Atiq",
          "affiliation": null
        },
        {
          "name": "Shaikh Anowarul Fattah",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17773v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17773v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17772v1",
      "title": "Atlas-based Manifold Representations for Interpretable Riemannian Machine Learning",
      "abstract": "Despite the popularity of the manifold hypothesis, current manifold-learning\nmethods do not support machine learning directly on the latent $d$-dimensional\ndata manifold, as they primarily aim to perform dimensionality reduction into\n$\\mathbb{R}^D$, losing key manifold features when the embedding dimension $D$\napproaches $d$.\n  On the other hand, methods that directly learn the latent manifold as a\ndifferentiable atlas have been relatively underexplored.\n  In this paper, we aim to give a proof of concept of the effectiveness and\npotential of atlas-based methods. To this end, we implement a generic data\nstructure to maintain a differentiable atlas that enables Riemannian\noptimization over the manifold. We complement this with an unsupervised\nheuristic that learns a differentiable atlas from point cloud data. We\nexperimentally demonstrate that this approach has advantages in terms of\nefficiency and accuracy in selected settings. Moreover, in a supervised\nclassification task over the Klein bottle and in RNA velocity analysis of\nhematopoietic data, we showcase the improved interpretability and robustness of\nour approach.",
      "authors": [
        {
          "name": "Ryan A. Robinett",
          "affiliation": null
        },
        {
          "name": "Sophia A. Madejski",
          "affiliation": null
        },
        {
          "name": "Kyle Ruark",
          "affiliation": null
        },
        {
          "name": "Samantha J. Riesenfeld",
          "affiliation": null
        },
        {
          "name": "Lorenzo Orecchia",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "stat.AP",
        "I.5.1"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17772v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17772v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17771v1",
      "title": "Seeing but Not Believing: Probing the Disconnect Between Visual Attention and Answer Correctness in VLMs",
      "abstract": "Vision-Language Models (VLMs) achieve strong results on multimodal tasks such\nas visual question answering, yet they can still fail even when the correct\nvisual evidence is present. In this work, we systematically investigate whether\nthese failures arise from not perceiving the evidence or from not leveraging it\neffectively. By examining layer-wise attention dynamics, we find that shallow\nlayers focus primarily on text, while deeper layers sparsely but reliably\nattend to localized evidence regions. Surprisingly, VLMs often perceive the\nvisual evidence when outputting incorrect answers, a phenomenon we term\n``seeing but not believing'' that widely exists in major VLM families. Building\non this, we introduce an inference-time intervention that highlights deep-layer\nevidence regions through selective attention-based masking. It requires no\ntraining and consistently improves accuracy across multiple families, including\nLLaVA, Qwen, Gemma, and InternVL. These results show that VLMs encode reliable\nevidence internally but under-utilize it, making such signals explicit can\nbridge the gap between perception and reasoning, advancing the diagnostic\nunderstanding and reliability of VLMs.",
      "authors": [
        {
          "name": "Zhining Liu",
          "affiliation": null
        },
        {
          "name": "Ziyi Chen",
          "affiliation": null
        },
        {
          "name": "Hui Liu",
          "affiliation": null
        },
        {
          "name": "Chen Luo",
          "affiliation": null
        },
        {
          "name": "Xianfeng Tang",
          "affiliation": null
        },
        {
          "name": "Suhang Wang",
          "affiliation": null
        },
        {
          "name": "Joy Zeng",
          "affiliation": null
        },
        {
          "name": "Zhenwei Dai",
          "affiliation": null
        },
        {
          "name": "Zhan Shi",
          "affiliation": null
        },
        {
          "name": "Tianxin Wei",
          "affiliation": null
        },
        {
          "name": "Benoit Dumoulin",
          "affiliation": null
        },
        {
          "name": "Hanghang Tong",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17771v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17771v1",
      "primary_category": "cs.AI",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17764v1",
      "title": "Evaluating Medical LLMs by Levels of Autonomy: A Survey Moving from Benchmarks to Applications",
      "abstract": "Medical Large language models achieve strong scores on standard benchmarks;\nhowever, the transfer of those results to safe and reliable performance in\nclinical workflows remains a challenge. This survey reframes evaluation through\na levels-of-autonomy lens (L0-L3), spanning informational tools, information\ntransformation and aggregation, decision support, and supervised agents. We\nalign existing benchmarks and metrics with the actions permitted at each level\nand their associated risks, making the evaluation targets explicit. This\nmotivates a level-conditioned blueprint for selecting metrics, assembling\nevidence, and reporting claims, alongside directions that link evaluation to\noversight. By centering autonomy, the survey moves the field beyond score-based\nclaims toward credible, risk-aware evidence for real clinical use.",
      "authors": [
        {
          "name": "Xiao Ye",
          "affiliation": null
        },
        {
          "name": "Jacob Dineen",
          "affiliation": null
        },
        {
          "name": "Zhaonan Li",
          "affiliation": null
        },
        {
          "name": "Zhikun Xu",
          "affiliation": null
        },
        {
          "name": "Weiyu Chen",
          "affiliation": null
        },
        {
          "name": "Shijie Lu",
          "affiliation": null
        },
        {
          "name": "Yuxi Huang",
          "affiliation": null
        },
        {
          "name": "Ming Shen",
          "affiliation": null
        },
        {
          "name": "Phu Tran",
          "affiliation": null
        },
        {
          "name": "Ji-Eun Irene Yum",
          "affiliation": null
        },
        {
          "name": "Muhammad Ali Khan",
          "affiliation": null
        },
        {
          "name": "Muhammad Umar Afzal",
          "affiliation": null
        },
        {
          "name": "Irbaz Bin Riaz",
          "affiliation": null
        },
        {
          "name": "Ben Zhou",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17764v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17764v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17759v1",
      "title": "VERA-V: Variational Inference Framework for Jailbreaking Vision-Language Models",
      "abstract": "Vision-Language Models (VLMs) extend large language models with visual\nreasoning, but their multimodal design also introduces new, underexplored\nvulnerabilities. Existing multimodal red-teaming methods largely rely on\nbrittle templates, focus on single-attack settings, and expose only a narrow\nsubset of vulnerabilities. To address these limitations, we introduce VERA-V, a\nvariational inference framework that recasts multimodal jailbreak discovery as\nlearning a joint posterior distribution over paired text-image prompts. This\nprobabilistic view enables the generation of stealthy, coupled adversarial\ninputs that bypass model guardrails. We train a lightweight attacker to\napproximate the posterior, allowing efficient sampling of diverse jailbreaks\nand providing distributional insights into vulnerabilities. VERA-V further\nintegrates three complementary strategies: (i) typography-based text prompts\nthat embed harmful cues, (ii) diffusion-based image synthesis that introduces\nadversarial signals, and (iii) structured distractors to fragment VLM\nattention. Experiments on HarmBench and HADES benchmarks show that VERA-V\nconsistently outperforms state-of-the-art baselines on both open-source and\nfrontier VLMs, achieving up to 53.75% higher attack success rate (ASR) over the\nbest baseline on GPT-4o.",
      "authors": [
        {
          "name": "Qilin Liao",
          "affiliation": null
        },
        {
          "name": "Anamika Lochab",
          "affiliation": null
        },
        {
          "name": "Ruqi Zhang",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CR",
        "cs.CL",
        "cs.CV",
        "cs.LG"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17759v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17759v1",
      "primary_category": "cs.CR",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17756v1",
      "title": "Prediction of Sea Ice Velocity and Concentration in the Arctic Ocean using Physics-informed Neural Network",
      "abstract": "As an increasing amount of remote sensing data becomes available in the\nArctic Ocean, data-driven machine learning (ML) techniques are becoming widely\nused to predict sea ice velocity (SIV) and sea ice concentration (SIC).\nHowever, fully data-driven ML models have limitations in generalizability and\nphysical consistency due to their excessive reliance on the quantity and\nquality of training data. In particular, as Arctic sea ice entered a new phase\nwith thinner ice and accelerated melting, there is a possibility that an ML\nmodel trained with historical sea ice data cannot fully represent the\ndynamically changing sea ice conditions in the future. In this study, we\ndevelop physics-informed neural network (PINN) strategies to integrate physical\nknowledge of sea ice into the ML model. Based on the Hierarchical\nInformation-sharing U-net (HIS-Unet) architecture, we incorporate the physics\nloss function and the activation function to produce physically plausible SIV\nand SIC outputs. Our PINN model outperforms the fully data-driven model in the\ndaily predictions of SIV and SIC, even when trained with a small number of\nsamples. The PINN approach particularly improves SIC predictions in melting and\nearly freezing seasons and near fast-moving ice regions.",
      "authors": [
        {
          "name": "Younghyun Koo",
          "affiliation": null
        },
        {
          "name": "Maryam Rahnemoonfar",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17756v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17756v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17753v1",
      "title": "Human-AI Interactions: Cognitive, Behavioral, and Emotional Impacts",
      "abstract": "As stories of human-AI interactions continue to be highlighted in the news\nand research platforms, the challenges are becoming more pronounced, including\npotential risks of overreliance, cognitive offloading, social and emotional\nmanipulation, and the nuanced degradation of human agency and judgment. This\npaper surveys recent research on these issues through the lens of the\npsychological triad: cognition, behavior, and emotion. Observations seem to\nsuggest that while AI can substantially enhance memory, creativity, and\nengagement, it also introduces risks such as diminished critical thinking,\nskill erosion, and increased anxiety. Emotional outcomes are similarly mixed,\nwith AI systems showing promise for support and stress reduction, but raising\nconcerns about dependency, inappropriate attachments, and ethical oversight.\nThis paper aims to underscore the need for responsible and context-aware AI\ndesign, highlighting gaps for longitudinal research and grounded evaluation\nframeworks to balance benefits with emerging human-centric risks.",
      "authors": [
        {
          "name": "Celeste Riley",
          "affiliation": null
        },
        {
          "name": "Omar Al-Refai",
          "affiliation": null
        },
        {
          "name": "Yadira Colunga Reyes",
          "affiliation": null
        },
        {
          "name": "Eman Hammad",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17753v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17753v1",
      "primary_category": "cs.HC",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17745v1",
      "title": "A Multi-Threading Kernel for Enabling Neuromorphic Edge Applications",
      "abstract": "Spiking Neural Networks (SNNs) have sparse, event driven processing that can\nleverage neuromorphic applications. In this work, we introduce a\nmulti-threading kernel that enables neuromorphic applications running at the\nedge, meaning they process sensory input directly and without any up-link to or\ndependency on a cloud service. The kernel shows speed-up gains over single\nthread processing by a factor of four on moderately sized SNNs and 1.7X on a\nSynfire network. Furthermore, it load-balances all cores available on\nmulti-core processors, such as ARM, which run today's mobile devices and is up\nto 70% more energy efficient compared to statical core assignment. The present\nwork can enable the development of edge applications that have low Size,\nWeight, and Power (SWaP), and can prototype the integration of neuromorphic\nchips.",
      "authors": [
        {
          "name": "Lars Niedermeier",
          "affiliation": null
        },
        {
          "name": "Vyom Shah",
          "affiliation": null
        },
        {
          "name": "Jeffrey L. Krichmar",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17745v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17745v1",
      "primary_category": "cs.NE",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17739v1",
      "title": "Joint Multi-Condition Representation Modelling via Matrix Factorisation for Visual Place Recognition",
      "abstract": "We address multi-reference visual place recognition (VPR), where reference\nsets captured under varying conditions are used to improve localisation\nperformance. While deep learning with large-scale training improves robustness,\nincreasing data diversity and model complexity incur extensive computational\ncost during training and deployment. Descriptor-level fusion via voting or\naggregation avoids training, but often targets multi-sensor setups or relies on\nheuristics with limited gains under appearance and viewpoint change. We propose\na training-free, descriptor-agnostic approach that jointly models places using\nmultiple reference descriptors via matrix decomposition into basis\nrepresentations, enabling projection-based residual matching. We also introduce\nSotonMV, a structured benchmark for multi-viewpoint VPR. On multi-appearance\ndata, our method improves Recall@1 by up to ~18% over single-reference and\noutperforms multi-reference baselines across appearance and viewpoint changes,\nwith gains of ~5% on unstructured data, demonstrating strong generalisation\nwhile remaining lightweight.",
      "authors": [
        {
          "name": "Timur Ismagilov",
          "affiliation": null
        },
        {
          "name": "Shakaiba Majeed",
          "affiliation": null
        },
        {
          "name": "Michael Milford",
          "affiliation": null
        },
        {
          "name": "Tan Viet Tuyen Nguyen",
          "affiliation": null
        },
        {
          "name": "Sarvapali D. Ramchurn",
          "affiliation": null
        },
        {
          "name": "Shoaib Ehsan",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17739v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17739v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17734v1",
      "title": "Efficient Tensor Completion Algorithms for Highly Oscillatory Operators",
      "abstract": "This paper presents low-complexity tensor completion algorithms and their\nefficient implementation to reconstruct highly oscillatory operators\ndiscretized as $n\\times n$ matrices. The underlying tensor decomposition is\nbased on the reshaping of the input matrix and its butterfly decomposition into\nan order $\\mathcal{O} (\\log n)$ tensor. The reshaping of the input matrix into\na tensor allows for representation of the butterfly decomposition as a tensor\ndecomposition with dense tensors. This leads to efficient utilization of the\nexisting software infrastructure for dense and sparse tensor computations. We\npropose two tensor completion algorithms in the butterfly format, using\nalternating least squares and gradient-based optimization, as well as a novel\nstrategy that uses low-rank matrix completion to efficiently generate an\ninitial guess for the proposed algorithms. To demonstrate the efficiency and\napplicability of our proposed algorithms, we perform three numerical\nexperiments using simulated oscillatory operators in seismic applications. In\nthese experiments, we use $\\mathcal {O} (n \\log n)$ observed entries in the\ninput matrix and demonstrate an $\\mathcal{O}(n\\log^3 n)$ computational cost of\nthe proposed algorithms, leading to a speedup of orders of magnitudes per\niteration for large matrices compared to the low-rank matrix and quantized\ntensor-train completion. Moreover, the proposed butterfly completion\nalgorithms, equipped with the novel initial guess generation strategy, achieve\nreconstruction errors that are smaller by an order of magnitude, enabling\naccurate recovery of the underlying structure compared to the state-of-the-art\ncompletion algorithms.",
      "authors": [
        {
          "name": "Navjot Singh",
          "affiliation": null
        },
        {
          "name": "Edgar Solomonik",
          "affiliation": null
        },
        {
          "name": "Xiaoye Sherry Li",
          "affiliation": null
        },
        {
          "name": "Yang Liu",
          "affiliation": null
        }
      ],
      "categories": [
        "math.NA",
        "cs.LG",
        "cs.NA"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17734v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17734v1",
      "primary_category": "math.NA",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17733v1",
      "title": "Train for Truth, Keep the Skills: Binary Retrieval-Augmented Reward Mitigates Hallucinations",
      "abstract": "Language models often generate factually incorrect information unsupported by\ntheir training data, a phenomenon known as extrinsic hallucination. Existing\nmitigation approaches often degrade performance on open-ended generation and\ndownstream tasks, limiting their practical utility. We propose an online\nreinforcement learning method using a novel binary retrieval-augmented reward\n(RAR) to address this tradeoff. Unlike continuous reward schemes, our approach\nassigns a reward of one only when the model's output is entirely factually\ncorrect, and zero otherwise. We evaluate our method on Qwen3 reasoning models\nacross diverse tasks. For open-ended generation, binary RAR achieves a 39.3%\nreduction in hallucination rates, substantially outperforming both supervised\ntraining and continuous-reward RL baselines. In short-form question answering,\nthe model learns calibrated abstention, strategically outputting \"I don't know\"\nwhen faced with insufficient parametric knowledge. This yields 44.4% and 21.7%\nfewer incorrect answers on PopQA and GPQA, respectively. Crucially, these\nfactuality gains come without performance degradation on instruction following,\nmath, or code, whereas continuous-reward RL, despite improving factuality,\ninduces quality regressions.",
      "authors": [
        {
          "name": "Tong Chen",
          "affiliation": null
        },
        {
          "name": "Akari Asai",
          "affiliation": null
        },
        {
          "name": "Luke Zettlemoyer",
          "affiliation": null
        },
        {
          "name": "Hannaneh Hajishirzi",
          "affiliation": null
        },
        {
          "name": "Faeze Brahman",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17733v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17733v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17731v1",
      "title": "Can Image-To-Video Models Simulate Pedestrian Dynamics?",
      "abstract": "Recent high-performing image-to-video (I2V) models based on variants of the\ndiffusion transformer (DiT) have displayed remarkable inherent world-modeling\ncapabilities by virtue of training on large scale video datasets. We\ninvestigate whether these models can generate realistic pedestrian movement\npatterns in crowded public scenes. Our framework conditions I2V models on\nkeyframes extracted from pedestrian trajectory benchmarks, then evaluates their\ntrajectory prediction performance using quantitative measures of pedestrian\ndynamics.",
      "authors": [
        {
          "name": "Aaron Appelle",
          "affiliation": null
        },
        {
          "name": "Jerome P. Lynch",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17731v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17731v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17727v1",
      "title": "Enabling Fine-Grained Operating Points for Black-Box LLMs",
      "abstract": "Black-box Large Language Models (LLMs) provide practical and accessible\nalternatives to other machine learning methods, as they require minimal labeled\ndata and machine learning expertise to develop solutions for various decision\nmaking problems. However, for applications that need operating with constraints\non specific metrics (e.g., precision $\\geq$ 95%), decision making with\nblack-box LLMs remains unfavorable, due to their low numerical output\ncardinalities. This results in limited control over their operating points,\npreventing fine-grained adjustment of their decision making behavior. In this\npaper, we study using black-box LLMs as classifiers, focusing on efficiently\nimproving their operational granularity without performance loss. Specifically,\nwe first investigate the reasons behind their low-cardinality numerical outputs\nand show that they are biased towards generating rounded but informative\nverbalized probabilities. Then, we experiment with standard prompt engineering,\nuncertainty estimation and confidence elicitation techniques, and observe that\nthey do not effectively improve operational granularity without sacrificing\nperformance or increasing inference cost. Finally, we propose efficient\napproaches to significantly increase the number and diversity of available\noperating points. Our proposed approaches provide finer-grained operating\npoints and achieve comparable to or better performance than the benchmark\nmethods across 11 datasets and 3 LLMs.",
      "authors": [
        {
          "name": "Ege Beyazit",
          "affiliation": null
        },
        {
          "name": "KL Navaneet",
          "affiliation": null
        },
        {
          "name": "Prashant Mathur",
          "affiliation": null
        },
        {
          "name": "Roi Blanco",
          "affiliation": null
        },
        {
          "name": "Vidit Bansal",
          "affiliation": null
        },
        {
          "name": "Karim Bouyarmane",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17727v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17727v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17725v1",
      "title": "AcademicEval: Live Long-Context LLM Benchmark",
      "abstract": "Large Language Models (LLMs) have recently achieved remarkable performance in\nlong-context understanding. However, current long-context LLM benchmarks are\nlimited by rigid context length, labor-intensive annotation, and the pressing\nchallenge of label leakage issues during LLM training. Therefore, we propose\n\\textsc{AcademicEval}, a live benchmark for evaluating LLMs over long-context\ngeneration tasks. \\textsc{AcademicEval} adopts papers on arXiv to introduce\nseveral academic writing tasks with long-context inputs, \\textit{i.e.},\n\\textsc{Title}, \\textsc{Abstract}, \\textsc{Introduction}, and \\textsc{Related\nWork}, which cover a wide range of abstraction levels and require no manual\nlabeling. Moreover, \\textsc{AcademicEval} integrates high-quality and\nexpert-curated few-shot demonstrations from a collected co-author graph to\nenable flexible context length. Especially, \\textsc{AcademicEval} features an\nefficient live evaluation, ensuring no label leakage. We conduct a holistic\nevaluation on \\textsc{AcademicEval}, and the results illustrate that LLMs\nperform poorly on tasks with hierarchical abstraction levels and tend to\nstruggle with long few-shot demonstrations, highlighting the challenge of our\nbenchmark. Through experimental analysis, we also reveal some insights for\nenhancing LLMs' long-context modeling capabilities. Code is available at\nhttps://github.com/ulab-uiuc/AcademicEval",
      "authors": [
        {
          "name": "Haozhen Zhang",
          "affiliation": null
        },
        {
          "name": "Tao Feng",
          "affiliation": null
        },
        {
          "name": "Pengrui Han",
          "affiliation": null
        },
        {
          "name": "Jiaxuan You",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17725v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17725v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17724v1",
      "title": "Signature Forgery Detection: Improving Cross-Dataset Generalization",
      "abstract": "Automated signature verification is a critical biometric technique used in\nbanking, identity authentication, and legal documentation. Despite the notable\nprogress achieved by deep learning methods, most approaches in offline\nsignature verification still struggle to generalize across datasets, as\nvariations in handwriting styles and acquisition protocols often degrade\nperformance. This study investigates feature learning strategies for signature\nforgery detection, focusing on improving cross-dataset generalization -- that\nis, model robustness when trained on one dataset and tested on another. Using\nthree public benchmarks -- CEDAR, ICDAR, and GPDS Synthetic -- two experimental\npipelines were developed: one based on raw signature images and another\nemploying a preprocessing method referred to as shell preprocessing. Several\nbehavioral patterns were identified and analyzed; however, no definitive\nsuperiority between the two approaches was established. The results show that\nthe raw-image model achieved higher performance across benchmarks, while the\nshell-based model demonstrated promising potential for future refinement toward\nrobust, cross-domain signature verification.",
      "authors": [
        {
          "name": "Matheus Ramos Parracho",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17724v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17724v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17722v1",
      "title": "MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating Multimodal LLMs in Multi-Turn Dialogues",
      "abstract": "The recent development of Multimodal Large Language Models (MLLMs) has\nsignificantly advanced AI's ability to understand visual modalities. However,\nexisting evaluation benchmarks remain limited to single-turn question\nanswering, overlooking the complexity of multi-turn dialogues in real-world\nscenarios. To bridge this gap, we introduce MT-Video-Bench, a holistic video\nunderstanding benchmark for evaluating MLLMs in multi-turn dialogues.\nSpecifically, our MT-Video-Bench mainly assesses six core competencies that\nfocus on perceptivity and interactivity, encompassing 987 meticulously curated\nmulti-turn dialogues from diverse domains. These capabilities are rigorously\naligned with real-world applications, such as interactive sports analysis and\nmulti-turn video-based intelligent tutoring. With MT-Video-Bench, we\nextensively evaluate various state-of-the-art open-source and closed-source\nMLLMs, revealing their significant performance discrepancies and limitations in\nhandling multi-turn video dialogues. The benchmark will be publicly available\nto foster future research.",
      "authors": [
        {
          "name": "Yaning Pan",
          "affiliation": null
        },
        {
          "name": "Zekun Wang",
          "affiliation": null
        },
        {
          "name": "Qianqian Xie",
          "affiliation": null
        },
        {
          "name": "Yongqian Wen",
          "affiliation": null
        },
        {
          "name": "Yuanxing Zhang",
          "affiliation": null
        },
        {
          "name": "Guohui Zhang",
          "affiliation": null
        },
        {
          "name": "Haoxuan Hu",
          "affiliation": null
        },
        {
          "name": "Zhiyu Pan",
          "affiliation": null
        },
        {
          "name": "Yibing Huang",
          "affiliation": null
        },
        {
          "name": "Zhidong Gan",
          "affiliation": null
        },
        {
          "name": "Yonghong Lin",
          "affiliation": null
        },
        {
          "name": "An Ping",
          "affiliation": null
        },
        {
          "name": "Tianhao Peng",
          "affiliation": null
        },
        {
          "name": "Jiaheng Liu",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17722v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17722v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17720v1",
      "title": "PANER: A Paraphrase-Augmented Framework for Low-Resource Named Entity Recognition",
      "abstract": "Named Entity Recognition (NER) is a critical task that requires substantial\nannotated data, making it challenging in low-resource scenarios where label\nacquisition is expensive. While zero-shot and instruction-tuned approaches have\nmade progress, they often fail to generalize to domain-specific entities and do\nnot effectively utilize limited available data. We present a lightweight\nfew-shot NER framework that addresses these challenges through two key\ninnovations: (1) a new instruction tuning template with a simplified output\nformat that combines principles from prior IT approaches to leverage the large\ncontext window of recent state-of-the-art LLMs; (2) introducing a strategic\ndata augmentation technique that preserves entity information while\nparaphrasing the surrounding context, thereby expanding our training data\nwithout compromising semantic relationships. Experiments on benchmark datasets\nshow that our method achieves performance comparable to state-of-the-art models\non few-shot and zero-shot tasks, with our few-shot approach attaining an\naverage F1 score of 80.1 on the CrossNER datasets. Models trained with our\nparaphrasing approach show consistent improvements in F1 scores of up to 17\npoints over baseline versions, offering a promising solution for groups with\nlimited NER training data and compute power.",
      "authors": [
        {
          "name": "Nanda Kumar Rengarajan",
          "affiliation": null
        },
        {
          "name": "Jun Yan",
          "affiliation": null
        },
        {
          "name": "Chun Wang",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17720v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17720v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17719v1",
      "title": "Raindrop GS: A Benchmark for 3D Gaussian Splatting under Raindrop Conditions",
      "abstract": "3D Gaussian Splatting (3DGS) under raindrop conditions suffers from severe\nocclusions and optical distortions caused by raindrop contamination on the\ncamera lens, substantially degrading reconstruction quality. Existing\nbenchmarks typically evaluate 3DGS using synthetic raindrop images with known\ncamera poses (constrained images), assuming ideal conditions. However, in\nreal-world scenarios, raindrops often interfere with accurate camera pose\nestimation and point cloud initialization. Moreover, a significant domain gap\nbetween synthetic and real raindrops further impairs generalization. To tackle\nthese issues, we introduce RaindropGS, a comprehensive benchmark designed to\nevaluate the full 3DGS pipeline-from unconstrained, raindrop-corrupted images\nto clear 3DGS reconstructions. Specifically, the whole benchmark pipeline\nconsists of three parts: data preparation, data processing, and raindrop-aware\n3DGS evaluation, including types of raindrop interference, camera pose\nestimation and point cloud initialization, single image rain removal\ncomparison, and 3D Gaussian training comparison. First, we collect a real-world\nraindrop reconstruction dataset, in which each scene contains three aligned\nimage sets: raindrop-focused, background-focused, and rain-free ground truth,\nenabling a comprehensive evaluation of reconstruction quality under different\nfocus conditions. Through comprehensive experiments and analyses, we reveal\ncritical insights into the performance limitations of existing 3DGS methods on\nunconstrained raindrop images and the varying impact of different pipeline\ncomponents: the impact of camera focus position on 3DGS reconstruction\nperformance, and the interference caused by inaccurate pose and point cloud\ninitialization on reconstruction. These insights establish clear directions for\ndeveloping more robust 3DGS methods under raindrop conditions.",
      "authors": [
        {
          "name": "Zhiqiang Teng",
          "affiliation": null
        },
        {
          "name": "Beibei Lin",
          "affiliation": null
        },
        {
          "name": "Tingting Chen",
          "affiliation": null
        },
        {
          "name": "Zifeng Yuan",
          "affiliation": null
        },
        {
          "name": "Xuanyi Li",
          "affiliation": null
        },
        {
          "name": "Xuanyu Zhang",
          "affiliation": null
        },
        {
          "name": "Shunli Zhang",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17719v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17719v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17716v1",
      "title": "Automatic Classification of Circulating Blood Cell Clusters based on Multi-channel Flow Cytometry Imaging",
      "abstract": "Circulating blood cell clusters (CCCs) containing red blood cells (RBCs),\nwhite blood cells(WBCs), and platelets are significant biomarkers linked to\nconditions like thrombosis, infection, and inflammation. Flow cytometry, paired\nwith fluorescence staining, is commonly used to analyze these cell clusters,\nrevealing cell morphology and protein profiles. While computational approaches\nbased on machine learning have advanced the automatic analysis of single-cell\nflow cytometry images, there is a lack of effort to build tools to\nautomatically analyze images containing CCCs. Unlike single cells, cell\nclusters often exhibit irregular shapes and sizes. In addition, these cell\nclusters often consist of heterogeneous cell types, which require multi-channel\nstaining to identify the specific cell types within the clusters. This study\nintroduces a new computational framework for analyzing CCC images and\nidentifying cell types within clusters. Our framework uses a two-step analysis\nstrategy. First, it categorizes images into cell cluster and non-cluster groups\nby fine-tuning the You Only Look Once(YOLOv11) model, which outperforms\ntraditional convolutional neural networks (CNNs), Vision Transformers (ViT).\nThen, it identifies cell types by overlaying cluster contours with regions from\nmulti-channel fluorescence stains, enhancing accuracy despite cell debris and\nstaining artifacts. This approach achieved over 95% accuracy in both cluster\nclassification and phenotype identification. In summary, our automated\nframework effectively analyzes CCC images from flow cytometry, leveraging both\nbright-field and fluorescence data. Initially tested on blood cells, it holds\npotential for broader applications, such as analyzing immune and tumor cell\nclusters, supporting cellular research across various diseases.",
      "authors": [
        {
          "name": "Suqiang Ma",
          "affiliation": null
        },
        {
          "name": "Subhadeep Sengupta",
          "affiliation": null
        },
        {
          "name": "Yao Lee",
          "affiliation": null
        },
        {
          "name": "Beikang Gu",
          "affiliation": null
        },
        {
          "name": "Xianyan Chen",
          "affiliation": null
        },
        {
          "name": "Xianqiao Wang",
          "affiliation": null
        },
        {
          "name": "Yang Liu",
          "affiliation": null
        },
        {
          "name": "Mengjia Xu",
          "affiliation": null
        },
        {
          "name": "Galit H. Frydman",
          "affiliation": null
        },
        {
          "name": "He Li",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17716v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17716v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17715v1",
      "title": "QueST: Incentivizing LLMs to Generate Difficult Problems",
      "abstract": "Large Language Models have achieved strong performance on reasoning tasks,\nsolving competition-level coding and math problems. However, their scalability\nis limited by human-labeled datasets and the lack of large-scale, challenging\ncoding problem training data. Existing competitive coding datasets contain only\nthousands to tens of thousands of problems. Previous synthetic data generation\nmethods rely on either augmenting existing instruction datasets or selecting\nchallenging problems from human-labeled data. In this paper, we propose QueST,\na novel framework which combines difficulty-aware graph sampling and\ndifficulty-aware rejection fine-tuning that directly optimizes specialized\ngenerators to create challenging coding problems. Our trained generators\ndemonstrate superior capability compared to even GPT-4o at creating challenging\nproblems that benefit downstream performance. We leverage QueST to generate\nlarge-scale synthetic coding problems, which we then use to distill from strong\nteacher models with long chain-of-thought or to conduct reinforcement learning\nfor smaller models, proving effective in both scenarios. Our distillation\nexperiments demonstrate significant performance gains. Specifically, after\nfine-tuning Qwen3-8B-base on 100K difficult problems generated by QueST, we\nsurpass the performance of the original Qwen3-8B on LiveCodeBench. With an\nadditional 112K examples (i.e., 28K human-written problems paired with multiple\nsynthetic solutions), our 8B model matches the performance of the much larger\nDeepSeek-R1-671B. These findings indicate that generating complex problems via\nQueST offers an effective and scalable approach to advancing the frontiers of\ncompetitive coding and reasoning for large language models.",
      "authors": [
        {
          "name": "Hanxu Hu",
          "affiliation": null
        },
        {
          "name": "Xingxing Zhang",
          "affiliation": null
        },
        {
          "name": "Jannis Vamvas",
          "affiliation": null
        },
        {
          "name": "Rico Sennrich",
          "affiliation": null
        },
        {
          "name": "Furu Wei",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17715v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17715v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17714v1",
      "title": "The Marked Edge Walk: A Novel MCMC Algorithm for Sampling of Graph Partitions",
      "abstract": "Novel Markov Chain Monte Carlo (MCMC) methods have enabled the generation of\nlarge ensembles of redistricting plans through graph partitioning. However,\nexisting algorithms such as Reversible Recombination (RevReCom) and\nMetropolized Forest Recombination (MFR) are constrained to sampling from\ndistributions related to spanning trees. We introduce the marked edge walk\n(MEW), a novel MCMC algorithm for sampling from the space of graph partitions\nunder a tunable distribution. The walk operates on the space of spanning trees\nwith marked edges, allowing for calculable transition probabilities for use in\nthe Metropolis-Hastings algorithm. Empirical results on real-world dual graphs\nshow convergence under target distributions unrelated to spanning trees. For\nthis reason, MEW represents an advancement in flexible ensemble generation.",
      "authors": [
        {
          "name": "Atticus McWhorter",
          "affiliation": null
        },
        {
          "name": "Daryl DeFord",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.DS",
        "cs.LG",
        "physics.soc-ph"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17714v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17714v1",
      "primary_category": "cs.DS",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17709v1",
      "title": "Closing the Sim2Real Performance Gap in RL",
      "abstract": "Sim2Real aims at training policies in high-fidelity simulation environments\nand effectively transferring them to the real world. Despite the developments\nof accurate simulators and Sim2Real RL approaches, the policies trained purely\nin simulation often suffer significant performance drops when deployed in real\nenvironments. This drop is referred to as the Sim2Real performance gap. Current\nSim2Real RL methods optimize the simulator accuracy and variability as proxies\nfor real-world performance. However, these metrics do not necessarily correlate\nwith the real-world performance of the policy as established theoretically and\nempirically in the literature. We propose a novel framework to address this\nissue by directly adapting the simulator parameters based on real-world\nperformance. We frame this problem as a bi-level RL framework: the inner-level\nRL trains a policy purely in simulation, and the outer-level RL adapts the\nsimulation model and in-sim reward parameters to maximize real-world\nperformance of the in-sim policy. We derive and validate in simple examples the\nmathematical tools needed to develop bi-level RL algorithms that close the\nSim2Real performance gap.",
      "authors": [
        {
          "name": "Akhil S Anand",
          "affiliation": null
        },
        {
          "name": "Shambhuraj Sawant",
          "affiliation": null
        },
        {
          "name": "Jasper Hoffmann",
          "affiliation": null
        },
        {
          "name": "Dirk Reinhardt",
          "affiliation": null
        },
        {
          "name": "Sebastien Gros",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17709v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17709v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17705v1",
      "title": "Contextual Attention Modulation: Towards Efficient Multi-Task Adaptation in Large Language Models",
      "abstract": "Large Language Models (LLMs) possess remarkable generalization capabilities\nbut struggle with multi-task adaptation, particularly in balancing knowledge\nretention with task-specific specialization. Conventional fine-tuning methods\nsuffer from catastrophic forgetting and substantial resource consumption, while\nexisting parameter-efficient methods perform suboptimally in complex multi-task\nscenarios. To address this, we propose Contextual Attention Modulation (CAM), a\nnovel mechanism that dynamically modulates the representations of\nself-attention modules in LLMs. CAM enhances task-specific features while\npreserving general knowledge, thereby facilitating more effective and efficient\nadaptation. For effective multi-task adaptation, CAM is integrated into our\nHybrid Contextual Attention Modulation (HyCAM) framework, which combines a\nshared, full-parameter CAM module with multiple specialized, lightweight CAM\nmodules, enhanced by a dynamic routing strategy for adaptive knowledge fusion.\nExtensive experiments on heterogeneous tasks, including question answering,\ncode generation, and logical reasoning, demonstrate that our approach\nsignificantly outperforms existing approaches, achieving an average performance\nimprovement of 3.65%. The implemented code and data are available to ease\nreproducibility at https://github.com/Applied-Machine-Learning-Lab/HyCAM.",
      "authors": [
        {
          "name": "Dayan Pan",
          "affiliation": null
        },
        {
          "name": "Zhaoyang Fu",
          "affiliation": null
        },
        {
          "name": "Jingyuan Wang",
          "affiliation": null
        },
        {
          "name": "Xiao Han",
          "affiliation": null
        },
        {
          "name": "Yue Zhu",
          "affiliation": null
        },
        {
          "name": "Xiangyu Zhao",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17705v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17705v1",
      "primary_category": "cs.AI",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17703v1",
      "title": "Improving Cross-Patient Generalization in Parkinson's Disease Detection through Chunk-Based Analysis of Hand-Drawn Patterns",
      "abstract": "Parkinson's disease (PD) is a neurodegenerative disease affecting about 1% of\npeople over the age of 60, causing motor impairments that impede hand\ncoordination activities such as writing and drawing. Many approaches have tried\nto support early detection of Parkinson's disease based on hand-drawn images;\nhowever, we identified two major limitations in the related works: (1) the lack\nof sufficient datasets, (2) the robustness when dealing with unseen patient\ndata. In this paper, we propose a new approach to detect Parkinson's disease\nthat consists of two stages: The first stage classifies based on their drawing\ntype(circle, meander, spiral), and the second stage extracts the required\nfeatures from the images and detects Parkinson's disease. We overcame the\nprevious two limitations by applying a chunking strategy where we divide each\nimage into 2x2 chunks. Each chunk is processed separately when extracting\nfeatures and recognizing Parkinson's disease indicators. To make the final\nclassification, an ensemble method is used to merge the decisions made from\neach chunk. Our evaluation shows that our proposed approach outperforms the top\nperforming state-of-the-art approaches, in particular on unseen patients. On\nthe NewHandPD dataset our approach, it achieved 97.08% accuracy for seen\npatients and 94.91% for unseen patients, our proposed approach maintained a gap\nof only 2.17 percentage points, compared to the 4.76-point drop observed in\nprior work.",
      "authors": [
        {
          "name": "Mhd Adnan Albani",
          "affiliation": null
        },
        {
          "name": "Riad Sonbol",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17703v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17703v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17700v1",
      "title": "Elastic ViTs from Pretrained Models without Retraining",
      "abstract": "Vision foundation models achieve remarkable performance but are only\navailable in a limited set of pre-determined sizes, forcing sub-optimal\ndeployment choices under real-world constraints. We introduce SnapViT:\nSingle-shot network approximation for pruned Vision Transformers, a new\npost-pretraining structured pruning method that enables elastic inference\nacross a continuum of compute budgets. Our approach efficiently combines\ngradient information with cross-network structure correlations, approximated\nvia an evolutionary algorithm, does not require labeled data, generalizes to\nmodels without a classification head, and is retraining-free. Experiments on\nDINO, SigLIPv2, DeIT, and AugReg models demonstrate superior performance over\nstate-of-the-art methods across various sparsities, requiring less than five\nminutes on a single A100 GPU to generate elastic models that can be adjusted to\nany computational budget. Our key contributions include an efficient pruning\nstrategy for pretrained Vision Transformers, a novel evolutionary approximation\nof Hessian off-diagonal structures, and a self-supervised importance scoring\nmechanism that maintains strong performance without requiring retraining or\nlabels. Code and pruned models are available at: https://elastic.ashita.nl/",
      "authors": [
        {
          "name": "Walter Simoncini",
          "affiliation": null
        },
        {
          "name": "Michael Dorkenwald",
          "affiliation": null
        },
        {
          "name": "Tijmen Blankevoort",
          "affiliation": null
        },
        {
          "name": "Cees G. M. Snoek",
          "affiliation": null
        },
        {
          "name": "Yuki M. Asano",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17700v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17700v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17699v1",
      "title": "GAS: Improving Discretization of Diffusion ODEs via Generalized Adversarial Solver",
      "abstract": "While diffusion models achieve state-of-the-art generation quality, they\nstill suffer from computationally expensive sampling. Recent works address this\nissue with gradient-based optimization methods that distill a few-step ODE\ndiffusion solver from the full sampling process, reducing the number of\nfunction evaluations from dozens to just a few. However, these approaches often\nrely on intricate training techniques and do not explicitly focus on preserving\nfine-grained details. In this paper, we introduce the Generalized Solver: a\nsimple parameterization of the ODE sampler that does not require additional\ntraining tricks and improves quality over existing approaches. We further\ncombine the original distillation loss with adversarial training, which\nmitigates artifacts and enhances detail fidelity. We call the resulting method\nthe Generalized Adversarial Solver and demonstrate its superior performance\ncompared to existing solver training methods under similar resource\nconstraints. Code is available at https://github.com/3145tttt/GAS.",
      "authors": [
        {
          "name": "Aleksandr Oganov",
          "affiliation": null
        },
        {
          "name": "Ilya Bykov",
          "affiliation": null
        },
        {
          "name": "Eva Neudachina",
          "affiliation": null
        },
        {
          "name": "Mishan Aliev",
          "affiliation": null
        },
        {
          "name": "Alexander Tolmachev",
          "affiliation": null
        },
        {
          "name": "Alexander Sidorov",
          "affiliation": null
        },
        {
          "name": "Aleksandr Zuev",
          "affiliation": null
        },
        {
          "name": "Andrey Okhotin",
          "affiliation": null
        },
        {
          "name": "Denis Rakitin",
          "affiliation": null
        },
        {
          "name": "Aibek Alanov",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17699v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17699v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17698v1",
      "title": "Towards Mining Effective Pedagogical Strategies from Learner-LLM Educational Dialogues",
      "abstract": "Dialogue plays a crucial role in educational settings, yet existing\nevaluation methods for educational applications of large language models (LLMs)\nprimarily focus on technical performance or learning outcomes, often neglecting\nattention to learner-LLM interactions. To narrow this gap, this AIED Doctoral\nConsortium paper presents an ongoing study employing a dialogue analysis\napproach to identify effective pedagogical strategies from learner-LLM\ndialogues. The proposed approach involves dialogue data collection, dialogue\nact (DA) annotation, DA pattern mining, and predictive model building. Early\ninsights are outlined as an initial step toward future research. The work\nunderscores the need to evaluate LLM-based educational applications by focusing\non dialogue dynamics and pedagogical strategies.",
      "authors": [
        {
          "name": "Liqun He",
          "affiliation": null
        },
        {
          "name": "Manolis Mavrikis",
          "affiliation": null
        },
        {
          "name": "Mutlu Cukurova",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17698v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17698v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17697v1",
      "title": "A Principle of Targeted Intervention for Multi-Agent Reinforcement Learning",
      "abstract": "Steering cooperative multi-agent reinforcement learning (MARL) towards\ndesired outcomes is challenging, particularly when the global guidance from a\nhuman on the whole multi-agent system is impractical in a large-scale MARL. On\nthe other hand, designing mechanisms to coordinate agents most relies on\nempirical studies, lacking a easy-to-use research tool. In this work, we employ\nmulti-agent influence diagrams (MAIDs) as a graphical framework to address the\nabove issues. First, we introduce interaction paradigms that leverage MAIDs to\nanalyze and visualize existing approaches in MARL. Then, we design a new\ninteraction paradigm based on MAIDs, referred to as targeted intervention that\nis applied to only a single targeted agent, so the problem of global guidance\ncan be mitigated. In our implementation, we introduce a causal inference\ntechnique-referred to as Pre-Strategy Intervention (PSI)-to realize the\ntargeted intervention paradigm. Since MAIDs can be regarded as a special class\nof causal diagrams, a composite desired outcome that integrates the primary\ntask goal and an additional desired outcome can be achieved by maximizing the\ncorresponding causal effect through the PSI. Moreover, the bundled relevance\ngraph analysis of MAIDs provides a tool to identify whether an MARL learning\nparadigm is workable under the design of an interaction paradigm. In\nexperiments, we demonstrate the effectiveness of our proposed targeted\nintervention, and verify the result of relevance graph analysis.",
      "authors": [
        {
          "name": "Anjie Liu",
          "affiliation": null
        },
        {
          "name": "Jianhong Wang",
          "affiliation": null
        },
        {
          "name": "Samuel Kaski",
          "affiliation": null
        },
        {
          "name": "Jun Wang",
          "affiliation": null
        },
        {
          "name": "Mengyue Yang",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.MA",
        "I.2.11; I.2.6"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17697v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17697v1",
      "primary_category": "cs.AI",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17690v1",
      "title": "Efficient Algorithms for Mitigating Uncertainty and Risk in Reinforcement Learning",
      "abstract": "This dissertation makes three main contributions. First, We identify a new\nconnection between policy gradient and dynamic programming in MMDPs and propose\nthe Coordinate Ascent Dynamic Programming (CADP) algorithm to compute a Markov\npolicy that maximizes the discounted return averaged over the uncertain models.\nCADP adjusts model weights iteratively to guarantee monotone policy\nimprovements to a local maximum. Second, We establish sufficient and necessary\nconditions for the exponential ERM Bellman operator to be a contraction and\nprove the existence of stationary deterministic optimal policies for ERM-TRC\nand EVaR-TRC. We also propose exponential value iteration, policy iteration,\nand linear programming algorithms for computing optimal stationary policies for\nERM-TRC and EVaR-TRC. Third, We propose model-free Q-learning algorithms for\ncomputing policies with risk-averse objectives: ERM-TRC and EVaR-TRC. The\nchallenge is that Q-learning ERM Bellman may not be a contraction. Instead, we\nuse the monotonicity of Q-learning ERM Bellman operators to derive a rigorous\nproof that the ERM-TRC and the EVaR-TRC Q-learning algorithms converge to the\noptimal risk-averse value functions. The proposed Q-learning algorithms compute\nthe optimal stationary policy for ERM-TRC and EVaR-TRC.",
      "authors": [
        {
          "name": "Xihong Su",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17690v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17690v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17688v1",
      "title": "Quantum Synthetic Data Generation for Industrial Bioprocess Monitoring",
      "abstract": "Data scarcity and sparsity in bio-manufacturing poses challenges for accurate\nmodel\n  development, process monitoring, and optimization. We aim to replicate and\ncapture\n  the complex dynamics of industrial bioprocesses by proposing the use of a\nQuantum\n  Wasserstein Generative Adversarial Network with Gradient Penalty (QWGAN-GP)\nto\n  generate synthetic time series data for industrially relevant processes. The\n  generator within our GAN is comprised of a Parameterized Quantum Circuit\n(PQC). This\n  methodology offers potential advantages in process monitoring, modeling,\n  forecasting, and optimization, enabling more efficient bioprocess management\nby\n  reducing the dependence on scarce experimental data. Our results demonstrate\n  acceptable performance in capturing the temporal dynamics of real bioprocess\ndata.\n  We focus on Optical Density, a key measurement for Dry Biomass estimation.\nThe data\n  generated showed high fidelity to the actual historical experimental data.\nThis\n  intersection of quantum computing and machine learning has opened new\nfrontiers in\n  data analysis and generation, particularly in computationally intensive\nfields, for\n  use cases such as increasing prediction accuracy for soft sensor design or\nfor use\n  in predictive control.",
      "authors": [
        {
          "name": "Shawn M. Gibford",
          "affiliation": null
        },
        {
          "name": "Mohammad Reza Boskabadi",
          "affiliation": null
        },
        {
          "name": "Christopher J. Savoie",
          "affiliation": null
        },
        {
          "name": "Seyed Soheil Mansouri",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.ET",
        "cs.LG"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17688v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17688v1",
      "primary_category": "cs.ET",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17687v1",
      "title": "CrossGuard: Safeguarding MLLMs against Joint-Modal Implicit Malicious Attacks",
      "abstract": "Multimodal Large Language Models (MLLMs) achieve strong reasoning and\nperception capabilities but are increasingly vulnerable to jailbreak attacks.\nWhile existing work focuses on explicit attacks, where malicious content\nresides in a single modality, recent studies reveal implicit attacks, in which\nbenign text and image inputs jointly express unsafe intent. Such joint-modal\nthreats are difficult to detect and remain underexplored, largely due to the\nscarcity of high-quality implicit data. We propose ImpForge, an automated\nred-teaming pipeline that leverages reinforcement learning with tailored reward\nmodules to generate diverse implicit samples across 14 domains. Building on\nthis dataset, we further develop CrossGuard, an intent-aware safeguard\nproviding robust and comprehensive defense against both explicit and implicit\nthreats. Extensive experiments across safe and unsafe benchmarks, implicit and\nexplicit attacks, and multiple out-of-domain settings demonstrate that\nCrossGuard significantly outperforms existing defenses, including advanced\nMLLMs and guardrails, achieving stronger security while maintaining high\nutility. This offers a balanced and practical solution for enhancing MLLM\nrobustness against real-world multimodal threats.",
      "authors": [
        {
          "name": "Xu Zhang",
          "affiliation": null
        },
        {
          "name": "Hao Li",
          "affiliation": null
        },
        {
          "name": "Zhichao Lu",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17687v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17687v1",
      "primary_category": "cs.CR",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17686v1",
      "title": "Towards 3D Objectness Learning in an Open World",
      "abstract": "Recent advancements in 3D object detection and novel category detection have\nmade significant progress, yet research on learning generalized 3D objectness\nremains insufficient. In this paper, we delve into learning open-world 3D\nobjectness, which focuses on detecting all objects in a 3D scene, including\nnovel objects unseen during training. Traditional closed-set 3D detectors\nstruggle to generalize to open-world scenarios, while directly incorporating 3D\nopen-vocabulary models for open-world ability struggles with vocabulary\nexpansion and semantic overlap. To achieve generalized 3D object discovery, We\npropose OP3Det, a class-agnostic Open-World Prompt-free 3D Detector to detect\nany objects within 3D scenes without relying on hand-crafted text prompts. We\nintroduce the strong generalization and zero-shot capabilities of 2D foundation\nmodels, utilizing both 2D semantic priors and 3D geometric priors for\nclass-agnostic proposals to broaden 3D object discovery. Then, by integrating\ncomplementary information from point cloud and RGB image in the cross-modal\nmixture of experts, OP3Det dynamically routes uni-modal and multi-modal\nfeatures to learn generalized 3D objectness. Extensive experiments demonstrate\nthe extraordinary performance of OP3Det, which significantly surpasses existing\nopen-world 3D detectors by up to 16.0% in AR and achieves a 13.5% improvement\ncompared to closed-world 3D detectors.",
      "authors": [
        {
          "name": "Taichi Liu",
          "affiliation": null
        },
        {
          "name": "Zhenyu Wang",
          "affiliation": null
        },
        {
          "name": "Ruofeng Liu",
          "affiliation": null
        },
        {
          "name": "Guang Wang",
          "affiliation": null
        },
        {
          "name": "Desheng Zhang",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17686v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17686v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17685v1",
      "title": "Multilingual Text-to-Image Person Retrieval via Bidirectional Relation Reasoning and Aligning",
      "abstract": "Text-to-image person retrieval (TIPR) aims to identify the target person\nusing textual descriptions, facing challenge in modality heterogeneity. Prior\nworks have attempted to address it by developing cross-modal global or local\nalignment strategies. However, global methods typically overlook fine-grained\ncross-modal differences, whereas local methods require prior information to\nexplore explicit part alignments. Additionally, current methods are\nEnglish-centric, restricting their application in multilingual contexts. To\nalleviate these issues, we pioneer a multilingual TIPR task by developing a\nmultilingual TIPR benchmark, for which we leverage large language models for\ninitial translations and refine them by integrating domain-specific knowledge.\nCorrespondingly, we propose Bi-IRRA: a Bidirectional Implicit Relation\nReasoning and Aligning framework to learn alignment across languages and\nmodalities. Within Bi-IRRA, a bidirectional implicit relation reasoning module\nenables bidirectional prediction of masked image and text, implicitly enhancing\nthe modeling of local relations across languages and modalities, a\nmulti-dimensional global alignment module is integrated to bridge the modality\nheterogeneity. The proposed method achieves new state-of-the-art results on all\nmultilingual TIPR datasets. Data and code are presented in\nhttps://github.com/Flame-Chasers/Bi-IRRA.",
      "authors": [
        {
          "name": "Min Cao",
          "affiliation": null
        },
        {
          "name": "Xinyu Zhou",
          "affiliation": null
        },
        {
          "name": "Ding Jiang",
          "affiliation": null
        },
        {
          "name": "Bo Du",
          "affiliation": null
        },
        {
          "name": "Mang Ye",
          "affiliation": null
        },
        {
          "name": "Min Zhang",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17685v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17685v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17684v1",
      "title": "Intelligent Communication Mixture-of-Experts Boosted-Medical Image Segmentation Foundation Model",
      "abstract": "Foundation models for medical image segmentation have achieved remarkable\nperformance. Adaptive fine-tuning of natural image segmentation foundation\nmodels is crucial for medical image segmentation tasks. However, some\nlimitations exist in existing fine-tuning methods: 1) insufficient\nrepresentation of high-level features and 2) the fine-tuning process disrupts\nthe structural integrity of pretrained weights. Inspired by these critical\nproblems, we propose an intelligent communication mixture-of-experts\nboosted-medical image segmentation foundation model, named IC-MoE, with twofold\nideas: 1) We construct basic experts, semantic experts, and adaptive experts.\nMoreover, we implement a pixel probability adaptive voting strategy, which\nenables expert selection and fusion through label consistency and load\nbalancing. This approach preliminarily enhances the representation capability\nof high-level features while preserving the structural integrity of pretrained\nweights. 2) We propose a semantic-guided contrastive learning method to address\nthe issue of weak supervision in contrastive learning. This method further\nenhances the representation capability of high-level features while preserving\nthe structural integrity of pretrained weights. Extensive experiments across\nthree public medical image segmentation datasets demonstrate that the IC-MoE\noutperforms other SOTA models. Consequently, the proposed IC-MoE effectively\nsupplements foundational medical image segmentation models with high-level\nfeatures and pretrained structural integrity. We also validate the superior\ngeneralizability of the IC-MoE across diverse medical image segmentation\nscenarios.",
      "authors": [
        {
          "name": "Xinwei Zhang",
          "affiliation": null
        },
        {
          "name": "Hu Chen",
          "affiliation": null
        },
        {
          "name": "Zhe Yuan",
          "affiliation": null
        },
        {
          "name": "Sukun Tian",
          "affiliation": null
        },
        {
          "name": "Peng Feng",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17684v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17684v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17681v1",
      "title": "PICABench: How Far Are We from Physically Realistic Image Editing?",
      "abstract": "Image editing has achieved remarkable progress recently. Modern editing\nmodels could already follow complex instructions to manipulate the original\ncontent. However, beyond completing the editing instructions, the accompanying\nphysical effects are the key to the generation realism. For example, removing\nan object should also remove its shadow, reflections, and interactions with\nnearby objects. Unfortunately, existing models and benchmarks mainly focus on\ninstruction completion but overlook these physical effects. So, at this moment,\nhow far are we from physically realistic image editing? To answer this, we\nintroduce PICABench, which systematically evaluates physical realism across\neight sub-dimension (spanning optics, mechanics, and state transitions) for\nmost of the common editing operations (add, remove, attribute change, etc). We\nfurther propose the PICAEval, a reliable evaluation protocol that uses\nVLM-as-a-judge with per-case, region-level human annotations and questions.\nBeyond benchmarking, we also explore effective solutions by learning physics\nfrom videos and construct a training dataset PICA-100K. After evaluating most\nof the mainstream models, we observe that physical realism remains a\nchallenging problem with large rooms to explore. We hope that our benchmark and\nproposed solutions can serve as a foundation for future work moving from naive\ncontent editing toward physically consistent realism.",
      "authors": [
        {
          "name": "Yuandong Pu",
          "affiliation": null
        },
        {
          "name": "Le Zhuo",
          "affiliation": null
        },
        {
          "name": "Songhao Han",
          "affiliation": null
        },
        {
          "name": "Jinbo Xing",
          "affiliation": null
        },
        {
          "name": "Kaiwen Zhu",
          "affiliation": null
        },
        {
          "name": "Shuo Cao",
          "affiliation": null
        },
        {
          "name": "Bin Fu",
          "affiliation": null
        },
        {
          "name": "Si Liu",
          "affiliation": null
        },
        {
          "name": "Hongsheng Li",
          "affiliation": null
        },
        {
          "name": "Yu Qiao",
          "affiliation": null
        },
        {
          "name": "Wenlong Zhang",
          "affiliation": null
        },
        {
          "name": "Xi Chen",
          "affiliation": null
        },
        {
          "name": "Yihao Liu",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17681v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17681v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17671v1",
      "title": "LILO: Bayesian Optimization with Interactive Natural Language Feedback",
      "abstract": "For many real-world applications, feedback is essential in translating\ncomplex, nuanced, or subjective goals into quantifiable optimization\nobjectives. We propose a language-in-the-loop framework that uses a large\nlanguage model (LLM) to convert unstructured feedback in the form of natural\nlanguage into scalar utilities to conduct BO over a numeric search space.\nUnlike preferential BO, which only accepts restricted feedback formats and\nrequires customized models for each domain-specific problem, our approach\nleverages LLMs to turn varied types of textual feedback into consistent utility\nsignals and to easily include flexible user priors without manual kernel\ndesign. At the same time, our method maintains the sample efficiency and\nprincipled uncertainty quantification of BO. We show that this hybrid method\nnot only provides a more natural interface to the decision maker but also\noutperforms conventional BO baselines and LLM-only optimizers, particularly in\nfeedback-limited regimes.",
      "authors": [
        {
          "name": "Katarzyna Kobalczyk",
          "affiliation": null
        },
        {
          "name": "Zhiyuan Jerry Lin",
          "affiliation": null
        },
        {
          "name": "Benjamin Letham",
          "affiliation": null
        },
        {
          "name": "Zhuokai Zhao",
          "affiliation": null
        },
        {
          "name": "Maximilian Balandat",
          "affiliation": null
        },
        {
          "name": "Eytan Bakshy",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17671v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17671v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17670v1",
      "title": "On-the-Fly OVD Adaptation with FLAME: Few-shot Localization via Active Marginal-Samples Exploration",
      "abstract": "Open-vocabulary object detection (OVD) models offer remarkable flexibility by\ndetecting objects from arbitrary text queries. However, their zero-shot\nperformance in specialized domains like Remote Sensing (RS) is often\ncompromised by the inherent ambiguity of natural language, limiting critical\ndownstream applications. For instance, an OVD model may struggle to distinguish\nbetween fine-grained classes such as \"fishing boat\" and \"yacht\" since their\nembeddings are similar and often inseparable. This can hamper specific user\ngoals, such as monitoring illegal fishing, by producing irrelevant detections.\nTo address this, we propose a cascaded approach that couples the broad\ngeneralization of a large pre-trained OVD model with a lightweight few-shot\nclassifier. Our method first employs the zero-shot model to generate\nhigh-recall object proposals. These proposals are then refined for high\nprecision by a compact classifier trained in real-time on only a handful of\nuser-annotated examples - drastically reducing the high costs of RS imagery\nannotation.The core of our framework is FLAME, a one-step active learning\nstrategy that selects the most informative samples for training. FLAME\nidentifies, on the fly, uncertain marginal candidates near the decision\nboundary using density estimation, followed by clustering to ensure sample\ndiversity. This efficient sampling technique achieves high accuracy without\ncostly full-model fine-tuning and enables instant adaptation, within less then\na minute, which is significantly faster than state-of-the-art alternatives.Our\nmethod consistently surpasses state-of-the-art performance on RS benchmarks,\nestablishing a practical and resource-efficient framework for adapting\nfoundation models to specific user needs.",
      "authors": [
        {
          "name": "Yehonathan Refael",
          "affiliation": null
        },
        {
          "name": "Amit Aides",
          "affiliation": null
        },
        {
          "name": "Aviad Barzilai",
          "affiliation": null
        },
        {
          "name": "George Leifman",
          "affiliation": null
        },
        {
          "name": "Genady Beryozkin",
          "affiliation": null
        },
        {
          "name": "Vered Silverman",
          "affiliation": null
        },
        {
          "name": "Bolous Jaber",
          "affiliation": null
        },
        {
          "name": "Tomer Shekel",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IR"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17670v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17670v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17664v1",
      "title": "4DSegStreamer: Streaming 4D Panoptic Segmentation via Dual Threads",
      "abstract": "4D panoptic segmentation in a streaming setting is critical for highly\ndynamic environments, such as evacuating dense crowds and autonomous driving in\ncomplex scenarios, where real-time, fine-grained perception within a\nconstrained time budget is essential. In this paper, we introduce\n4DSegStreamer, a novel framework that employs a Dual-Thread System to\nefficiently process streaming frames. The framework is general and can be\nseamlessly integrated into existing 3D and 4D segmentation methods to enable\nreal-time capability. It also demonstrates superior robustness compared to\nexisting streaming perception approaches, particularly under high FPS\nconditions. The system consists of a predictive thread and an inference thread.\nThe predictive thread leverages historical motion and geometric information to\nextract features and forecast future dynamics. The inference thread ensures\ntimely prediction for incoming frames by aligning with the latest memory and\ncompensating for ego-motion and dynamic object movements. We evaluate\n4DSegStreamer on the indoor HOI4D dataset and the outdoor SemanticKITTI and\nnuScenes datasets. Comprehensive experiments demonstrate the effectiveness of\nour approach, particularly in accurately predicting dynamic objects in complex\nscenes.",
      "authors": [
        {
          "name": "Ling Liu",
          "affiliation": null
        },
        {
          "name": "Jun Tian",
          "affiliation": null
        },
        {
          "name": "Li Yi",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17664v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17664v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17662v1",
      "title": "DELULU: Discriminative Embedding Learning Using Latent Units for Speaker-Aware Self-Supervised Speech Foundational Model",
      "abstract": "Self-supervised speech models have achieved remarkable success on\ncontent-driven tasks, yet they remain limited in capturing\nspeaker-discriminative features critical for verification, diarization, and\nprofiling applications. We introduce DELULU, a speaker-aware self-supervised\nfoundational model that addresses this limitation by integrating external\nsupervision into the pseudo-label generation process. DELULU leverages\nframe-level embeddings from ReDimNet, a state-of-the-art speaker verification\nmodel, to guide the k-means clustering step during pre-training, introducing a\nstrong speaker-discriminative inductive bias that aligns representation\nlearning with speaker identity. The model is trained using a dual objective\nthat combines masked prediction and denoising, further enhancing robustness and\ngeneralization. DELULU significantly outperforms prior self-supervised learning\n(SSL) models across a range of speaker-centric tasks, achieving up to 62%\nrelative improvement in equal error rate (EER) for speaker verification and\nconsistent gains on zero-shot profiling tasks such as gender, age, accent, and\nspeaker counting. Our findings demonstrate that DELULU is a strong universal\nencoder for speaker-aware speech processing, enabling superior performance even\nwithout task-specific fine-tuning.",
      "authors": [
        {
          "name": "Massa Baali",
          "affiliation": null
        },
        {
          "name": "Rita Singh",
          "affiliation": null
        },
        {
          "name": "Bhiksha Raj",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.SD",
        "cs.CL"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17662v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17662v1",
      "primary_category": "cs.SD",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17661v1",
      "title": "Handling Extreme Class Imbalance: Using GANs in Data Augmentation for Suicide Prediction",
      "abstract": "Suicide prediction is the key for prevention, but real data with sufficient\npositive samples is rare and causes extreme class imbalance. We utilized\nmachine learning (ML) to build the model and deep learning (DL) techniques,\nlike Generative Adversarial Networks (GAN), to generate synthetic data samples\nto enhance the dataset. The initial dataset contained 656 samples, with only\nfour positive cases, prompting the need for data augmentation. A variety of\nmachine learning models, ranging from interpretable data models to black box\nalgorithmic models, were used. On real test data, Logistic Regression (LR)\nachieved a weighted precision of 0.99, a weighted recall of 0.85, and a\nweighted F1 score of 0.91; Random Forest (RF) showed 0.98, 0.99, and 0.99,\nrespectively; and Support Vector Machine (SVM) achieved 0.99, 0.76, and 0.86.\nLR and SVM correctly identified one suicide attempt case (sensitivity:1.0) and\nmisclassified LR(20) and SVM (31) non-attempts as attempts (specificity: 0.85 &\n0.76, respectively). RF identified 0 suicide attempt cases (sensitivity: 0.0)\nwith 0 false positives (specificity: 1.0). These results highlight the models'\neffectiveness, with GAN playing a key role in generating synthetic data to\nsupport suicide prevention modeling efforts.",
      "authors": [
        {
          "name": "Vaishnavi Visweswaraiah",
          "affiliation": null
        },
        {
          "name": "Tanvi Banerjee",
          "affiliation": null
        },
        {
          "name": "William Romine",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17661v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17661v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17652v1",
      "title": "Qomhra: A Bilingual Irish-English Large Language Model",
      "abstract": "This paper introduces Qomhr\\'a, a bilingual Irish-English large language\nmodel (LLM), developed under low-resource constraints presenting a complete\npipeline spanning bilingual continued pre-training, instruction tuning, and\nalignment from human preferences. Newly accessible Irish corpora and English\ntext are mixed and curated to improve Irish performance while preserving\nEnglish ability. 6 closed-weight LLMs are judged for their Irish text\ngeneration by a native speaker, a learner and other LLMs. Google's\nGemini-2.5-Pro is ranked the highest and is subsequently used to synthesise\ninstruction tuning and human preference datasets. Two datasets are contributed\nleveraging Gemini-2.5-Pro: a 30K Irish-English parallel instruction tuning\ndataset and a 1K human preference dataset, generating accepted and rejected\nresponses that show near perfect alignment with a native Irish speaker.\nQomhr\\'a is comprehensively evaluated across benchmarks testing translation,\ngender understanding, topic identification and world knowledge with gains of up\nto 29% in Irish and 44% in English. Qomhr\\'a also undergoes instruction tuning\nand demonstrates clear progress in instruction following, crucial for chatbot\nfunctionality.",
      "authors": [
        {
          "name": "Joseph McInerney",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL",
        "I.2.7"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17652v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17652v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17651v1",
      "title": "Frugal Federated Learning for Violence Detection: A Comparison of LoRA-Tuned VLMs and Personalized CNNs",
      "abstract": "We examine frugal federated learning approaches to violence detection by\ncomparing two complementary strategies: (i) zero-shot and federated fine-tuning\nof vision-language models (VLMs), and (ii) personalized training of a compact\n3D convolutional neural network (CNN3D). Using LLaVA-7B and a 65.8M parameter\nCNN3D as representative cases, we evaluate accuracy, calibration, and energy\nusage under realistic non-IID settings. Both approaches exceed 90% accuracy.\nCNN3D slightly outperforms Low-Rank Adaptation(LoRA)-tuned VLMs in ROC AUC and\nlog loss, while using less energy. VLMs remain favorable for contextual\nreasoning and multimodal inference. We quantify energy and CO$_2$ emissions\nacross training and inference, and analyze sustainability trade-offs for\ndeployment. To our knowledge, this is the first comparative study of LoRA-tuned\nvision-language models and personalized CNNs for federated violence detection,\nwith an emphasis on energy efficiency and environmental metrics. These findings\nsupport a hybrid model: lightweight CNNs for routine classification, with\nselective VLM activation for complex or descriptive scenarios. The resulting\nframework offers a reproducible baseline for responsible, resource-aware AI in\nvideo surveillance, with extensions toward real-time, multimodal, and\nlifecycle-aware systems.",
      "authors": [
        {
          "name": "S\u00e9bastien Thuau",
          "affiliation": null
        },
        {
          "name": "Siba Haidar",
          "affiliation": null
        },
        {
          "name": "Ayush Bajracharya",
          "affiliation": null
        },
        {
          "name": "Rachid Chelouah",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17651v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17651v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17650v1",
      "title": "ZACH-ViT: A Zero-Token Vision Transformer with ShuffleStrides Data Augmentation for Robust Lung Ultrasound Classification",
      "abstract": "Differentiating cardiogenic pulmonary oedema (CPE) from non-cardiogenic and\nstructurally normal lungs in lung ultrasound (LUS) videos remains challenging\ndue to the high visual variability of non-cardiogenic inflammatory patterns\n(NCIP/ARDS-like), interstitial lung disease, and healthy lungs. This\nheterogeneity complicates automated classification as overlapping B-lines and\npleural artefacts are common. We introduce ZACH-ViT (Zero-token Adaptive\nCompact Hierarchical Vision Transformer), a 0.25 M-parameter Vision Transformer\nvariant that removes both positional embeddings and the [CLS] token, making it\nfully permutation-invariant and suitable for unordered medical image data. To\nenhance generalization, we propose ShuffleStrides Data Augmentation (SSDA),\nwhich permutes probe-view sequences and frame orders while preserving\nanatomical validity. ZACH-ViT was evaluated on 380 LUS videos from 95\ncritically ill patients against nine state-of-the-art baselines. Despite the\nheterogeneity of the non-cardiogenic group, ZACH-ViT achieved the highest\nvalidation and test ROC-AUC (0.80 and 0.79) with balanced sensitivity (0.60)\nand specificity (0.91), while all competing models collapsed to trivial\nclassification. It trains 1.35x faster than Minimal ViT (0.62M parameters) with\n2.5x fewer parameters, supporting real-time clinical deployment. These results\nshow that aligning architectural design with data structure can outperform\nscale in small-data medical imaging.",
      "authors": [
        {
          "name": "Athanasios Angelakis",
          "affiliation": null
        },
        {
          "name": "Amne Mousa",
          "affiliation": null
        },
        {
          "name": "Micah L. A. Heldeweg",
          "affiliation": null
        },
        {
          "name": "Laurens A. Biesheuvel",
          "affiliation": null
        },
        {
          "name": "Mark A. Haaksma",
          "affiliation": null
        },
        {
          "name": "Jasper M. Smit",
          "affiliation": null
        },
        {
          "name": "Pieter R. Tuinman",
          "affiliation": null
        },
        {
          "name": "Paul W. G. Elbers",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17650v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17650v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17644v1",
      "title": "Self-supervised Pre-training for Mapping of Archaeological Stone Wall in Historic Landscapes Using High-Resolution DEM Derivatives",
      "abstract": "Dry-stone walls hold significant heritage and environmental value. Mapping\nthese structures is essential for ecosystem preservation and wildfire\nmanagement in Australia. Yet, many walls remain unidentified due to their\ninaccessibility and the high cost of manual mapping. Deep learning-based\nsegmentation offers a scalable solution, but two major challenges persist: (1)\nvisual occlusion of low-lying walls by dense vegetation, and (2) limited\nlabeled data for supervised training. We propose DINO-CV, a segmentation\nframework for automatic mapping of low-lying dry-stone walls using\nhigh-resolution Airborne LiDAR-derived digital elevation models (DEMs). DEMs\novercome visual occlusion by capturing terrain structures hidden beneath\nvegetation, enabling analysis of structural rather than spectral cues. DINO-CV\nintroduces a self-supervised cross-view pre-training strategy based on\nknowledge distillation to mitigate data scarcity. It learns invariant visual\nand geometric representations across multiple DEM derivatives, supporting\nvarious vision backbones including ResNet, Wide ResNet, and Vision\nTransformers. Applied to the UNESCO World Heritage cultural landscape of Budj\nBim, Victoria, the method identifies one of Australia's densest collections of\ncolonial dry-stone walls beyond Indigenous heritage contexts. DINO-CV achieves\na mean Intersection over Union (mIoU) of 68.6% on test areas and maintains\n63.8% mIoU when fine-tuned with only 10% labeled data. These results\ndemonstrate the potential of self-supervised learning on high-resolution DEM\nderivatives for automated dry-stone wall mapping in vegetated and heritage-rich\nenvironments with scarce annotations.",
      "authors": [
        {
          "name": "Zexian Huang",
          "affiliation": null
        },
        {
          "name": "Mashnoon Islam",
          "affiliation": null
        },
        {
          "name": "Brian Armstrong",
          "affiliation": null
        },
        {
          "name": "Kourosh Khoshelham",
          "affiliation": null
        },
        {
          "name": "Martin Tomko",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17644v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17644v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17642v1",
      "title": "Quantum Federated Learning: Architectural Elements and Future Directions",
      "abstract": "Federated learning (FL) focuses on collaborative model training without the\nneed to move the private data silos to a central server. Despite its several\nbenefits, the classical FL is plagued with several limitations, such as high\ncomputational power required for model training(which is critical for\nlow-resource clients), privacy risks, large update traffic, and non-IID\nheterogeneity. This chapter surveys a hybrid paradigm - Quantum Federated\nLearning (QFL), which introduces quantum computation, that addresses multiple\nchallenges of classical FL and offers rapid computing capability while keeping\nthe classical orchestration intact. Firstly, we motivate QFL with a concrete\npresentation on pain points of classical FL, followed by a discussion on a\ngeneral architecture of QFL frameworks specifying the roles of client and\nserver, communication primitives and the quantum model placement. We classify\nthe existing QFL systems based on four criteria - quantum architecture (pure\nQFL, hybrid QFL), data processing method (quantum data encoding, quantum\nfeature mapping, and quantum feature selection & dimensionality reduction),\nnetwork topology (centralized, hierarchial, decentralized), and quantum\nsecurity mechanisms (quantum key distribution, quantum homomorphic encryption,\nquantum differential privacy, blind quantum computing). We then describe\napplications of QFL in healthcare, vehicular networks, wireless networks, and\nnetwork security, clearly highlighting where QFL improves communication\nefficiency, security, and performance compared to classical FL. We close with\nmultiple challenges and future works in QFL, including extension of QFL beyond\nclassification tasks, adversarial attacks, realistic hardware deployment,\nquantum communication protocols deployment, aggregation of different quantum\nmodels, and quantum split learning as an alternative to QFL.",
      "authors": [
        {
          "name": "Siva Sai",
          "affiliation": null
        },
        {
          "name": "Abhishek Sawaika",
          "affiliation": null
        },
        {
          "name": "Prabhjot Singh",
          "affiliation": null
        },
        {
          "name": "Rajkumar Buyya",
          "affiliation": null
        }
      ],
      "categories": [
        "quant-ph",
        "cs.DC",
        "cs.LG",
        "I.2; A.1"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17642v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17642v1",
      "primary_category": "quant-ph",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17640v1",
      "title": "RESample: A Robust Data Augmentation Framework via Exploratory Sampling for Robotic Manipulation",
      "abstract": "Vision-Language-Action models (VLAs) have demonstrated remarkable performance\non complex robotic manipulation tasks through imitation learning. However,\nexisting imitation learning datasets contain only successful trajectories and\nlack failure or recovery data, especially for out-of-distribution (OOD) states\nwhere the robot deviates from the main policy due to minor perturbations or\nerrors, leading VLA models to struggle with states deviating from the training\ndistribution. To this end, we propose an automated OOD data augmentation\nframework named RESample through exploratory sampling. Specifically, we first\nleverage offline reinforcement learning to obtain an action-value network that\naccurately identifies sub-optimal actions under the current manipulation\npolicy. We further sample potential OOD states from trajectories via rollout,\nand design an exploratory sampling mechanism that adaptively incorporates these\naction proxies into the training dataset to ensure efficiency. Subsequently,\nour framework explicitly encourages the VLAs to recover from OOD states and\nenhances their robustness against distributional shifts. We conduct extensive\nexperiments on the LIBERO benchmark as well as real-world robotic manipulation\ntasks, demonstrating that RESample consistently improves the stability and\ngeneralization ability of VLA models.",
      "authors": [
        {
          "name": "Yuquan Xue",
          "affiliation": null
        },
        {
          "name": "Guanxing Lu",
          "affiliation": null
        },
        {
          "name": "Zhenyu Wu",
          "affiliation": null
        },
        {
          "name": "Chuanrui Zhang",
          "affiliation": null
        },
        {
          "name": "Bofang Jia",
          "affiliation": null
        },
        {
          "name": "Zhengyi Gu",
          "affiliation": null
        },
        {
          "name": "Yansong Tang",
          "affiliation": null
        },
        {
          "name": "Ziwei Wang",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17640v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17640v1",
      "primary_category": "cs.RO",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17638v1",
      "title": "LLM-as-a-Prophet: Understanding Predictive Intelligence with Prophet Arena",
      "abstract": "Forecasting is not only a fundamental intellectual pursuit but also is of\nsignificant importance to societal systems such as finance and economics. With\nthe rapid advances of large language models (LLMs) trained on Internet-scale\ndata, it raises the promise of employing LLMs to forecast real-world future\nevents, an emerging paradigm we call \"LLM-as-a-Prophet\". This paper\nsystematically investigates such predictive intelligence of LLMs. To this end,\nwe build Prophet Arena, a general evaluation benchmark that continuously\ncollects live forecasting tasks and decomposes each task into distinct pipeline\nstages, in order to support our controlled and large-scale experimentation. Our\ncomprehensive evaluation reveals that many LLMs already exhibit impressive\nforecasting capabilities, reflected in, e.g., their small calibration errors,\nconsistent prediction confidence and promising market returns. However, we also\nuncover key bottlenecks towards achieving superior predictive intelligence via\nLLM-as-a-Prophet, such as LLMs' inaccurate event recalls, misunderstanding of\ndata sources and slower information aggregation compared to markets when\nresolution nears.",
      "authors": [
        {
          "name": "Qingchuan Yang",
          "affiliation": null
        },
        {
          "name": "Simon Mahns",
          "affiliation": null
        },
        {
          "name": "Sida Li",
          "affiliation": null
        },
        {
          "name": "Anri Gu",
          "affiliation": null
        },
        {
          "name": "Jibang Wu",
          "affiliation": null
        },
        {
          "name": "Haifeng Xu",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17638v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17638v1",
      "primary_category": "cs.AI",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17626v1",
      "title": "CaMiT: A Time-Aware Car Model Dataset for Classification and Generation",
      "abstract": "AI systems must adapt to evolving visual environments, especially in domains\nwhere object appearances change over time. We introduce Car Models in Time\n(CaMiT), a fine-grained dataset capturing the temporal evolution of car models,\na representative class of technological artifacts. CaMiT includes 787K labeled\nsamples of 190 car models (2007-2023) and 5.1M unlabeled samples (2005-2023),\nsupporting both supervised and self-supervised learning. Static pretraining on\nin-domain data achieves competitive performance with large-scale generalist\nmodels while being more resource-efficient, yet accuracy declines when models\nare tested across years. To address this, we propose a time-incremental\nclassification setting, a realistic continual learning scenario with emerging,\nevolving, and disappearing classes. We evaluate two strategies:\ntime-incremental pretraining, which updates the backbone, and time-incremental\nclassifier learning, which updates only the final layer, both improving\ntemporal robustness. Finally, we explore time-aware image generation that\nleverages temporal metadata during training, yielding more realistic outputs.\nCaMiT offers a rich benchmark for studying temporal adaptation in fine-grained\nvisual recognition and generation.",
      "authors": [
        {
          "name": "Fr\u00e9d\u00e9ric LIN",
          "affiliation": null
        },
        {
          "name": "Biruk Abere Ambaw",
          "affiliation": null
        },
        {
          "name": "Adrian Popescu",
          "affiliation": null
        },
        {
          "name": "Hejer Ammar",
          "affiliation": null
        },
        {
          "name": "Romaric Audigier",
          "affiliation": null
        },
        {
          "name": "Herv\u00e9 Le Borgne",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17626v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17626v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17622v1",
      "title": "Just-In-Time Piecewise-Linear Semantics for ReLU-type Networks",
      "abstract": "We present a JIT PL semantics for ReLU-type networks that compiles models\ninto a guarded CPWL transducer with shared guards. The system adds hyperplanes\nonly when operands are affine on the current cell, maintains global lower/upper\nenvelopes, and uses a budgeted branch-and-bound. We obtain anytime soundness,\nexactness on fully refined cells, monotone progress, guard-linear complexity\n(avoiding global $\\binom{k}{2}$), dominance pruning, and decidability under\nfinite refinement. The shared carrier supports region extraction, decision\ncomplexes, Jacobians, exact/certified Lipschitz, LP/SOCP robustness, and\nmaximal causal influence. A minimal prototype returns certificates or\ncounterexamples with cost proportional to visited subdomains.",
      "authors": [
        {
          "name": "Hongyi Duan",
          "affiliation": null
        },
        {
          "name": "Haoyang Liu",
          "affiliation": null
        },
        {
          "name": "Jian'an Zhang",
          "affiliation": null
        },
        {
          "name": "Fengrui Liu",
          "affiliation": null
        },
        {
          "name": "Yiyi Wang",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LO",
        "cs.LG"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17622v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17622v1",
      "primary_category": "cs.LO",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17621v1",
      "title": "GUIDE: Enhancing Gradient Inversion Attacks in Federated Learning with Denoising Models",
      "abstract": "Federated Learning (FL) enables collaborative training of Machine Learning\n(ML) models across multiple clients while preserving their privacy. Rather than\nsharing raw data, federated clients transmit locally computed updates to train\nthe global model. Although this paradigm should provide stronger privacy\nguarantees than centralized ML, client updates remain vulnerable to privacy\nleakage. Adversaries can exploit them to infer sensitive properties about the\ntraining data or even to reconstruct the original inputs via Gradient Inversion\nAttacks (GIAs). Under the honest-butcurious threat model, GIAs attempt to\nreconstruct training data by reversing intermediate updates using\noptimizationbased techniques. We observe that these approaches usually\nreconstruct noisy approximations of the original inputs, whose quality can be\nenhanced with specialized denoising models. This paper presents Gradient Update\nInversion with DEnoising (GUIDE), a novel methodology that leverages diffusion\nmodels as denoising tools to improve image reconstruction attacks in FL. GUIDE\ncan be integrated into any GIAs that exploits surrogate datasets, a widely\nadopted assumption in GIAs literature. We comprehensively evaluate our approach\nin two attack scenarios that use different FL algorithms, models, and datasets.\nOur results demonstrate that GUIDE integrates seamlessly with two state-ofthe-\nart GIAs, substantially improving reconstruction quality across multiple\nmetrics. Specifically, GUIDE achieves up to 46% higher perceptual similarity,\nas measured by the DreamSim metric.",
      "authors": [
        {
          "name": "Vincenzo Carletti",
          "affiliation": null
        },
        {
          "name": "Pasquale Foggia",
          "affiliation": null
        },
        {
          "name": "Carlo Mazzocca",
          "affiliation": null
        },
        {
          "name": "Giuseppe Parrella",
          "affiliation": null
        },
        {
          "name": "Mario Vento",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17621v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17621v1",
      "primary_category": "cs.CR",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17620v1",
      "title": "Forget to Know, Remember to Use: Context-Aware Unlearning for Large Language Models",
      "abstract": "Large language models may encode sensitive information or outdated knowledge\nthat needs to be removed, to ensure responsible and compliant model responses.\nUnlearning has emerged as an efficient alternative to full retraining, aiming\nto remove specific knowledge while preserving overall model utility. Existing\nevaluations of unlearning methods focus on (1) the extent of forgetting of the\ntarget knowledge (forget set) and (2) maintaining performance on the retain set\n(i.e., utility). However, these evaluations overlook an important usability\naspect: users may still want the model to leverage the removed information if\nit is re-introduced in the prompt. In a systematic evaluation of six\nstate-of-the-art unlearning methods, we find that they consistently impair such\ncontextual utility. To address this, we augment unlearning objectives with a\nplug-in term that preserves the model's ability to use forgotten knowledge when\nit is present in context. Extensive experiments demonstrate that our approach\nrestores contextual utility to near original levels while still maintaining\neffective forgetting and retain-set utility.",
      "authors": [
        {
          "name": "Yuefeng Peng",
          "affiliation": null
        },
        {
          "name": "Parnian Afshar",
          "affiliation": null
        },
        {
          "name": "Megan Ganji",
          "affiliation": null
        },
        {
          "name": "Thomas Butler",
          "affiliation": null
        },
        {
          "name": "Amir Houmansadr",
          "affiliation": null
        },
        {
          "name": "Mingxian Wang",
          "affiliation": null
        },
        {
          "name": "Dezhi Hong",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17620v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17620v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17617v1",
      "title": "ImaGGen: Zero-Shot Generation of Co-Speech Semantic Gestures Grounded in Language and Image Input",
      "abstract": "Human communication combines speech with expressive nonverbal cues such as\nhand gestures that serve manifold communicative functions. Yet, current\ngenerative gesture generation approaches are restricted to simple, repetitive\nbeat gestures that accompany the rhythm of speaking but do not contribute to\ncommunicating semantic meaning. This paper tackles a core challenge in\nco-speech gesture synthesis: generating iconic or deictic gestures that are\nsemantically coherent with a verbal utterance. Such gestures cannot be derived\nfrom language input alone, which inherently lacks the visual meaning that is\noften carried autonomously by gestures. We therefore introduce a zero-shot\nsystem that generates gestures from a given language input and additionally is\ninformed by imagistic input, without manual annotation or human intervention.\nOur method integrates an image analysis pipeline that extracts key object\nproperties such as shape, symmetry, and alignment, together with a semantic\nmatching module that links these visual details to spoken text. An inverse\nkinematics engine then synthesizes iconic and deictic gestures and combines\nthem with co-generated natural beat gestures for coherent multimodal\ncommunication. A comprehensive user study demonstrates the effectiveness of our\napproach. In scenarios where speech alone was ambiguous, gestures generated by\nour system significantly improved participants' ability to identify object\nproperties, confirming their interpretability and communicative value. While\nchallenges remain in representing complex shapes, our results highlight the\nimportance of context-aware semantic gestures for creating expressive and\ncollaborative virtual agents or avatars, marking a substantial step forward\ntowards efficient and robust, embodied human-agent interaction. More\ninformation and example videos are available here:\nhttps://review-anon-io.github.io/ImaGGen.github.io/",
      "authors": [
        {
          "name": "Hendric Voss",
          "affiliation": null
        },
        {
          "name": "Stefan Kopp",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.HC",
        "cs.CV"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17617v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17617v1",
      "primary_category": "cs.HC",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17614v1",
      "title": "OG-Rank: Learning to Rank Fast and Slow with Uncertainty and Reward-Trend Guided Adaptive Exploration",
      "abstract": "Clinicians need ranking systems that work in real time and still justify\ntheir choices. Motivated by the need for a low-latency, decoder-based reranker,\nwe present OG-Rank, a single-decoder approach that pairs a pooled first-token\nscoring signal with an uncertainty-gated explanation step. The model scores all\ncandidates in one pass and generates a brief, structured rationale only when\nthe list is genuinely ambiguous, keeping latency predictable. Trained with a\ncurriculum that concentrates effort on hard cases, OG-Rank delivers strong\neffectiveness on encounter-scoped order selection (fast path: Recall@1~0.45,\nnDCG@20~0.625) and improves further when the gate activates (Recall@1~0.56,\nnDCG@20~0.699 at a 45\\% gate rate), while compact backbones show similar gains\nunder the same policy. Encoder baselines trail in both effectiveness and\nflexibility. The result is a practical recipe: rank fast by default and explain\nwhen it helps, a pattern that applies broadly to decision tasks where selective\ngeneration buys accuracy at acceptable cost. The single-policy design\nsimplifies deployment and budget planning, and the curriculum principle (spend\nmore on the hard cases, less on the easy ones) readily transfers beyond\nclinical order selection.",
      "authors": [
        {
          "name": "Praphul Singh",
          "affiliation": null
        },
        {
          "name": "Corey Barrett",
          "affiliation": null
        },
        {
          "name": "Sumana Srivasta",
          "affiliation": null
        },
        {
          "name": "Irfan Bulu",
          "affiliation": null
        },
        {
          "name": "Sri Gadde",
          "affiliation": null
        },
        {
          "name": "Krishnaram Kenthapadi",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.AI",
        "cs.IR"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17614v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17614v1",
      "primary_category": "cs.AI",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17611v1",
      "title": "One Dinomaly2 Detect Them All: A Unified Framework for Full-Spectrum Unsupervised Anomaly Detection",
      "abstract": "Unsupervised anomaly detection (UAD) has evolved from building specialized\nsingle-class models to unified multi-class models, yet existing multi-class\nmodels significantly underperform the most advanced one-for-one counterparts.\nMoreover, the field has fragmented into specialized methods tailored to\nspecific scenarios (multi-class, 3D, few-shot, etc.), creating deployment\nbarriers and highlighting the need for a unified solution. In this paper, we\npresent Dinomaly2, the first unified framework for full-spectrum image UAD,\nwhich bridges the performance gap in multi-class models while seamlessly\nextending across diverse data modalities and task settings. Guided by the \"less\nis more\" philosophy, we demonstrate that the orchestration of five simple\nelement achieves superior performance in a standard reconstruction-based\nframework. This methodological minimalism enables natural extension across\ndiverse tasks without modification, establishing that simplicity is the\nfoundation of true universality. Extensive experiments on 12 UAD benchmarks\ndemonstrate Dinomaly2's full-spectrum superiority across multiple modalities\n(2D, multi-view, RGB-3D, RGB-IR), task settings (single-class, multi-class,\ninference-unified multi-class, few-shot) and application domains (industrial,\nbiological, outdoor). For example, our multi-class model achieves unprecedented\n99.9% and 99.3% image-level (I-) AUROC on MVTec-AD and VisA respectively. For\nmulti-view and multi-modal inspection, Dinomaly2 demonstrates state-of-the-art\nperformance with minimum adaptations. Moreover, using only 8 normal examples\nper class, our method surpasses previous full-shot models, achieving 98.7% and\n97.4% I-AUROC on MVTec-AD and VisA. The combination of minimalistic design,\ncomputational scalability, and universal applicability positions Dinomaly2 as a\nunified solution for the full spectrum of real-world anomaly detection\napplications.",
      "authors": [
        {
          "name": "Jia Guo",
          "affiliation": null
        },
        {
          "name": "Shuai Lu",
          "affiliation": null
        },
        {
          "name": "Lei Fan",
          "affiliation": null
        },
        {
          "name": "Zelin Li",
          "affiliation": null
        },
        {
          "name": "Donglin Di",
          "affiliation": null
        },
        {
          "name": "Yang Song",
          "affiliation": null
        },
        {
          "name": "Weihang Zhang",
          "affiliation": null
        },
        {
          "name": "Wenbing Zhu",
          "affiliation": null
        },
        {
          "name": "Hong Yan",
          "affiliation": null
        },
        {
          "name": "Fang Chen",
          "affiliation": null
        },
        {
          "name": "Huiqi Li",
          "affiliation": null
        },
        {
          "name": "Hongen Liao",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17611v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17611v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17609v1",
      "title": "Integrating BIM and UAV-based photogrammetry for Automated 3D Structure Model Segmentation",
      "abstract": "The advancement of UAV technology has enabled efficient, non-contact\nstructural health monitoring. Combined with photogrammetry, UAVs can capture\nhigh-resolution scans and reconstruct detailed 3D models of infrastructure.\nHowever, a key challenge remains in segmenting specific structural components\nfrom these models-a process traditionally reliant on time-consuming and\nerror-prone manual labeling. To address this issue, we propose a machine\nlearning-based framework for automated segmentation of 3D point clouds. Our\napproach uses the complementary strengths of real-world UAV-scanned point\nclouds and synthetic data generated from Building Information Modeling (BIM) to\novercome the limitations associated with manual labeling. Validation on a\nrailroad track dataset demonstrated high accuracy in identifying and segmenting\nmajor components such as rails and crossties. Moreover, by using smaller-scale\ndatasets supplemented with BIM data, the framework significantly reduced\ntraining time while maintaining reasonable segmentation accuracy. This\nautomated approach improves the precision and efficiency of 3D infrastructure\nmodel segmentation and advances the integration of UAV and BIM technologies in\nstructural health monitoring and infrastructure management.",
      "authors": [
        {
          "name": "Siqi Chen",
          "affiliation": null
        },
        {
          "name": "Shanyue Guan",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17609v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17609v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17608v1",
      "title": "Non-asymptotic error bounds for probability flow ODEs under weak log-concavity",
      "abstract": "Score-based generative modeling, implemented through probability flow ODEs,\nhas shown impressive results in numerous practical settings. However, most\nconvergence guarantees rely on restrictive regularity assumptions on the target\ndistribution -- such as strong log-concavity or bounded support. This work\nestablishes non-asymptotic convergence bounds in the 2-Wasserstein distance for\na general class of probability flow ODEs under considerably weaker assumptions:\nweak log-concavity and Lipschitz continuity of the score function. Our\nframework accommodates non-log-concave distributions, such as Gaussian\nmixtures, and explicitly accounts for initialization errors, score\napproximation errors, and effects of discretization via an exponential\nintegrator scheme. Bridging a key theoretical challenge in diffusion-based\ngenerative modeling, our results extend convergence theory to more realistic\ndata distributions and practical ODE solvers. We provide concrete guarantees\nfor the efficiency and correctness of the sampling algorithm, complementing the\nempirical success of diffusion models with rigorous theory. Moreover, from a\npractical perspective, our explicit rates might be helpful in choosing\nhyperparameters, such as the step size in the discretization.",
      "authors": [
        {
          "name": "Gitte Kremling",
          "affiliation": null
        },
        {
          "name": "Francesco Iafrate",
          "affiliation": null
        },
        {
          "name": "Mahsa Taheri",
          "affiliation": null
        },
        {
          "name": "Johannes Lederer",
          "affiliation": null
        }
      ],
      "categories": [
        "stat.ML",
        "cs.LG",
        "math.ST",
        "stat.TH"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17608v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17608v1",
      "primary_category": "stat.ML",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17603v1",
      "title": "ShapeCraft: LLM Agents for Structured, Textured and Interactive 3D Modeling",
      "abstract": "3D generation from natural language offers significant potential to reduce\nexpert manual modeling efforts and enhance accessibility to 3D assets. However,\nexisting methods often yield unstructured meshes and exhibit poor\ninteractivity, making them impractical for artistic workflows. To address these\nlimitations, we represent 3D assets as shape programs and introduce ShapeCraft,\na novel multi-agent framework for text-to-3D generation. At its core, we\npropose a Graph-based Procedural Shape (GPS) representation that decomposes\ncomplex natural language into a structured graph of sub-tasks, thereby\nfacilitating accurate LLM comprehension and interpretation of spatial\nrelationships and semantic shape details. Specifically, LLM agents\nhierarchically parse user input to initialize GPS, then iteratively refine\nprocedural modeling and painting to produce structured, textured, and\ninteractive 3D assets. Qualitative and quantitative experiments demonstrate\nShapeCraft's superior performance in generating geometrically accurate and\nsemantically rich 3D assets compared to existing LLM-based agents. We further\nshow the versatility of ShapeCraft through examples of animated and\nuser-customized editing, highlighting its potential for broader interactive\napplications.",
      "authors": [
        {
          "name": "Shuyuan Zhang",
          "affiliation": null
        },
        {
          "name": "Chenhan Jiang",
          "affiliation": null
        },
        {
          "name": "Zuoou Li",
          "affiliation": null
        },
        {
          "name": "Jiankang Deng",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17603v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17603v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17602v1",
      "title": "LawChain: Modeling Legal Reasoning Chains for Chinese Tort Case Analysis",
      "abstract": "Legal reasoning is a fundamental component of legal analysis and\ndecision-making. Existing computational approaches to legal reasoning\npredominantly rely on generic reasoning frameworks such as syllogism and IRAC,\nwhich do not comprehensively examine the nuanced processes that underpin legal\nreasoning. Moreover, current research has largely focused on criminal cases,\nwith insufficient modeling for civil cases. In this work, we present a novel\nframework for explicitly modeling legal reasoning in the analysis of Chinese\ntort-related civil cases. We first operationalize the legal reasoning processes\nused in tort analysis into the LawChain framework. LawChain is a three-module\nreasoning framework, with each module consisting of multiple finer-grained\nsub-steps. Informed by the LawChain framework, we introduce the task of tort\nlegal reasoning and construct an evaluation benchmark, LawChain$_{eval}$, to\nsystematically assess the critical steps within analytical reasoning chains for\ntort analysis. Leveraging this benchmark, we evaluate state-of-the-art large\nlanguage models for their legal reasoning ability in civil tort contexts. Our\nresults indicate that current models still fall short in accurately handling\ncrucial elements of tort legal reasoning. Furthermore, we introduce several\nbaseline approaches that explicitly incorporate LawChain-style reasoning\nthrough prompting or post-training. We conduct further experiments on\nadditional legal analysis tasks, such as Legal Named-Entity Recognition and\nCriminal Damages Calculation, to verify the generalizability of these\nbaselines. The proposed baseline approaches achieve significant improvements in\ntort-related legal reasoning and generalize well to related legal analysis\ntasks, thus demonstrating the value of explicitly modeling legal reasoning\nchains to enhance the reasoning capabilities of language models.",
      "authors": [
        {
          "name": "Huiyuan Xie",
          "affiliation": null
        },
        {
          "name": "Chenyang Li",
          "affiliation": null
        },
        {
          "name": "Huining Zhu",
          "affiliation": null
        },
        {
          "name": "Chubin Zhang",
          "affiliation": null
        },
        {
          "name": "Yuxiao Ye",
          "affiliation": null
        },
        {
          "name": "Zhenghao Liu",
          "affiliation": null
        },
        {
          "name": "Zhiyuan Liu",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17602v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17602v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17599v1",
      "title": "Conveying Meaning through Gestures: An Investigation into Semantic Co-Speech Gesture Generation",
      "abstract": "This study explores two frameworks for co-speech gesture generation, AQ-GT\nand its semantically-augmented variant AQ-GT-a, to evaluate their ability to\nconvey meaning through gestures and how humans perceive the resulting\nmovements. Using sentences from the SAGA spatial communication corpus,\ncontextually similar sentences, and novel movement-focused sentences, we\nconducted a user-centered evaluation of concept recognition and human-likeness.\nResults revealed a nuanced relationship between semantic annotations and\nperformance. The original AQ-GT framework, lacking explicit semantic input, was\nsurprisingly more effective at conveying concepts within its training domain.\nConversely, the AQ-GT-a framework demonstrated better generalization,\nparticularly for representing shape and size in novel contexts. While\nparticipants rated gestures from AQ-GT-a as more expressive and helpful, they\ndid not perceive them as more human-like. These findings suggest that explicit\nsemantic enrichment does not guarantee improved gesture generation and that its\neffectiveness is highly dependent on the context, indicating a potential\ntrade-off between specialization and generalization.",
      "authors": [
        {
          "name": "Hendric Voss",
          "affiliation": null
        },
        {
          "name": "Lisa Michelle Bohnenkamp",
          "affiliation": null
        },
        {
          "name": "Stefan Kopp",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.HC",
        "cs.CV"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17599v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17599v1",
      "primary_category": "cs.HC",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17598v1",
      "title": "Reasoning Distillation and Structural Alignment for Improved Code Generation",
      "abstract": "Effective code generation with language models hinges on two critical\nfactors: accurately understanding the intent of the prompt and generating code\nthat applies algorithmic reasoning to produce correct solutions capable of\npassing diverse test cases while adhering to the syntax of the target\nprogramming language. Unlike other language tasks, code generation requires\nmore than accurate token prediction; it demands comprehension of solution-level\nand structural relationships rather than merely generating the most likely\ntokens. very large language model (VLLM) are capable of generating detailed\nsteps toward the correct solution of complex tasks where reasoning is crucial\nin solving the problem. Such reasoning capabilities may be absent in smaller\nlanguage models. Therefore, in this work, we distill the reasoning capabilities\nof a VLLM into a smaller, more efficient model that is faster and cheaper to\ndeploy. Our approach trains the model to emulate the reasoning and\nproblem-solving abilities of the VLLM by learning to identify correct solution\npathways and establishing a structural correspondence between problem\ndefinitions and potential solutions through a novel method of structure-aware\nloss optimization. This enables the model to transcend token-level generation\nand to deeply grasp the overarching structure of solutions for given problems.\nExperimental results show that our fine-tuned model, developed through a cheap\nand simple to implement process, significantly outperforms our baseline model\nin terms of pass@1, average data flow, and average syntax match metrics across\nthe MBPP, MBPP Plus, and HumanEval benchmarks.",
      "authors": [
        {
          "name": "Amir Jalilifard",
          "affiliation": null
        },
        {
          "name": "Anderson de Rezende Rocha",
          "affiliation": null
        },
        {
          "name": "Marcos Medeiros Raimundo",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17598v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17598v1",
      "primary_category": "cs.AI",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17591v1",
      "title": "HGAdapter: Hypergraph-based Adapters in Language Models for Code Summarization and Clone Detection",
      "abstract": "Pre-trained language models (PLMs) are increasingly being applied to\ncode-related tasks. Although PLMs have achieved good results, they do not take\ninto account potential high-order data correlations within the code. We propose\nthree types of high-order correlations in code tokens, i.e. abstract syntax\ntree family correlation, lexical correlation, and line correlation. We design a\ntokens and hyperedges generator to capture these high-order data correlations.\nWe improve the architecture of hypergraph neural networks and combine it with\nadapter tuning to propose a novel hypergraph-based adapter (HGAdapter) to\nfine-tune PLMs. HGAdapter can encode high-order data correlations and is\nallowed to be inserted into various PLMs to enhance performance. Experiments\nwere conducted on several public datasets, including six languages of code\nsummarization and code clone detection tasks. Our methods improved the\nperformance of PLMs in datasets to varying degrees. Experimental results\nvalidate the introduction of high-order data correlations that contribute to\nimproved effectiveness.",
      "authors": [
        {
          "name": "Guang Yang",
          "affiliation": null
        },
        {
          "name": "Yujie Zhu",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.SE"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17591v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17591v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17590v1",
      "title": "MIRAGE: Agentic Framework for Multimodal Misinformation Detection with Web-Grounded Reasoning",
      "abstract": "Misinformation spreads across web platforms through billions of daily\nmultimodal posts that combine text and images, overwhelming manual\nfact-checking capacity. Supervised detection models require domain-specific\ntraining data and fail to generalize across diverse manipulation tactics. We\npresent MIRAGE, an inference-time, model-pluggable agentic framework that\ndecomposes multimodal verification into four sequential modules: visual\nveracity assessment detects AI-generated images, cross-modal consistency\nanalysis identifies out-of-context repurposing, retrieval-augmented factual\nchecking grounds claims in web evidence through iterative question generation,\nand a calibrated judgment module integrates all signals. MIRAGE orchestrates\nvision-language model reasoning with targeted web retrieval, outputs structured\nand citation-linked rationales. On MMFakeBench validation set (1,000 samples),\nMIRAGE with GPT-4o-mini achieves 81.65% F1 and 75.1% accuracy, outperforming\nthe strongest zero-shot baseline (GPT-4V with MMD-Agent at 74.0% F1) by 7.65\npoints while maintaining 34.3% false positive rate versus 97.3% for a\njudge-only baseline. Test set results (5,000 samples) confirm generalization\nwith 81.44% F1 and 75.08% accuracy. Ablation studies show visual verification\ncontributes 5.18 F1 points and retrieval-augmented reasoning contributes 2.97\npoints. Our results demonstrate that decomposed agentic reasoning with web\nretrieval can match supervised detector performance without domain-specific\ntraining, enabling misinformation detection across modalities where labeled\ndata remains scarce.",
      "authors": [
        {
          "name": "Mir Nafis Sharear Shopnil",
          "affiliation": null
        },
        {
          "name": "Sharad Duwal",
          "affiliation": null
        },
        {
          "name": "Abhishek Tyagi",
          "affiliation": null
        },
        {
          "name": "Adiba Mahbub Proma",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.CY",
        "cs.LG",
        "I.2.7; H.3.3; I.4.9"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17590v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17590v1",
      "primary_category": "cs.AI",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17585v1",
      "title": "Expose Camouflage in the Water: Underwater Camouflaged Instance Segmentation and Dataset",
      "abstract": "With the development of underwater exploration and marine protection,\nunderwater vision tasks are widespread. Due to the degraded underwater\nenvironment, characterized by color distortion, low contrast, and blurring,\ncamouflaged instance segmentation (CIS) faces greater challenges in accurately\nsegmenting objects that blend closely with their surroundings. Traditional\ncamouflaged instance segmentation methods, trained on terrestrial-dominated\ndatasets with limited underwater samples, may exhibit inadequate performance in\nunderwater scenes. To address these issues, we introduce the first underwater\ncamouflaged instance segmentation (UCIS) dataset, abbreviated as UCIS4K, which\ncomprises 3,953 images of camouflaged marine organisms with instance-level\nannotations. In addition, we propose an Underwater Camouflaged Instance\nSegmentation network based on Segment Anything Model (UCIS-SAM). Our UCIS-SAM\nincludes three key modules. First, the Channel Balance Optimization Module\n(CBOM) enhances channel characteristics to improve underwater feature learning,\neffectively addressing the model's limited understanding of underwater\nenvironments. Second, the Frequency Domain True Integration Module (FDTIM) is\nproposed to emphasize intrinsic object features and reduce interference from\ncamouflage patterns, enhancing the segmentation performance of camouflaged\nobjects blending with their surroundings. Finally, the Multi-scale Feature\nFrequency Aggregation Module (MFFAM) is designed to strengthen the boundaries\nof low-contrast camouflaged instances across multiple frequency bands,\nimproving the model's ability to achieve more precise segmentation of\ncamouflaged objects. Extensive experiments on the proposed UCIS4K and public\nbenchmarks show that our UCIS-SAM outperforms state-of-the-art approaches.",
      "authors": [
        {
          "name": "Chuhong Wang",
          "affiliation": null
        },
        {
          "name": "Hua Li",
          "affiliation": null
        },
        {
          "name": "Chongyi Li",
          "affiliation": null
        },
        {
          "name": "Huazhong Liu",
          "affiliation": null
        },
        {
          "name": "Xiongxin Tang",
          "affiliation": null
        },
        {
          "name": "Sam Kwong",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17585v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17585v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17584v1",
      "title": "CEPerFed: Communication-Efficient Personalized Federated Learning for Multi-Pulse MRI Classification",
      "abstract": "Multi-pulse magnetic resonance imaging (MRI) is widely utilized for clinical\npractice such as Alzheimer's disease diagnosis. To train a robust model for\nmulti-pulse MRI classification, it requires large and diverse data from various\nmedical institutions while protecting privacy by preventing raw data sharing\nacross institutions. Although federated learning (FL) is a feasible solution to\naddress this issue, it poses challenges of model convergence due to the effect\nof data heterogeneity and substantial communication overhead due to large\nnumbers of parameters transmitted within the model. To address these\nchallenges, we propose CEPerFed, a communication-efficient personalized FL\nmethod. It mitigates the effect of data heterogeneity by incorporating\nclient-side historical risk gradients and historical mean gradients to\ncoordinate local and global optimization. The former is used to weight the\ncontributions from other clients, enhancing the reliability of local updates,\nwhile the latter enforces consistency between local updates and the global\noptimization direction to ensure stable convergence across heterogeneous data\ndistributions. To address the high communication overhead, we propose a\nhierarchical SVD (HSVD) strategy that transmits only the most critical\ninformation required for model updates. Experiments on five classification\ntasks demonstrate the effectiveness of the CEPerFed method. The code will be\nreleased upon acceptance at https://github.com/LD0416/CEPerFed.",
      "authors": [
        {
          "name": "Ludi Li",
          "affiliation": null
        },
        {
          "name": "Junbin Mao",
          "affiliation": null
        },
        {
          "name": "Hanhe Lin",
          "affiliation": null
        },
        {
          "name": "Xu Tian",
          "affiliation": null
        },
        {
          "name": "Fang-Xiang Wu",
          "affiliation": null
        },
        {
          "name": "Jin Liu",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17584v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17584v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17576v1",
      "title": "Intent-Driven LLM Ensemble Planning for Flexible Multi-Robot Disassembly: Demonstration on EV Batteries",
      "abstract": "This paper addresses the problem of planning complex manipulation tasks, in\nwhich multiple robots with different end-effectors and capabilities, informed\nby computer vision, must plan and execute concatenated sequences of actions on\na variety of objects that can appear in arbitrary positions and configurations\nin unstructured scenes. We propose an intent-driven planning pipeline which can\nrobustly construct such action sequences with varying degrees of supervisory\ninput from a human using simple language instructions. The pipeline integrates:\n(i) perception-to-text scene encoding, (ii) an ensemble of large language\nmodels (LLMs) that generate candidate removal sequences based on the operator's\nintent, (iii) an LLM-based verifier that enforces formatting and precedence\nconstraints, and (iv) a deterministic consistency filter that rejects\nhallucinated objects. The pipeline is evaluated on an example task in which two\nrobot arms work collaboratively to dismantle an Electric Vehicle battery for\nrecycling applications. A variety of components must be grasped and removed in\nspecific sequences, determined by human instructions and/or by task-order\nfeasibility decisions made by the autonomous system. On 200 real scenes with\n600 operator prompts across five component classes, we used metrics of\nfull-sequence correctness and next-task correctness to evaluate and compare\nfive LLM-based planners (including ablation analyses of pipeline components).\nWe also evaluated the LLM-based human interface in terms of time to execution\nand NASA TLX with human participant experiments. Results indicate that our\nensemble-with-verification approach reliably maps operator intent to safe,\nexecutable multi-robot plans while maintaining low user effort.",
      "authors": [
        {
          "name": "Cansu Erdogan",
          "affiliation": null
        },
        {
          "name": "Cesar Alan Contreras",
          "affiliation": null
        },
        {
          "name": "Alireza Rastegarpanah",
          "affiliation": null
        },
        {
          "name": "Manolis Chiou",
          "affiliation": null
        },
        {
          "name": "Rustam Stolkin",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.HC",
        "cs.MA"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17576v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17576v1",
      "primary_category": "cs.RO",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17569v1",
      "title": "Semi-supervised Latent Bayesian Optimization for Designing Antimicrobial Peptides",
      "abstract": "Antimicrobial peptides (AMPs) are a promising class of therapeutics to treat\nbacterial infections. Discovering and designing such peptides is difficult\nbecause of the vast number of possible sequences of amino acids. Deep\ngenerative models, such as variational autoencoders, have shown value in\npeptide design due to their ability to model sequence space with a\ncontinuous-valued latent space. Although such models have already been used to\ngreat effect in biomolecular design, they still suffer from a lack of\ninterpretability and rigorous quantification of latent space quality as a\nsearch space. We investigate (1) whether further compression of the design\nspace via dimensionality reduction may facilitate optimization, (2) the\ninterpretability of the spaces, and (3) how organizing latent spaces with\nphysicochemical properties may improve the efficiency of optimizing\nantimicrobial activity. We find that further reduction of the latent space via\ndimensionality reduction can be advantageous when organizing the space with\nmore relevant information at data availability, that using the dimensionality\nreduction search space can be more interpretable, and that we can organize the\nlatent space with different physicochemical properties even at different\npercentages of available labels.",
      "authors": [
        {
          "name": "Jyler Menard",
          "affiliation": null
        },
        {
          "name": "R. A. Mansbach",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "physics.comp-ph"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17569v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17569v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17568v1",
      "title": "PAGE-4D: Disentangled Pose and Geometry Estimation for 4D Perception",
      "abstract": "Recent 3D feed-forward models, such as the Visual Geometry Grounded\nTransformer (VGGT), have shown strong capability in inferring 3D attributes of\nstatic scenes. However, since they are typically trained on static datasets,\nthese models often struggle in real-world scenarios involving complex dynamic\nelements, such as moving humans or deformable objects like umbrellas. To\naddress this limitation, we introduce PAGE-4D, a feedforward model that extends\nVGGT to dynamic scenes, enabling camera pose estimation, depth prediction, and\npoint cloud reconstruction -- all without post-processing. A central challenge\nin multi-task 4D reconstruction is the inherent conflict between tasks:\naccurate camera pose estimation requires suppressing dynamic regions, while\ngeometry reconstruction requires modeling them. To resolve this tension, we\npropose a dynamics-aware aggregator that disentangles static and dynamic\ninformation by predicting a dynamics-aware mask -- suppressing motion cues for\npose estimation while amplifying them for geometry reconstruction. Extensive\nexperiments show that PAGE-4D consistently outperforms the original VGGT in\ndynamic scenarios, achieving superior results in camera pose estimation,\nmonocular and video depth estimation, and dense point map reconstruction.",
      "authors": [
        {
          "name": "Kaichen Zhou",
          "affiliation": null
        },
        {
          "name": "Yuhan Wang",
          "affiliation": null
        },
        {
          "name": "Grace Chen",
          "affiliation": null
        },
        {
          "name": "Xinhai Chang",
          "affiliation": null
        },
        {
          "name": "Gaspard Beaudouin",
          "affiliation": null
        },
        {
          "name": "Fangneng Zhan",
          "affiliation": null
        },
        {
          "name": "Paul Pu Liang",
          "affiliation": null
        },
        {
          "name": "Mengyu Wang",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17568v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17568v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17566v1",
      "title": "WP-CrackNet: A Collaborative Adversarial Learning Framework for End-to-End Weakly-Supervised Road Crack Detection",
      "abstract": "Road crack detection is essential for intelligent infrastructure maintenance\nin smart cities. To reduce reliance on costly pixel-level annotations, we\npropose WP-CrackNet, an end-to-end weakly-supervised method that trains with\nonly image-level labels for pixel-wise crack detection. WP-CrackNet integrates\nthree components: a classifier generating class activation maps (CAMs), a\nreconstructor measuring feature inferability, and a detector producing\npixel-wise road crack detection results. During training, the classifier and\nreconstructor alternate in adversarial learning to encourage crack CAMs to\ncover complete crack regions, while the detector learns from pseudo labels\nderived from post-processed crack CAMs. This mutual feedback among the three\ncomponents improves learning stability and detection accuracy. To further boost\ndetection performance, we design a path-aware attention module (PAAM) that\nfuses high-level semantics from the classifier with low-level structural cues\nfrom the reconstructor by modeling spatial and channel-wise dependencies.\nAdditionally, a center-enhanced CAM consistency module (CECCM) is proposed to\nrefine crack CAMs using center Gaussian weighting and consistency constraints,\nenabling better pseudo-label generation. We create three image-level datasets\nand extensive experiments show that WP-CrackNet achieves comparable results to\nsupervised methods and outperforms existing weakly-supervised methods,\nsignificantly advancing scalable road inspection. The source code package and\ndatasets are available at https://mias.group/WP-CrackNet/.",
      "authors": [
        {
          "name": "Nachuan Ma",
          "affiliation": null
        },
        {
          "name": "Zhengfei Song",
          "affiliation": null
        },
        {
          "name": "Qiang Hu",
          "affiliation": null
        },
        {
          "name": "Xiaoyu Tang",
          "affiliation": null
        },
        {
          "name": "Chengxi Zhang",
          "affiliation": null
        },
        {
          "name": "Rui Fan",
          "affiliation": null
        },
        {
          "name": "Lihua Xie",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17566v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17566v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17564v1",
      "title": "An Empirical Study of Lagrangian Methods in Safe Reinforcement Learning",
      "abstract": "In safety-critical domains such as robotics, navigation and power systems,\nconstrained optimization problems arise where maximizing performance must be\ncarefully balanced with associated constraints. Safe reinforcement learning\nprovides a framework to address these challenges, with Lagrangian methods being\na popular choice. However, the effectiveness of Lagrangian methods crucially\ndepends on the choice of the Lagrange multiplier $\\lambda$, which governs the\ntrade-off between return and constraint cost. A common approach is to update\nthe multiplier automatically during training. Although this is standard in\npractice, there remains limited empirical evidence on the robustness of an\nautomated update and its influence on overall performance. Therefore, we\nanalyze (i) optimality and (ii) stability of Lagrange multipliers in safe\nreinforcement learning across a range of tasks. We provide $\\lambda$-profiles\nthat give a complete visualization of the trade-off between return and\nconstraint cost of the optimization problem. These profiles show the highly\nsensitive nature of $\\lambda$ and moreover confirm the lack of general\nintuition for choosing the optimal value $\\lambda^*$. Our findings additionally\nshow that automated multiplier updates are able to recover and sometimes even\nexceed the optimal performance found at $\\lambda^*$ due to the vast difference\nin their learning trajectories. Furthermore, we show that automated multiplier\nupdates exhibit oscillatory behavior during training, which can be mitigated\nthrough PID-controlled updates. However, this method requires careful tuning to\nachieve consistently better performance across tasks. This highlights the need\nfor further research on stabilizing Lagrangian methods in safe reinforcement\nlearning. The code used to reproduce our results can be found at\nhttps://github.com/lindsayspoor/Lagrangian_SafeRL.",
      "authors": [
        {
          "name": "Lindsay Spoor",
          "affiliation": null
        },
        {
          "name": "\u00c1lvaro Serra-G\u00f3mez",
          "affiliation": null
        },
        {
          "name": "Aske Plaat",
          "affiliation": null
        },
        {
          "name": "Thomas Moerland",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO",
        "cs.SY",
        "eess.SY"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17564v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17564v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17562v1",
      "title": "Formally Exploring Time-Series Anomaly Detection Evaluation Metrics",
      "abstract": "Undetected anomalies in time series can trigger catastrophic failures in\nsafety-critical systems, such as chemical plant explosions or power grid\noutages. Although many detection methods have been proposed, their performance\nremains unclear because current metrics capture only narrow aspects of the task\nand often yield misleading results. We address this issue by introducing\nverifiable properties that formalize essential requirements for evaluating\ntime-series anomaly detection. These properties enable a theoretical framework\nthat supports principled evaluations and reliable comparisons. Analyzing 37\nwidely used metrics, we show that most satisfy only a few properties, and none\nsatisfy all, explaining persistent inconsistencies in prior results. To close\nthis gap, we propose LARM, a flexible metric that provably satisfies all\nproperties, and extend it to ALARM, an advanced variant meeting stricter\nrequirements.",
      "authors": [
        {
          "name": "Dennis Wagner",
          "affiliation": null
        },
        {
          "name": "Arjun Nair",
          "affiliation": null
        },
        {
          "name": "Billy Joe Franks",
          "affiliation": null
        },
        {
          "name": "Justus Arweiler",
          "affiliation": null
        },
        {
          "name": "Aparna Muraleedharan",
          "affiliation": null
        },
        {
          "name": "Indra Jungjohann",
          "affiliation": null
        },
        {
          "name": "Fabian Hartung",
          "affiliation": null
        },
        {
          "name": "Mayank C. Ahuja",
          "affiliation": null
        },
        {
          "name": "Andriy Balinskyy",
          "affiliation": null
        },
        {
          "name": "Saurabh Varshneya",
          "affiliation": null
        },
        {
          "name": "Nabeel Hussain Syed",
          "affiliation": null
        },
        {
          "name": "Mayank Nagda",
          "affiliation": null
        },
        {
          "name": "Phillip Liznerski",
          "affiliation": null
        },
        {
          "name": "Steffen Reithermann",
          "affiliation": null
        },
        {
          "name": "Maja Rudolph",
          "affiliation": null
        },
        {
          "name": "Sebastian Vollmer",
          "affiliation": null
        },
        {
          "name": "Ralf Schulz",
          "affiliation": null
        },
        {
          "name": "Torsten Katz",
          "affiliation": null
        },
        {
          "name": "Stephan Mandt",
          "affiliation": null
        },
        {
          "name": "Michael Bortz",
          "affiliation": null
        },
        {
          "name": "Heike Leitte",
          "affiliation": null
        },
        {
          "name": "Daniel Neider",
          "affiliation": null
        },
        {
          "name": "Jakob Burger",
          "affiliation": null
        },
        {
          "name": "Fabian Jirasek",
          "affiliation": null
        },
        {
          "name": "Hans Hasse",
          "affiliation": null
        },
        {
          "name": "Sophie Fellenz",
          "affiliation": null
        },
        {
          "name": "Marius Kloft",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17562v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17562v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17561v1",
      "title": "Spectral Thresholds in Correlated Spiked Models and Fundamental Limits of Partial Least Squares",
      "abstract": "We provide a rigorous random matrix theory analysis of spiked\ncross-covariance models where the signals across two high-dimensional data\nchannels are partially aligned. These models are motivated by multi-modal\nlearning and form the standard generative setting underlying Partial Least\nSquares (PLS), a widely used yet theoretically underdeveloped method. We show\nthat the leading singular values of the sample cross-covariance matrix undergo\na Baik-Ben Arous-Peche (BBP)-type phase transition, and we characterize the\nprecise thresholds for the emergence of informative components. Our results\nyield the first sharp asymptotic description of the signal recovery\ncapabilities of PLS in this setting, revealing a fundamental performance gap\nbetween PLS and the Bayes-optimal estimator. In particular, we identify the SNR\nand correlation regimes where PLS fails to recover any signal, despite\ndetectability being possible in principle. These findings clarify the\ntheoretical limits of PLS and provide guidance for the design of reliable\nmulti-modal inference methods in high dimensions.",
      "authors": [
        {
          "name": "Pierre Mergny",
          "affiliation": null
        },
        {
          "name": "Lenka Zdeborov\u00e1",
          "affiliation": null
        }
      ],
      "categories": [
        "math.ST",
        "cond-mat.dis-nn",
        "stat.ML",
        "stat.TH"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17561v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17561v1",
      "primary_category": "math.ST",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17558v1",
      "title": "The Free Transformer",
      "abstract": "We propose an extension of the decoder Transformer that conditions its\ngenerative process on random latent variables which are learned without\nsupervision thanks to a variational procedure. Experimental evaluations show\nthat allowing such a conditioning translates into substantial improvements on\ndownstream tasks.",
      "authors": [
        {
          "name": "Fran\u00e7ois Fleuret",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17558v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17558v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17555v1",
      "title": "Language Confusion Gate: Language-Aware Decoding Through Model Self-Distillation",
      "abstract": "Large language models (LLMs) often experience language confusion, which is\nthe unintended mixing of languages during text generation. Current solutions to\nthis problem either necessitate model retraining or cannot differentiate\nbetween harmful confusion and acceptable code-switching. This paper introduces\nthe Language Confusion Gate (LCG), a lightweight, plug-in solution that filters\ntokens during decoding without altering the base LLM. The LCG is trained using\nnorm-adjusted self-distillation to predict appropriate language families and\napply masking only when needed. Our method is based on the findings that\nlanguage confusion is infrequent, correct-language tokens are usually among the\ntop predictions, and output token embedding norms are larger for high-resource\nlanguages, which biases sampling. When evaluated across various models,\nincluding Qwen3, GPT-OSS, Gemma3, Llama3.1, LCG decreases language confusion\nsignificantly, often by an order of magnitude, without negatively impacting\ntask performance. Code is available at\nhttps://github.com/collinzrj/language_confusion_gate.",
      "authors": [
        {
          "name": "Collin Zhang",
          "affiliation": null
        },
        {
          "name": "Fei Huang",
          "affiliation": null
        },
        {
          "name": "Chenhan Yuan",
          "affiliation": null
        },
        {
          "name": "Junyang Lin",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17555v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17555v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17548v1",
      "title": "When Annotators Disagree, Topology Explains: Mapper, a Topological Tool for Exploring Text Embedding Geometry and Ambiguity",
      "abstract": "Language models are often evaluated with scalar metrics like accuracy, but\nsuch measures fail to capture how models internally represent ambiguity,\nespecially when human annotators disagree. We propose a topological perspective\nto analyze how fine-tuned models encode ambiguity and more generally instances.\n  Applied to RoBERTa-Large on the MD-Offense dataset, Mapper, a tool from\ntopological data analysis, reveals that fine-tuning restructures embedding\nspace into modular, non-convex regions aligned with model predictions, even for\nhighly ambiguous cases. Over $98\\%$ of connected components exhibit $\\geq 90\\%$\nprediction purity, yet alignment with ground-truth labels drops in ambiguous\ndata, surfacing a hidden tension between structural confidence and label\nuncertainty.\n  Unlike traditional tools such as PCA or UMAP, Mapper captures this geometry\ndirectly uncovering decision regions, boundary collapses, and overconfident\nclusters. Our findings position Mapper as a powerful diagnostic tool for\nunderstanding how models resolve ambiguity. Beyond visualization, it also\nenables topological metrics that may inform proactive modeling strategies in\nsubjective NLP tasks.",
      "authors": [
        {
          "name": "Nisrine Rair",
          "affiliation": null
        },
        {
          "name": "Alban Goupil",
          "affiliation": null
        },
        {
          "name": "Valeriu Vrabie",
          "affiliation": null
        },
        {
          "name": "Emmanuel Chochoy",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17548v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17548v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17545v1",
      "title": "TrajMamba: An Efficient and Semantic-rich Vehicle Trajectory Pre-training Model",
      "abstract": "Vehicle GPS trajectories record how vehicles move over time, storing valuable\ntravel semantics, including movement patterns and travel purposes. Learning\ntravel semantics effectively and efficiently is crucial for real-world\napplications of trajectory data, which is hindered by two major challenges.\nFirst, travel purposes are tied to the functions of the roads and\npoints-of-interest (POIs) involved in a trip. Such information is encoded in\ntextual addresses and descriptions and introduces heavy computational burden to\nmodeling. Second, real-world trajectories often contain redundant points, which\nharm both computational efficiency and trajectory embedding quality. To address\nthese challenges, we propose TrajMamba, a novel approach for efficient and\nsemantically rich vehicle trajectory learning. TrajMamba introduces a\nTraj-Mamba Encoder that captures movement patterns by jointly modeling both GPS\nand road perspectives of trajectories, enabling robust representations of\ncontinuous travel behaviors. It also incorporates a Travel Purpose-aware\nPre-training procedure to integrate travel purposes into the learned embeddings\nwithout introducing extra overhead to embedding calculation. To reduce\nredundancy in trajectories, TrajMamba features a Knowledge Distillation\nPre-training scheme to identify key trajectory points through a learnable mask\ngenerator and obtain effective compressed trajectory embeddings. Extensive\nexperiments on two real-world datasets and three downstream tasks show that\nTrajMamba outperforms state-of-the-art baselines in both efficiency and\naccuracy.",
      "authors": [
        {
          "name": "Yichen Liu",
          "affiliation": null
        },
        {
          "name": "Yan Lin",
          "affiliation": null
        },
        {
          "name": "Shengnan Guo",
          "affiliation": null
        },
        {
          "name": "Zeyu Zhou",
          "affiliation": null
        },
        {
          "name": "Youfang Lin",
          "affiliation": null
        },
        {
          "name": "Huaiyu Wan",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17545v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17545v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17543v1",
      "title": "Reliable Inference in Edge-Cloud Model Cascades via Conformal Alignment",
      "abstract": "Edge intelligence enables low-latency inference via compact on-device models,\nbut assuring reliability remains challenging. We study edge-cloud cascades that\nmust preserve conditional coverage: whenever the edge returns a prediction set,\nit should contain the true label with a user-specified probability, as if\nproduced by the cloud model. We formalize conditional coverage with respect to\nthe cloud predictive distribution, and introduce a conformal alignment-based\n(CAb) cascading mechanism that certifies this property with user control over\nthe risk level. Our method casts escalation from edge to cloud models as a\nmultiple-hypothesis testing (MHT) problem, tailoring conformal alignment (CA)\nto select which inputs can be safely handled at the edge. The proposed CAb\nmodel cascading method yields statistical guarantees on the average fraction of\nedge decisions that satisfy cloud-level conditional coverage. The procedure\napplies to arbitrary edge prediction sets, including variants of conformal\nprediction (CP), and exposes a tunable trade-off among coverage, deferral rate,\nand set size. Experiments on CIFAR-100 image classification and the TeleQnA\nquestion-answering (QA) benchmark show that the proposed CAb cascade maintains\nthe target conditional coverage for edge predictions while substantially\nreducing offloading to the cloud and incurring modest increases in\nprediction-set size.",
      "authors": [
        {
          "name": "Jiayi Huang",
          "affiliation": null
        },
        {
          "name": "Sangwoo Park",
          "affiliation": null
        },
        {
          "name": "Nicola Paoletti",
          "affiliation": null
        },
        {
          "name": "Osvaldo Simeone",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "eess.SP",
        "stat.ML"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17543v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17543v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17540v1",
      "title": "Detecting streaks in smart telescopes images with Deep Learning",
      "abstract": "The growing negative impact of the visibility of satellites in the night sky\nis influencing the practice of astronomy and astrophotograph, both at the\namateur and professional levels. The presence of these satellites has the\neffect of introducing streaks into the images captured during astronomical\nobservation, requiring the application of additional post processing to\nmitigate the undesirable impact, whether for data loss or cosmetic reasons. In\nthis paper, we show how we test and adapt various Deep Learning approaches to\ndetect streaks in raw astronomical data captured between March 2022 and\nFebruary 2023 with smart telescopes.",
      "authors": [
        {
          "name": "Olivier Parisot",
          "affiliation": null
        },
        {
          "name": "Mahmoud Jaziri",
          "affiliation": null
        }
      ],
      "categories": [
        "astro-ph.IM",
        "cs.CV"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17540v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17540v1",
      "primary_category": "astro-ph.IM",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17532v1",
      "title": "OncoReason: Structuring Clinical Reasoning in LLMs for Robust and Interpretable Survival Prediction",
      "abstract": "Predicting cancer treatment outcomes requires models that are both accurate\nand interpretable, particularly in the presence of heterogeneous clinical data.\nWhile large language models (LLMs) have shown strong performance in biomedical\nNLP, they often lack structured reasoning capabilities critical for high-stakes\ndecision support. We present a unified, multi-task learning framework that\naligns autoregressive LLMs with clinical reasoning for outcome prediction on\nthe MSK-CHORD dataset. Our models are trained to jointly perform binary\nsurvival classification, continuous survival time regression, and natural\nlanguage rationale generation. We evaluate three alignment strategies: (1)\nstandard supervised fine-tuning (SFT), (2) SFT with Chain-of-Thought (CoT)\nprompting to elicit step-by-step reasoning, and (3) Group Relative Policy\nOptimization (GRPO), a reinforcement learning method that aligns model outputs\nto expert-derived reasoning trajectories. Experiments with LLaMa3-8B and\nMed42-8B backbones demonstrate that CoT prompting improves F1 by +6.0 and\nreduces MAE by 12%, while GRPO achieves state-of-the-art interpretability and\npredictive performance across BLEU, ROUGE, and BERTScore. We further show that\nexisting biomedical LLMs often fail to produce valid reasoning traces due to\narchitectural constraints. Our findings underscore the importance of\nreasoning-aware alignment in multi-task clinical modeling and set a new\nbenchmark for interpretable, trustworthy LLMs in precision oncology.",
      "authors": [
        {
          "name": "Raghu Vamshi Hemadri",
          "affiliation": null
        },
        {
          "name": "Geetha Krishna Guruju",
          "affiliation": null
        },
        {
          "name": "Kristi Topollai",
          "affiliation": null
        },
        {
          "name": "Anna Ewa Choromanska",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17532v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17532v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17531v1",
      "title": "Plasma Shape Control via Zero-shot Generative Reinforcement Learning",
      "abstract": "Traditional PID controllers have limited adaptability for plasma shape\ncontrol, and task-specific reinforcement learning (RL) methods suffer from\nlimited generalization and the need for repetitive retraining. To overcome\nthese challenges, this paper proposes a novel framework for developing a\nversatile, zero-shot control policy from a large-scale offline dataset of\nhistorical PID-controlled discharges. Our approach synergistically combines\nGenerative Adversarial Imitation Learning (GAIL) with Hilbert space\nrepresentation learning to achieve dual objectives: mimicking the stable\noperational style of the PID data and constructing a geometrically structured\nlatent space for efficient, goal-directed control. The resulting foundation\npolicy can be deployed for diverse trajectory tracking tasks in a zero-shot\nmanner without any task-specific fine-tuning. Evaluations on the HL-3 tokamak\nsimulator demonstrate that the policy excels at precisely and stably tracking\nreference trajectories for key shape parameters across a range of plasma\nscenarios. This work presents a viable pathway toward developing highly\nflexible and data-efficient intelligent control systems for future fusion\nreactors.",
      "authors": [
        {
          "name": "Niannian Wu",
          "affiliation": null
        },
        {
          "name": "Rongpeng Li",
          "affiliation": null
        },
        {
          "name": "Zongyu Yang",
          "affiliation": null
        },
        {
          "name": "Yong Xiao",
          "affiliation": null
        },
        {
          "name": "Ning Wei",
          "affiliation": null
        },
        {
          "name": "Yihang Chen",
          "affiliation": null
        },
        {
          "name": "Bo Li",
          "affiliation": null
        },
        {
          "name": "Zhifeng Zhao",
          "affiliation": null
        },
        {
          "name": "Wulyu Zhong",
          "affiliation": null
        }
      ],
      "categories": [
        "physics.plasm-ph",
        "cs.LG"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17531v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17531v1",
      "primary_category": "physics.plasm-ph",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17529v1",
      "title": "MambaX-Net: Dual-Input Mamba-Enhanced Cross-Attention Network for Longitudinal MRI Segmentation",
      "abstract": "Active Surveillance (AS) is a treatment option for managing low and\nintermediate-risk prostate cancer (PCa), aiming to avoid overtreatment while\nmonitoring disease progression through serial MRI and clinical follow-up.\nAccurate prostate segmentation is an important preliminary step for automating\nthis process, enabling automated detection and diagnosis of PCa. However,\nexisting deep-learning segmentation models are often trained on\nsingle-time-point and expertly annotated datasets, making them unsuitable for\nlongitudinal AS analysis, where multiple time points and a scarcity of expert\nlabels hinder their effective fine-tuning. To address these challenges, we\npropose MambaX-Net, a novel semi-supervised, dual-scan 3D segmentation\narchitecture that computes the segmentation for time point t by leveraging the\nMRI and the corresponding segmentation mask from the previous time point. We\nintroduce two new components: (i) a Mamba-enhanced Cross-Attention Module,\nwhich integrates the Mamba block into cross attention to efficiently capture\ntemporal evolution and long-range spatial dependencies, and (ii) a Shape\nExtractor Module that encodes the previous segmentation mask into a latent\nanatomical representation for refined zone delination. Moreover, we introduce a\nsemi-supervised self-training strategy that leverages pseudo-labels generated\nfrom a pre-trained nnU-Net, enabling effective learning without expert\nannotations. MambaX-Net was evaluated on a longitudinal AS dataset, and results\nshowed that it significantly outperforms state-of-the-art U-Net and\nTransformer-based models, achieving superior prostate zone segmentation even\nwhen trained on limited and noisy data.",
      "authors": [
        {
          "name": "Yovin Yahathugoda",
          "affiliation": null
        },
        {
          "name": "Davide Prezzi",
          "affiliation": null
        },
        {
          "name": "Piyalitt Ittichaiwong",
          "affiliation": null
        },
        {
          "name": "Vicky Goh",
          "affiliation": null
        },
        {
          "name": "Sebastien Ourselin",
          "affiliation": null
        },
        {
          "name": "Michela Antonelli",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17529v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17529v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17526v1",
      "title": "How Does Label Noise Gradient Descent Improve Generalization in the Low SNR Regime?",
      "abstract": "The capacity of deep learning models is often large enough to both learn the\nunderlying statistical signal and overfit to noise in the training set. This\nnoise memorization can be harmful especially for data with a low\nsignal-to-noise ratio (SNR), leading to poor generalization. Inspired by prior\nobservations that label noise provides implicit regularization that improves\ngeneralization, in this work, we investigate whether introducing label noise to\nthe gradient updates can enhance the test performance of neural network (NN) in\nthe low SNR regime. Specifically, we consider training a two-layer NN with a\nsimple label noise gradient descent (GD) algorithm, in an idealized\nsignal-noise data setting. We prove that adding label noise during training\nsuppresses noise memorization, preventing it from dominating the learning\nprocess; consequently, label noise GD enjoys rapid signal growth while the\noverfitting remains controlled, thereby achieving good generalization despite\nthe low SNR. In contrast, we also show that NN trained with standard GD tends\nto overfit to noise in the same low SNR setting and establish a non-vanishing\nlower bound on its test error, thus demonstrating the benefit of introducing\nlabel noise in gradient-based training.",
      "authors": [
        {
          "name": "Wei Huang",
          "affiliation": null
        },
        {
          "name": "Andi Han",
          "affiliation": null
        },
        {
          "name": "Yujin Song",
          "affiliation": null
        },
        {
          "name": "Yilan Chen",
          "affiliation": null
        },
        {
          "name": "Denny Wu",
          "affiliation": null
        },
        {
          "name": "Difan Zou",
          "affiliation": null
        },
        {
          "name": "Taiji Suzuki",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17526v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17526v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17524v1",
      "title": "Mitigating Clever Hans Strategies in Image Classifiers through Generating Counterexamples",
      "abstract": "Deep learning models remain vulnerable to spurious correlations, leading to\nso-called Clever Hans predictors that undermine robustness even in large-scale\nfoundation and self-supervised models. Group distributional robustness methods,\nsuch as Deep Feature Reweighting (DFR) rely on explicit group labels to\nupweight underrepresented subgroups, but face key limitations: (1) group labels\nare often unavailable, (2) low within-group sample sizes hinder coverage of the\nsubgroup distribution, and (3) performance degrades sharply when multiple\nspurious correlations fragment the data into even smaller groups. We propose\nCounterfactual Knowledge Distillation (CFKD), a framework that sidesteps these\nissues by generating diverse counterfactuals, enabling a human annotator to\nefficiently explore and correct the model's decision boundaries through a\nknowledge distillation step. Unlike DFR, our method not only reweights the\nundersampled groups, but it also enriches them with new data points. Our method\ndoes not require any confounder labels, achieves effective scaling to multiple\nconfounders, and yields balanced generalization across groups. We demonstrate\nCFKD's efficacy across five datasets, spanning synthetic tasks to an industrial\napplication, with particularly strong gains in low-data regimes with pronounced\nspurious correlations. Additionally, we provide an ablation study on the effect\nof the chosen counterfactual explainer and teacher model, highlighting their\nimpact on robustness.",
      "authors": [
        {
          "name": "Sidney Bender",
          "affiliation": null
        },
        {
          "name": "Ole Delzer",
          "affiliation": null
        },
        {
          "name": "Jan Herrmann",
          "affiliation": null
        },
        {
          "name": "Heike Antje Marxfeld",
          "affiliation": null
        },
        {
          "name": "Klaus-Robert M\u00fcller",
          "affiliation": null
        },
        {
          "name": "Gr\u00e9goire Montavon",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17524v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17524v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17520v1",
      "title": "Curiosity Meets Cooperation: A Game-Theoretic Approach to Long-Tail Multi-Label Learning",
      "abstract": "Long-tail imbalance is endemic to multi-label learning: a few head labels\ndominate the gradient signal, while the many rare labels that matter in\npractice are silently ignored. We tackle this problem by casting the task as a\ncooperative potential game. In our Curiosity-Driven Game-Theoretic Multi-Label\nLearning (CD-GTMLL) framework, the label space is split among several\ncooperating players that share a global accuracy payoff yet earn additional\ncuriosity rewards that rise with label rarity and inter-player disagreement.\nThese curiosity bonuses inject gradient on under-represented tags without\nhand-tuned class weights. We prove that gradient best-response updates ascend a\ndifferentiable potential and converge to tail-aware stationary points that\ntighten a lower bound on the expected Rare-F1. Extensive experiments on\nconventional benchmarks and three extreme-scale datasets show consistent\nstate-of-the-art gains, delivering up to +4.3% Rare-F1 and +1.6% P@3 over the\nstrongest baselines, while ablations reveal emergent division of labour and\nfaster consensus on rare classes. CD-GTMLL thus offers a principled, scalable\nroute to long-tail robustness in multi-label prediction.",
      "authors": [
        {
          "name": "Canran Xiao",
          "affiliation": null
        },
        {
          "name": "Chuangxin Zhao",
          "affiliation": null
        },
        {
          "name": "Zong Ke",
          "affiliation": null
        },
        {
          "name": "Fei Shen",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17520v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17520v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17519v1",
      "title": "MUG-V 10B: High-efficiency Training Pipeline for Large Video Generation Models",
      "abstract": "In recent years, large-scale generative models for visual content\n(\\textit{e.g.,} images, videos, and 3D objects/scenes) have made remarkable\nprogress. However, training large-scale video generation models remains\nparticularly challenging and resource-intensive due to cross-modal text-video\nalignment, the long sequences involved, and the complex spatiotemporal\ndependencies. To address these challenges, we present a training framework that\noptimizes four pillars: (i) data processing, (ii) model architecture, (iii)\ntraining strategy, and (iv) infrastructure for large-scale video generation\nmodels. These optimizations delivered significant efficiency gains and\nperformance improvements across all stages of data preprocessing, video\ncompression, parameter scaling, curriculum-based pretraining, and\nalignment-focused post-training. Our resulting model, MUG-V 10B, matches recent\nstate-of-the-art video generators overall and, on e-commerce-oriented video\ngeneration tasks, surpasses leading open-source baselines in human evaluations.\nMore importantly, we open-source the complete stack, including model weights,\nMegatron-Core-based large-scale training code, and inference pipelines for\nvideo generation and enhancement. To our knowledge, this is the first public\nrelease of large-scale video generation training code that exploits\nMegatron-Core to achieve high training efficiency and near-linear multi-node\nscaling, details are available in\n\\href{https://github.com/Shopee-MUG/MUG-V}{our webpage}.",
      "authors": [
        {
          "name": "Yongshun Zhang",
          "affiliation": null
        },
        {
          "name": "Zhongyi Fan",
          "affiliation": null
        },
        {
          "name": "Yonghang Zhang",
          "affiliation": null
        },
        {
          "name": "Zhangzikang Li",
          "affiliation": null
        },
        {
          "name": "Weifeng Chen",
          "affiliation": null
        },
        {
          "name": "Zhongwei Feng",
          "affiliation": null
        },
        {
          "name": "Chaoyue Wang",
          "affiliation": null
        },
        {
          "name": "Peng Hou",
          "affiliation": null
        },
        {
          "name": "Anxiang Zeng",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17519v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17519v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17517v1",
      "title": "SAFE-D: A Spatiotemporal Detection Framework for Abnormal Driving Among Parkinson's Disease-like Drivers",
      "abstract": "A driver's health state serves as a determinant factor in driving behavioral\nregulation. Subtle deviations from normalcy can lead to operational anomalies,\nposing risks to public transportation safety. While prior efforts have\ndeveloped detection mechanisms for functionally-driven temporary anomalies such\nas drowsiness and distraction, limited research has addressed\npathologically-triggered deviations, especially those stemming from chronic\nmedical conditions. To bridge this gap, we investigate the driving behavior of\nParkinson's disease patients and propose SAFE-D, a novel framework for\ndetecting Parkinson-related behavioral anomalies to enhance driving safety. Our\nmethodology starts by performing analysis of Parkinson's disease\nsymptomatology, focusing on primary motor impairments, and establishes causal\nlinks to degraded driving performance. To represent the subclinical behavioral\nvariations of early-stage Parkinson's disease, our framework integrates data\nfrom multiple vehicle control components to build a behavioral profile. We then\ndesign an attention-based network that adaptively prioritizes spatiotemporal\nfeatures, enabling robust anomaly detection under physiological variability.\nFinally, we validate SAFE-D on the Logitech G29 platform and CARLA simulator,\nusing data from three road maps to emulate real-world driving. Our results show\nSAFE-D achieves 96.8% average accuracy in distinguishing normal and\nParkinson-affected driving patterns.",
      "authors": [
        {
          "name": "Hangcheng Cao",
          "affiliation": null
        },
        {
          "name": "Baixiang Huang",
          "affiliation": null
        },
        {
          "name": "Longzhi Yuan",
          "affiliation": null
        },
        {
          "name": "Haonan An",
          "affiliation": null
        },
        {
          "name": "Zihan Fang",
          "affiliation": null
        },
        {
          "name": "Xianhao Chen",
          "affiliation": null
        },
        {
          "name": "Yuguang Fang",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cs.HC"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17517v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17517v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17516v1",
      "title": "SimBench: Benchmarking the Ability of Large Language Models to Simulate Human Behaviors",
      "abstract": "Large language model (LLM) simulations of human behavior have the potential\nto revolutionize the social and behavioral sciences, if and only if they\nfaithfully reflect real human behaviors. Current evaluations are fragmented,\nbased on bespoke tasks and metrics, creating a patchwork of incomparable\nresults. To address this, we introduce SimBench, the first large-scale,\nstandardized benchmark for a robust, reproducible science of LLM simulation. By\nunifying 20 diverse datasets covering tasks from moral decision-making to\neconomic choice across a large global participant pool, SimBench provides the\nnecessary foundation to ask fundamental questions about when, how, and why LLM\nsimulations succeed or fail. We show that, while even the best LLMs today have\nlimited simulation ability (score: 40.80/100), performance scales log-linearly\nwith model size. Simulation performance is not improved by increased\ninference-time compute. We demonstrate an alignment-simulation trade-off:\ninstruction-tuning improves performance on low-entropy (consensus) questions\nbut degrades it on high-entropy (diverse) ones. Models particularly struggle\nwhen simulating specific demographic groups. Finally, we demonstrate that\nsimulation ability correlates most strongly with deep, knowledge-intensive\nreasoning (MMLU-Pro, r=0.939). By making progress measurable, we aim to\naccelerate the development of more faithful LLM simulators.",
      "authors": [
        {
          "name": "Tiancheng Hu",
          "affiliation": null
        },
        {
          "name": "Joachim Baumann",
          "affiliation": null
        },
        {
          "name": "Lorenzo Lupo",
          "affiliation": null
        },
        {
          "name": "Dirk Hovy",
          "affiliation": null
        },
        {
          "name": "Nigel Collier",
          "affiliation": null
        },
        {
          "name": "Paul R\u00f6ttger",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17516v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17516v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17515v1",
      "title": "The Graphon Limit Hypothesis: Understanding Neural Network Pruning via Infinite Width Analysis",
      "abstract": "Sparse neural networks promise efficiency, yet training them effectively\nremains a fundamental challenge. Despite advances in pruning methods that\ncreate sparse architectures, understanding why some sparse structures are\nbetter trainable than others with the same level of sparsity remains poorly\nunderstood. Aiming to develop a systematic approach to this fundamental\nproblem, we propose a novel theoretical framework based on the theory of graph\nlimits, particularly graphons, that characterizes sparse neural networks in the\ninfinite-width regime. Our key insight is that connectivity patterns of sparse\nneural networks induced by pruning methods converge to specific graphons as\nnetworks' width tends to infinity, which encodes implicit structural biases of\ndifferent pruning methods. We postulate the Graphon Limit Hypothesis and\nprovide empirical evidence to support it. Leveraging this graphon\nrepresentation, we derive a Graphon Neural Tangent Kernel (Graphon NTK) to\nstudy the training dynamics of sparse networks in the infinite width limit.\nGraphon NTK provides a general framework for the theoretical analysis of sparse\nnetworks. We empirically show that the spectral analysis of Graphon NTK\ncorrelates with observed training dynamics of sparse networks, explaining the\nvarying convergence behaviours of different pruning methods. Our framework\nprovides theoretical insights into the impact of connectivity patterns on the\ntrainability of various sparse network architectures.",
      "authors": [
        {
          "name": "Hoang Pham",
          "affiliation": null
        },
        {
          "name": "The-Anh Ta",
          "affiliation": null
        },
        {
          "name": "Tom Jacobs",
          "affiliation": null
        },
        {
          "name": "Rebekka Burkholz",
          "affiliation": null
        },
        {
          "name": "Long Tran-Thanh",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17515v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17515v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17512v1",
      "title": "AWARE: Audio Watermarking with Adversarial Resistance to Edits",
      "abstract": "Prevailing practice in learning-based audio watermarking is to pursue\nrobustness by expanding the set of simulated distortions during training.\nHowever, such surrogates are narrow and prone to overfitting. This paper\npresents AWARE (Audio Watermarking with Adversarial Resistance to Edits), an\nalternative approach that avoids reliance on attack-simulation stacks and\nhandcrafted differentiable distortions. Embedding is obtained via adversarial\noptimization in the time-frequency domain under a level-proportional perceptual\nbudget. Detection employs a time-order-agnostic detector with a Bitwise Readout\nHead (BRH) that aggregates temporal evidence into one score per watermark bit,\nenabling reliable watermark decoding even under desynchronization and temporal\ncuts. Empirically, AWARE attains high audio quality and speech intelligibility\n(PESQ/STOI) and consistently low BER across various audio edits, often\nsurpassing representative state-of-the-art learning-based audio watermarking\nsystems.",
      "authors": [
        {
          "name": "Kosta Pavlovi\u0107",
          "affiliation": null
        },
        {
          "name": "Lazar Stanarevi\u0107",
          "affiliation": null
        },
        {
          "name": "Petar Nedi\u0107",
          "affiliation": null
        },
        {
          "name": "Slavko Kova\u010devi\u0107",
          "affiliation": null
        },
        {
          "name": "Igor Djurovi\u0107",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.SD",
        "cs.LG",
        "cs.MM",
        "eess.AS"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17512v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17512v1",
      "primary_category": "cs.SD",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17509v1",
      "title": "Annotation-Efficient Universal Honesty Alignment",
      "abstract": "Honesty alignment-the ability of large language models (LLMs) to recognize\ntheir knowledge boundaries and express calibrated confidence-is essential for\ntrustworthy deployment. Existing methods either rely on training-free\nconfidence estimation (e.g., token probabilities, self-consistency) or\ntraining-based calibration with correctness annotations. While effective,\nachieving universal honesty alignment with training-based calibration requires\ncostly, large-scale labeling. To support annotation-efficient training, we\nintroduce Elicitation-Then-Calibration (EliCal), a two-stage framework that\nfirst elicits internal confidence using inexpensive self-consistency\nsupervision, then calibrates this confidence with a small set of correctness\nannotations. To support a large-scale study, we release HonestyBench, a\nbenchmark covering ten free-form QA datasets with 560k training and 70k\nevaluation instances annotated with correctness and self-consistency signals.\nExperiments show that EliCal achieves near-optimal alignment with only 1k\ncorrectness annotations (0.18% of full supervision) and better alignment\nperformance on unseen MMLU tasks than the calibration-only baseline, offering a\nscalable solution toward universal honesty alignment in LLMs.",
      "authors": [
        {
          "name": "Shiyu Ni",
          "affiliation": null
        },
        {
          "name": "Keping Bi",
          "affiliation": null
        },
        {
          "name": "Jiafeng Guo",
          "affiliation": null
        },
        {
          "name": "Minghao Tang",
          "affiliation": null
        },
        {
          "name": "Jingtong Wu",
          "affiliation": null
        },
        {
          "name": "Zengxin Han",
          "affiliation": null
        },
        {
          "name": "Xueqi Cheng",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17509v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17509v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17506v1",
      "title": "Convergence Rates for Gradient Descent on the Edge of Stability in Overparametrised Least Squares",
      "abstract": "Classical optimisation theory guarantees monotonic objective decrease for\ngradient descent (GD) when employed in a small step size, or ``stable\", regime.\nIn contrast, gradient descent on neural networks is frequently performed in a\nlarge step size regime called the ``edge of stability\", in which the objective\ndecreases non-monotonically with an observed implicit bias towards flat minima.\nIn this paper, we take a step toward quantifying this phenomenon by providing\nconvergence rates for gradient descent with large learning rates in an\noverparametrised least squares setting. The key insight behind our analysis is\nthat, as a consequence of overparametrisation, the set of global minimisers\nforms a Riemannian manifold $M$, which enables the decomposition of the GD\ndynamics into components parallel and orthogonal to $M$. The parallel component\ncorresponds to Riemannian gradient descent on the objective sharpness, while\nthe orthogonal component is a bifurcating dynamical system. This insight allows\nus to derive convergence rates in three regimes characterised by the learning\nrate size: (a) the subcritical regime, in which transient instability is\novercome in finite time before linear convergence to a suboptimally flat global\nminimum; (b) the critical regime, in which instability persists for all time\nwith a power-law convergence toward the optimally flat global minimum; and (c)\nthe supercritical regime, in which instability persists for all time with\nlinear convergence to an orbit of period two centred on the optimally flat\nglobal minimum.",
      "authors": [
        {
          "name": "Lachlan Ewen MacDonald",
          "affiliation": null
        },
        {
          "name": "Hancheng Min",
          "affiliation": null
        },
        {
          "name": "Leandro Palma",
          "affiliation": null
        },
        {
          "name": "Salma Tarmoun",
          "affiliation": null
        },
        {
          "name": "Ziqing Xu",
          "affiliation": null
        },
        {
          "name": "Ren\u00e9 Vidal",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "math.OC"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17506v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17506v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17504v1",
      "title": "Lingua Custodi's participation at the WMT 2025 Terminology shared task",
      "abstract": "While BERT is an effective method for learning monolingual sentence\nembeddings for semantic similarity and embedding based transfer learning BERT\nbased cross-lingual sentence embeddings have yet to be explored. We\nsystematically investigate methods for learning multilingual sentence\nembeddings by combining the best methods for learning monolingual and\ncross-lingual representations including: masked language modeling (MLM),\ntranslation language modeling (TLM), dual encoder translation ranking, and\nadditive margin softmax. We show that introducing a pre-trained multilingual\nlanguage model dramatically reduces the amount of parallel training data\nrequired to achieve good performance by 80%. Composing the best of these\nmethods produces a model that achieves 83.7% bi-text retrieval accuracy over\n112 languages on Tatoeba, well above the 65.5 achieved by LASER, while still\nperforming competitively on monolingual transfer learning benchmarks. Parallel\ndata mined from CommonCrawl using our best model is shown to train competitive\nNMT models for en-zh and en-de. We publicly release our best multilingual\nsentence embedding model for 109+ languages at https://tfhub.dev/google/LaBSE.",
      "authors": [
        {
          "name": "Jingshu Liu",
          "affiliation": null
        },
        {
          "name": "Raheel Qader",
          "affiliation": null
        },
        {
          "name": "Ga\u00ebtan Caillaut",
          "affiliation": null
        },
        {
          "name": "Mariam Nakhl\u00e9",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17504v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17504v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17503v1",
      "title": "Stochastic Difference-of-Convex Optimization with Momentum",
      "abstract": "Stochastic difference-of-convex (DC) optimization is prevalent in numerous\nmachine learning applications, yet its convergence properties under small batch\nsizes remain poorly understood. Existing methods typically require large\nbatches or strong noise assumptions, which limit their practical use. In this\nwork, we show that momentum enables convergence under standard smoothness and\nbounded variance assumptions (of the concave part) for any batch size. We prove\nthat without momentum, convergence may fail regardless of stepsize,\nhighlighting its necessity. Our momentum-based algorithm achieves provable\nconvergence and demonstrates strong empirical performance.",
      "authors": [
        {
          "name": "El Mahdi Chayti",
          "affiliation": null
        },
        {
          "name": "Martin Jaggi",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "math.OC",
        "stat.ML"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17503v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17503v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17501v1",
      "title": "Context-Aware Pseudo-Label Scoring for Zero-Shot Video Summarization",
      "abstract": "With the rapid proliferation of video content across social media,\nsurveillance, and education platforms, efficiently summarizing long videos into\nconcise yet semantically faithful surrogates has become increasingly vital.\nExisting supervised methods achieve strong in-domain accuracy by learning from\ndense annotations but suffer from high labeling costs and limited cross-dataset\ngeneralization, while unsupervised approaches, though label-free, often fail to\ncapture high-level human semantics and fine-grained narrative cues. More\nrecently, zero-shot prompting pipelines have leveraged large language models\n(LLMs) for training-free video summarization, yet remain highly sensitive to\nhandcrafted prompt templates and dataset-specific score normalization. To\novercome these limitations, we introduce a rubric-guided, pseudo-labeled\nprompting framework that transforms a small subset of ground-truth annotations\ninto high-confidence pseudo labels, which are aggregated into structured,\ndataset-adaptive scoring rubrics guiding interpretable scene evaluation. During\ninference, first and last segments are scored based solely on their\ndescriptions, whereas intermediate ones incorporate brief contextual summaries\nof adjacent scenes to assess narrative progression and redundancy. This\ncontextual prompting enables the LLM to balance local salience and global\ncoherence without parameter tuning. On SumMe and TVSum, our method achieves F1\nscores of \\textbf{57.58} and \\textbf{63.05}, surpassing unsupervised and prior\nzero-shot baselines while approaching supervised performance. The results\ndemonstrate that rubric-guided pseudo labeling effectively stabilizes LLM-based\nscoring and establishes a general, interpretable zero-shot paradigm for video\nsummarization.",
      "authors": [
        {
          "name": "Yuanli Wu",
          "affiliation": null
        },
        {
          "name": "Long Zhang",
          "affiliation": null
        },
        {
          "name": "Yue Du",
          "affiliation": null
        },
        {
          "name": "Bin Li",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17501v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17501v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17498v1",
      "title": "Deep Self-Evolving Reasoning",
      "abstract": "Long-form chain-of-thought reasoning has become a cornerstone of advanced\nreasoning in large language models. While recent verification-refinement\nframeworks have enabled proprietary models to solve Olympiad-level problems,\ntheir effectiveness hinges on strong, reliable verification and correction\ncapabilities, which remain fragile in open-weight, smaller-scale models. This\nwork demonstrates that even with weak verification and refinement capabilities\non hard tasks, the reasoning limits of such models can be substantially\nextended through a probabilistic paradigm we call Deep Self-Evolving Reasoning\n(DSER). We conceptualize iterative reasoning as a Markov chain, where each step\nrepresents a stochastic transition in the solution space. The key insight is\nthat convergence to a correct solution is guaranteed as long as the probability\nof improvement marginally exceeds that of degradation. By running multiple\nlong-horizon, self-evolving processes in parallel, DSER amplifies these small\npositive tendencies, enabling the model to asymptotically approach correct\nanswers. Empirically, we apply DSER to the DeepSeek-R1-0528-Qwen3-8B model. On\nthe challenging AIME 2024-2025 benchmark, DSER solves 5 out of 9 previously\nunsolvable problems and boosts overall performance, enabling this compact model\nto surpass the single-turn accuracy of its 600B-parameter teacher through\nmajority voting. Beyond its immediate utility for test-time scaling, the DSER\nframework serves to diagnose the fundamental limitations of current open-weight\nreasoners. By clearly delineating their shortcomings in self-verification,\nrefinement, and stability, our findings establish a clear research agenda for\ndeveloping next-generation models with powerful, intrinsic self-evolving\ncapabilities.",
      "authors": [
        {
          "name": "Zihan Liu",
          "affiliation": null
        },
        {
          "name": "Shun Zheng",
          "affiliation": null
        },
        {
          "name": "Xumeng Wen",
          "affiliation": null
        },
        {
          "name": "Yang Wang",
          "affiliation": null
        },
        {
          "name": "Jiang Bian",
          "affiliation": null
        },
        {
          "name": "Mao Yang",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17498v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17498v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17496v1",
      "title": "I-RAVEN-X: Benchmarking Generalization and Robustness of Analogical and Mathematical Reasoning in Large Language and Reasoning Models",
      "abstract": "We introduce I-RAVEN-X, a symbolic benchmark designed to evaluate\ngeneralization and robustness in analogical and mathematical reasoning for\nLarge Language Models (LLMs) and Large Reasoning Models (LRMs). I-RAVEN-X\nextends I-RAVEN by increasing operand complexity, attribute range, and\nintroducing perceptual uncertainty. Compared to LLMs, empirical results show\nthat LRMs achieve improved productivity and systematicity on longer reasoning\nrelations and wider attribute ranges, respectively. However, LRMs are still\nsignificantly challenged by reasoning under uncertainty and cannot effectively\nexplore multiple probabilistic outcomes.",
      "authors": [
        {
          "name": "Giacomo Camposampiero",
          "affiliation": null
        },
        {
          "name": "Michael Hersche",
          "affiliation": null
        },
        {
          "name": "Roger Wattenhofer",
          "affiliation": null
        },
        {
          "name": "Abu Sebastian",
          "affiliation": null
        },
        {
          "name": "Abbas Rahimi",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17496v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17496v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17491v1",
      "title": "Empowering Real-World: A Survey on the Technology, Practice, and Evaluation of LLM-driven Industry Agents",
      "abstract": "With the rise of large language models (LLMs), LLM agents capable of\nautonomous reasoning, planning, and executing complex tasks have become a\nfrontier in artificial intelligence. However, how to translate the research on\ngeneral agents into productivity that drives industry transformations remains a\nsignificant challenge. To address this, this paper systematically reviews the\ntechnologies, applications, and evaluation methods of industry agents based on\nLLMs. Using an industry agent capability maturity framework, it outlines the\nevolution of agents in industry applications, from \"process execution systems\"\nto \"adaptive social systems.\" First, we examine the three key technological\npillars that support the advancement of agent capabilities: Memory, Planning,\nand Tool Use. We discuss how these technologies evolve from supporting simple\ntasks in their early forms to enabling complex autonomous systems and\ncollective intelligence in more advanced forms. Then, we provide an overview of\nthe application of industry agents in real-world domains such as digital\nengineering, scientific discovery, embodied intelligence, collaborative\nbusiness execution, and complex system simulation. Additionally, this paper\nreviews the evaluation benchmarks and methods for both fundamental and\nspecialized capabilities, identifying the challenges existing evaluation\nsystems face regarding authenticity, safety, and industry specificity. Finally,\nwe focus on the practical challenges faced by industry agents, exploring their\ncapability boundaries, developmental potential, and governance issues in\nvarious scenarios, while providing insights into future directions. By\ncombining technological evolution with industry practices, this review aims to\nclarify the current state and offer a clear roadmap and theoretical foundation\nfor understanding and building the next generation of industry agents.",
      "authors": [
        {
          "name": "Yihong Tang",
          "affiliation": null
        },
        {
          "name": "Kehai Chen",
          "affiliation": null
        },
        {
          "name": "Liang Yue",
          "affiliation": null
        },
        {
          "name": "Jinxin Fan",
          "affiliation": null
        },
        {
          "name": "Caishen Zhou",
          "affiliation": null
        },
        {
          "name": "Xiaoguang Li",
          "affiliation": null
        },
        {
          "name": "Yuyang Zhang",
          "affiliation": null
        },
        {
          "name": "Mingming Zhao",
          "affiliation": null
        },
        {
          "name": "Shixiong Kai",
          "affiliation": null
        },
        {
          "name": "Kaiyang Guo",
          "affiliation": null
        },
        {
          "name": "Xingshan Zeng",
          "affiliation": null
        },
        {
          "name": "Wenjing Cun",
          "affiliation": null
        },
        {
          "name": "Lifeng Shang",
          "affiliation": null
        },
        {
          "name": "Min Zhang",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17491v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17491v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17489v1",
      "title": "DETree: DEtecting Human-AI Collaborative Texts via Tree-Structured Hierarchical Representation Learning",
      "abstract": "Detecting AI-involved text is essential for combating misinformation,\nplagiarism, and academic misconduct. However, AI text generation includes\ndiverse collaborative processes (AI-written text edited by humans,\nhuman-written text edited by AI, and AI-generated text refined by other AI),\nwhere various or even new LLMs could be involved. Texts generated through these\nvaried processes exhibit complex characteristics, presenting significant\nchallenges for detection. Current methods model these processes rather crudely,\nprimarily employing binary classification (purely human vs. AI-involved) or\nmulti-classification (treating human-AI collaboration as a new class). We\nobserve that representations of texts generated through different processes\nexhibit inherent clustering relationships. Therefore, we propose DETree, a\nnovel approach that models the relationships among different processes as a\nHierarchical Affinity Tree structure, and introduces a specialized loss\nfunction that aligns text representations with this tree. To facilitate this\nlearning, we developed RealBench, a comprehensive benchmark dataset that\nautomatically incorporates a wide spectrum of hybrid texts produced through\nvarious human-AI collaboration processes. Our method improves performance in\nhybrid text detection tasks and significantly enhances robustness and\ngeneralization in out-of-distribution scenarios, particularly in few-shot\nlearning conditions, further demonstrating the promise of training-based\napproaches in OOD settings. Our code and dataset are available at\nhttps://github.com/heyongxin233/DETree.",
      "authors": [
        {
          "name": "Yongxin He",
          "affiliation": null
        },
        {
          "name": "Shan Zhang",
          "affiliation": null
        },
        {
          "name": "Yixuan Cao",
          "affiliation": null
        },
        {
          "name": "Lei Ma",
          "affiliation": null
        },
        {
          "name": "Ping Luo",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17489v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17489v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17486v1",
      "title": "Local properties of neural networks through the lens of layer-wise Hessians",
      "abstract": "We introduce a methodology for analyzing neural networks through the lens of\nlayer-wise Hessian matrices. The local Hessian of each functional block (layer)\nis defined as the matrix of second derivatives of a scalar function with\nrespect to the parameters of that layer. This concept provides a formal tool\nfor characterizing the local geometry of the parameter space. We show that the\nspectral properties of local Hessians, such as the distribution of eigenvalues,\nreveal quantitative patterns associated with overfitting,\nunderparameterization, and expressivity in neural network architectures. We\nconduct an extensive empirical study involving 111 experiments across 37\ndatasets. The results demonstrate consistent structural regularities in the\nevolution of local Hessians during training and highlight correlations between\ntheir spectra and generalization performance. These findings establish a\nfoundation for using local geometric analysis to guide the diagnosis and design\nof deep neural networks. The proposed framework connects optimization geometry\nwith functional behavior and offers practical insight for improving network\narchitectures and training stability.",
      "authors": [
        {
          "name": "Maxim Bolshim",
          "affiliation": null
        },
        {
          "name": "Alexander Kugaevskikh",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "68T07, 68T05, 65K10, 90C30",
        "I.2.6; G.1.6; I.5.1"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17486v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17486v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17483v1",
      "title": "ReXMoE: Reusing Experts with Minimal Overhead in Mixture-of-Experts",
      "abstract": "Mixture-of-Experts (MoE) architectures have emerged as a promising approach\nto scale Large Language Models (LLMs). MoE boosts the efficiency by activating\na subset of experts per token. Recent works show that fine-grained experts\nsubstantially enriches the combinatorial flexibility of active experts and\nenhances model expressiveness. However, such a design is fundamentally limited\nby the layer-local routing mechanism: each layer is restricted to its own\nexpert pool. This requires a careful trade-off between expert dimensionality\nand routing diversity given fixed parameter budgets. We describe ReXMoE, a\nnovel MoE architecture that improves routing beyond the existing layer-local\napproaches by allowing routers to reuse experts across adjacent layers. ReXMoE\ndecouples expert dimensionality from per-layer budgets, enabling richer expert\ncombinations without sacrificing individual expert capacity or inflating\noverall parameters. To this end, we propose a new progressive scaling routing\n(PSR) strategy to gradually increase the candidate expert pool during training.\nAs a result, ReXMoE improves both language modeling and downstream task\nperformance. Extensive experiments on models ranging from 0.5B to 7B parameters\nacross different architectures demonstrate that ReXMoE consistently improves\nperformance under fixed architectural dimensions, confirming ReXMoE as new\ndesign paradigm for parameter-efficient and scalable MoE-based LLMs.",
      "authors": [
        {
          "name": "Zheyue Tan",
          "affiliation": null
        },
        {
          "name": "Zhiyuan Li",
          "affiliation": null
        },
        {
          "name": "Tao Yuan",
          "affiliation": null
        },
        {
          "name": "Dong Zhou",
          "affiliation": null
        },
        {
          "name": "Weilin Liu",
          "affiliation": null
        },
        {
          "name": "Yueqing Zhuang",
          "affiliation": null
        },
        {
          "name": "Yadong Li",
          "affiliation": null
        },
        {
          "name": "Guowei Niu",
          "affiliation": null
        },
        {
          "name": "Cheng Qin",
          "affiliation": null
        },
        {
          "name": "Zhuyu Yao",
          "affiliation": null
        },
        {
          "name": "Congyi Liu",
          "affiliation": null
        },
        {
          "name": "Haiyang Xu",
          "affiliation": null
        },
        {
          "name": "Boxun Li",
          "affiliation": null
        },
        {
          "name": "Guohao Dai",
          "affiliation": null
        },
        {
          "name": "Bo Zhao",
          "affiliation": null
        },
        {
          "name": "Yu Wang",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17483v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17483v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17484v1",
      "title": "Split-Fuse-Transport: Annotation-Free Saliency via Dual Clustering and Optimal Transport Alignment",
      "abstract": "Salient object detection (SOD) aims to segment visually prominent regions in\nimages and serves as a foundational task for various computer vision\napplications. We posit that SOD can now reach near-supervised accuracy without\na single pixel-level label, but only when reliable pseudo-masks are available.\nWe revisit the prototype-based line of work and make two key observations.\nFirst, boundary pixels and interior pixels obey markedly different geometry;\nsecond, the global consistency enforced by optimal transport (OT) is\nunderutilized if prototype quality is weak. To address this, we introduce\nPOTNet, an adaptation of Prototypical Optimal Transport that replaces POT's\nsingle k-means step with an entropy-guided dual-clustering head: high-entropy\npixels are organized by spectral clustering, low-entropy pixels by k-means, and\nthe two prototype sets are subsequently aligned by OT. This\nsplit-fuse-transport design yields sharper, part-aware pseudo-masks in a single\nforward pass, without handcrafted priors. Those masks supervise a standard\nMaskFormer-style encoder-decoder, giving rise to AutoSOD, an end-to-end\nunsupervised SOD pipeline that eliminates SelfMask's offline voting yet\nimproves both accuracy and training efficiency. Extensive experiments on five\nbenchmarks show that AutoSOD outperforms unsupervised methods by up to 26% and\nweakly supervised methods by up to 36% in F-measure, further narrowing the gap\nto fully supervised models.",
      "authors": [
        {
          "name": "Muhammad Umer Ramzan",
          "affiliation": null
        },
        {
          "name": "Ali Zia",
          "affiliation": null
        },
        {
          "name": "Abdelwahed Khamis",
          "affiliation": null
        },
        {
          "name": "Noman Ali",
          "affiliation": null
        },
        {
          "name": "Usman Ali",
          "affiliation": null
        },
        {
          "name": "Wei Xiang",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17484v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17484v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17482v1",
      "title": "SparseWorld: A Flexible, Adaptive, and Efficient 4D Occupancy World Model Powered by Sparse and Dynamic Queries",
      "abstract": "Semantic occupancy has emerged as a powerful representation in world models\nfor its ability to capture rich spatial semantics. However, most existing\noccupancy world models rely on static and fixed embeddings or grids, which\ninherently limit the flexibility of perception. Moreover, their ``in-place\nclassification\" over grids exhibits a potential misalignment with the dynamic\nand continuous nature of real scenarios.In this paper, we propose SparseWorld,\na novel 4D occupancy world model that is flexible, adaptive, and efficient,\npowered by sparse and dynamic queries. We propose a Range-Adaptive Perception\nmodule, in which learnable queries are modulated by the ego vehicle states and\nenriched with temporal-spatial associations to enable extended-range\nperception. To effectively capture the dynamics of the scene, we design a\nState-Conditioned Forecasting module, which replaces classification-based\nforecasting with regression-guided formulation, precisely aligning the dynamic\nqueries with the continuity of the 4D environment. In addition, We specifically\ndevise a Temporal-Aware Self-Scheduling training strategy to enable smooth and\nefficient training. Extensive experiments demonstrate that SparseWorld achieves\nstate-of-the-art performance across perception, forecasting, and planning\ntasks. Comprehensive visualizations and ablation studies further validate the\nadvantages of SparseWorld in terms of flexibility, adaptability, and\nefficiency. The code is available at https://github.com/MSunDYY/SparseWorld.",
      "authors": [
        {
          "name": "Chenxu Dang",
          "affiliation": null
        },
        {
          "name": "Haiyan Liu",
          "affiliation": null
        },
        {
          "name": "Guangjun Bao",
          "affiliation": null
        },
        {
          "name": "Pei An",
          "affiliation": null
        },
        {
          "name": "Xinyue Tang",
          "affiliation": null
        },
        {
          "name": "Jie Ma",
          "affiliation": null
        },
        {
          "name": "Bingchuan Sun",
          "affiliation": null
        },
        {
          "name": "Yan Wang",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17482v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17482v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17480v1",
      "title": "Unified Privacy Guarantees for Decentralized Learning via Matrix Factorization",
      "abstract": "Decentralized Learning (DL) enables users to collaboratively train models\nwithout sharing raw data by iteratively averaging local updates with neighbors\nin a network graph. This setting is increasingly popular for its scalability\nand its ability to keep data local under user control. Strong privacy\nguarantees in DL are typically achieved through Differential Privacy (DP), with\nresults showing that DL can even amplify privacy by disseminating noise across\npeer-to-peer communications. Yet in practice, the observed privacy-utility\ntrade-off often appears worse than in centralized training, which may be due to\nlimitations in current DP accounting methods for DL. In this paper, we show\nthat recent advances in centralized DP accounting based on Matrix Factorization\n(MF) for analyzing temporal noise correlations can also be leveraged in DL. By\ngeneralizing existing MF results, we show how to cast both standard DL\nalgorithms and common trust models into a unified formulation. This yields\ntighter privacy accounting for existing DP-DL algorithms and provides a\nprincipled way to develop new ones. To demonstrate the approach, we introduce\nMAFALDA-SGD, a gossip-based DL algorithm with user-level correlated noise that\noutperforms existing methods on synthetic and real-world graphs.",
      "authors": [
        {
          "name": "Aur\u00e9lien Bellet",
          "affiliation": null
        },
        {
          "name": "Edwige Cyffers",
          "affiliation": null
        },
        {
          "name": "Davide Frey",
          "affiliation": null
        },
        {
          "name": "Romaric Gaudel",
          "affiliation": null
        },
        {
          "name": "Dimitri Ler\u00e9v\u00e9rend",
          "affiliation": null
        },
        {
          "name": "Fran\u00e7ois Ta\u00efani",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17480v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17480v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17479v1",
      "title": "Initialize to Generalize: A Stronger Initialization Pipeline for Sparse-View 3DGS",
      "abstract": "Sparse-view 3D Gaussian Splatting (3DGS) often overfits to the training\nviews, leading to artifacts like blurring in novel view rendering. Prior work\naddresses it either by enhancing the initialization (\\emph{i.e.}, the point\ncloud from Structure-from-Motion (SfM)) or by adding training-time constraints\n(regularization) to the 3DGS optimization. Yet our controlled ablations reveal\nthat initialization is the decisive factor: it determines the attainable\nperformance band in sparse-view 3DGS, while training-time constraints yield\nonly modest within-band improvements at extra cost. Given initialization's\nprimacy, we focus our design there. Although SfM performs poorly under sparse\nviews due to its reliance on feature matching, it still provides reliable seed\npoints. Thus, building on SfM, our effort aims to supplement the regions it\nfails to cover as comprehensively as possible. Specifically, we design: (i)\nfrequency-aware SfM that improves low-texture coverage via low-frequency view\naugmentation and relaxed multi-view correspondences; (ii) 3DGS\nself-initialization that lifts photometric supervision into additional points,\ncompensating SfM-sparse regions with learned Gaussian centers; and (iii)\npoint-cloud regularization that enforces multi-view consistency and uniform\nspatial coverage through simple geometric/visibility priors, yielding a clean\nand reliable point cloud. Our experiments on LLFF and Mip-NeRF360 demonstrate\nconsistent gains in sparse-view settings, establishing our approach as a\nstronger initialization strategy. Code is available at\nhttps://github.com/zss171999645/ItG-GS.",
      "authors": [
        {
          "name": "Feng Zhou",
          "affiliation": null
        },
        {
          "name": "Wenkai Guo",
          "affiliation": null
        },
        {
          "name": "Pu Cao",
          "affiliation": null
        },
        {
          "name": "Zhicheng Zhang",
          "affiliation": null
        },
        {
          "name": "Jianqin Yin",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17479v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17479v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17478v1",
      "title": "Towards geological inference with process-based and deep generative modeling, part 2: inversion of fluvial deposits and latent-space disentanglement",
      "abstract": "High costs and uncertainties make subsurface decision-making challenging, as\nacquiring new data is rarely scalable. Embedding geological knowledge directly\ninto predictive models offers a valuable alternative. A joint approach enables\njust that: process-based models that mimic geological processes can help train\ngenerative models that make predictions more efficiently. This study explores\nwhether a generative adversarial network (GAN) - a type of deep-learning\nalgorithm for generative modeling - trained to produce fluvial deposits can be\ninverted to match well and seismic data. Four inversion approaches applied to\nthree test samples with 4, 8, and 20 wells struggled to match these well data,\nespecially as the well number increased or as the test sample diverged from the\ntraining data. The key bottleneck lies in the GAN's latent representation: it\nis entangled, so samples with similar sedimentological features are not\nnecessarily close in the latent space. Label conditioning or latent\noverparameterization can partially disentangle the latent space during\ntraining, although not yet sufficiently for a successful inversion. Fine-tuning\nthe GAN to restructure the latent space locally reduces mismatches to\nacceptable levels for all test cases, with and without seismic data. But this\napproach depends on an initial, partially successful inversion step, which\ninfluences the quality and diversity of the final samples. Overall, GANs can\nalready handle the tasks required for their integration into geomodeling\nworkflows. We still need to further assess their robustness, and how to best\nleverage them in support of geological interpretation.",
      "authors": [
        {
          "name": "Guillaume Rongier",
          "affiliation": null
        },
        {
          "name": "Luk Peeters",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "physics.geo-ph",
        "I.2.6; I.6.3; J.2"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17478v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17478v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17476v1",
      "title": "Disparities in Multilingual LLM-Based Healthcare Q&A",
      "abstract": "Equitable access to reliable health information is vital when integrating AI\ninto healthcare. Yet, information quality varies across languages, raising\nconcerns about the reliability and consistency of multilingual Large Language\nModels (LLMs). We systematically examine cross-lingual disparities in\npre-training source and factuality alignment in LLM answers for multilingual\nhealthcare Q&A across English, German, Turkish, Chinese (Mandarin), and\nItalian. We (i) constructed Multilingual Wiki Health Care\n(MultiWikiHealthCare), a multilingual dataset from Wikipedia; (ii) analyzed\ncross-lingual healthcare coverage; (iii) assessed LLM response alignment with\nthese references; and (iv) conducted a case study on factual alignment through\nthe use of contextual information and Retrieval-Augmented Generation (RAG). Our\nfindings reveal substantial cross-lingual disparities in both Wikipedia\ncoverage and LLM factual alignment. Across LLMs, responses align more with\nEnglish Wikipedia, even when the prompts are non-English. Providing contextual\nexcerpts from non-English Wikipedia at inference time effectively shifts\nfactual alignment toward culturally relevant knowledge. These results highlight\npractical pathways for building more equitable, multilingual AI systems for\nhealthcare.",
      "authors": [
        {
          "name": "Ipek Baris Schlicht",
          "affiliation": null
        },
        {
          "name": "Burcu Sayin",
          "affiliation": null
        },
        {
          "name": "Zhixue Zhao",
          "affiliation": null
        },
        {
          "name": "Frederik M. Labont\u00e9",
          "affiliation": null
        },
        {
          "name": "Cesare Barbera",
          "affiliation": null
        },
        {
          "name": "Marco Viviani",
          "affiliation": null
        },
        {
          "name": "Paolo Rosso",
          "affiliation": null
        },
        {
          "name": "Lucie Flek",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17476v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17476v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17475v1",
      "title": "DAMSDAN: Distribution-Aware Multi-Source Domain Adaptation Network for Cross-Domain EEG-based Emotion Recognition",
      "abstract": "Significant inter-individual variability limits the generalization of\nEEG-based emotion recognition under cross-domain settings. We address two core\nchallenges in multi-source adaptation: (1) dynamically modeling distributional\nheterogeneity across sources and quantifying their relevance to a target to\nreduce negative transfer; and (2) achieving fine-grained semantic consistency\nto strengthen class discrimination. We propose a distribution-aware\nmulti-source domain adaptation network (DAMSDAN). DAMSDAN integrates\nprototype-based constraints with adversarial learning to drive the encoder\ntoward discriminative, domain-invariant emotion representations. A domain-aware\nsource weighting strategy based on maximum mean discrepancy (MMD) dynamically\nestimates inter-domain shifts and reweights source contributions. In addition,\na prototype-guided conditional alignment module with dual pseudo-label\ninteraction enhances pseudo-label reliability and enables category-level,\nfine-grained alignment, mitigating noise propagation and semantic drift.\nExperiments on SEED and SEED-IV show average accuracies of 94.86\\% and 79.78\\%\nfor cross-subject, and 95.12\\% and 83.15\\% for cross-session protocols. On the\nlarge-scale FACED dataset, DAMSDAN achieves 82.88\\% (cross-subject). Extensive\nablations and interpretability analyses corroborate the effectiveness of the\nproposed framework for cross-domain EEG-based emotion recognition.",
      "authors": [
        {
          "name": "Fo Hu",
          "affiliation": null
        },
        {
          "name": "Can Wang",
          "affiliation": null
        },
        {
          "name": "Qinxu Zheng",
          "affiliation": null
        },
        {
          "name": "Xusheng Yang",
          "affiliation": null
        },
        {
          "name": "Bin Zhou",
          "affiliation": null
        },
        {
          "name": "Gang Li",
          "affiliation": null
        },
        {
          "name": "Yu Sun",
          "affiliation": null
        },
        {
          "name": "Wen-an Zhang",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17475v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17475v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17472v1",
      "title": "Certified Self-Consistency: Statistical Guarantees and Test-Time Training for Reliable Reasoning in LLMs",
      "abstract": "Recent advances such as self-consistency and test-time reinforcement learning\n(TTRL) improve the reliability of large language models (LLMs) without\nadditional supervision, yet their underlying mechanisms and statistical\nguarantees remain poorly understood. We present a unified framework for\ncertifiable inference in LLMs, showing that majority voting provides a\nstatistical certificate of self-consistency: under mild assumptions, the\naggregated answer coincides with the mode of the model's terminal distribution\nwith high probability. We derive finite-sample and anytime-valid concentration\nbounds that quantify this confidence, and introduce the Martingale Majority\nCertificate (MMC), a sequential stopping rule that adaptively determines when\nsufficient samples have been drawn. We further prove that label-free\npost-training methods such as TTRL implicitly sharpen the answer distribution\nby exponentially tilting it toward its mode, thereby reducing the number of\nsamples required for certification. Building on this insight, we propose new\npost-training objectives that explicitly optimise this trade-off between\nsharpness and bias. Together, these results explain and connect two central\ntest-time scaling strategies, self-consistency and TTRL, within a single\nstatistical framework for label-free, certifiable reliability in reasoning\nLLMs.",
      "authors": [
        {
          "name": "Paula Cordero-Encinar",
          "affiliation": null
        },
        {
          "name": "Andrew B. Duncan",
          "affiliation": null
        }
      ],
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17472v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17472v1",
      "primary_category": "stat.ML",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17469v1",
      "title": "Layer Specialization Underlying Compositional Reasoning in Transformers",
      "abstract": "Transformers exhibit compositional reasoning on sequences not observed during\ntraining, a capability often attributed to in-context learning (ICL) and skill\ncomposition. We investigate this phenomenon using the Random Hierarchy Model\n(RHM), a probabilistic context-free grammar that generates sequences through\nrecursive rule application. Models are trained on subsets of sequences and\nevaluated across four generalization conditions: memorization, in-distribution\ngeneralization, out-of-distribution generalization with the same rules, and\ncross-layer transfer. Behaviorally, performance improves systematically with\ntask complexity and the number of in-context examples, with out-of-distribution\ntasks requiring substantially more examples than in-distribution scenarios.\nMechanistically, we identify a progressive emergence of layer specialization\nduring training that correlates with generalization performance. Principal\ncomponent analysis and attention pattern clustering reveal that transformers\ndevelop structured, hierarchically organized representations in specialized\nlayers. These results demonstrate that transformers develop modular,\ninterpretable mechanisms supporting compositional reasoning, linking internal\nalgorithmic structure to observed behavioral capabilities.",
      "authors": [
        {
          "name": "Jing Liu",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17469v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17469v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17467v1",
      "title": "CrossStateECG: Multi-Scale Deep Convolutional Network with Attention for Rest-Exercise ECG Biometrics",
      "abstract": "Current research in Electrocardiogram (ECG) biometrics mainly emphasizes\nresting-state conditions, leaving the performance decline in rest-exercise\nscenarios largely unresolved. This paper introduces CrossStateECG, a robust\nECG-based authentication model explicitly tailored for cross-state\n(rest-exercise) conditions. The proposed model creatively combines multi-scale\ndeep convolutional feature extraction with attention mechanisms to ensure\nstrong identification across different physiological states. Experimental\nresults on the exercise-ECGID dataset validate the effectiveness of\nCrossStateECG, achieving an identification accuracy of 92.50% in the\nRest-to-Exercise scenario (training on resting ECG and testing on post-exercise\nECG) and 94.72% in the Exercise-to-Rest scenario (training on post-exercise ECG\nand testing on resting ECG). Furthermore, CrossStateECG demonstrates\nexceptional performance across both state combinations, reaching an accuracy of\n99.94% in Rest-to-Rest scenarios and 97.85% in Mixed-to-Mixed scenarios.\nAdditional validations on the ECG-ID and MIT-BIH datasets further confirmed the\ngeneralization abilities of CrossStateECG, underscoring its potential as a\npractical solution for post-exercise ECG-based authentication in dynamic\nreal-world settings.",
      "authors": [
        {
          "name": "Dan Zheng",
          "affiliation": null
        },
        {
          "name": "Jing Feng",
          "affiliation": null
        },
        {
          "name": "Juan Liu",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17467v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17467v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17463v1",
      "title": "Label Indeterminacy in AI & Law",
      "abstract": "Machine learning is increasingly used in the legal domain, where it typically\noperates retrospectively by treating past case outcomes as ground truth.\nHowever, legal outcomes are often shaped by human interventions that are not\ncaptured in most machine learning approaches. A final decision may result from\na settlement, an appeal, or other procedural actions. This creates label\nindeterminacy: the outcome could have been different if the intervention had or\nhad not taken place. We argue that legal machine learning applications need to\naccount for label indeterminacy. Methods exist that can impute these\nindeterminate labels, but they are all grounded in unverifiable assumptions. In\nthe context of classifying cases from the European Court of Human Rights, we\nshow that the way that labels are constructed during training can significantly\naffect model behaviour. We therefore position label indeterminacy as a relevant\nconcern in AI & Law and demonstrate how it can shape model behaviour.",
      "authors": [
        {
          "name": "Cor Steging",
          "affiliation": null
        },
        {
          "name": "Tadeusz Zbiegie\u0144",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.AI"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17463v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17463v1",
      "primary_category": "cs.AI",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17460v1",
      "title": "Evaluating Large Language Models on Urdu Idiom Translation",
      "abstract": "Idiomatic translation remains a significant challenge in machine translation,\nespecially for low resource languages such as Urdu, and has received limited\nprior attention. To advance research in this area, we introduce the first\nevaluation datasets for Urdu to English idiomatic translation, covering both\nNative Urdu and Roman Urdu scripts and annotated with gold-standard English\nequivalents. We evaluate multiple open-source Large Language Models (LLMs) and\nNeural Machine Translation (NMT) systems on this task, focusing on their\nability to preserve idiomatic and cultural meaning. Automatic metrics including\nBLEU, BERTScore, COMET, and XCOMET are used to assess translation quality. Our\nfindings indicate that prompt engineering enhances idiomatic translation\ncompared to direct translation, though performance differences among prompt\ntypes are relatively minor. Moreover, cross script comparisons reveal that text\nrepresentation substantially affects translation quality, with Native Urdu\ninputs producing more accurate idiomatic translations than Roman Urdu.",
      "authors": [
        {
          "name": "Muhammad Farmal Khan",
          "affiliation": null
        },
        {
          "name": "Mousumi Akter",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17460v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17460v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17459v1",
      "title": "Estimating Orbital Parameters of Direct Imaging Exoplanet Using Neural Network",
      "abstract": "In this work, we propose a new flow-matching Markov chain Monte Carlo\n(FM-MCMC) algorithm for estimating the orbital parameters of exoplanetary\nsystems, especially for those only one exoplanet is involved. Compared to\ntraditional methods that rely on random sampling within the Bayesian framework,\nour approach first leverages flow matching posterior estimation (FMPE) to\nefficiently constrain the prior range of physical parameters, and then employs\nMCMC to accurately infer the posterior distribution. For example, in the\norbital parameter inference of beta Pictoris b, our model achieved a\nsubstantial speed-up while maintaining comparable accuracy-running 77.8 times\nfaster than Parallel Tempered MCMC (PTMCMC) and 365.4 times faster than nested\nsampling. Moreover, our FM-MCMC method also attained the highest average\nlog-likelihood among all approaches, demonstrating its superior sampling\nefficiency and accuracy. This highlights the scalability and efficiency of our\napproach, making it well-suited for processing the massive datasets expected\nfrom future exoplanet surveys. Beyond astrophysics, our methodology establishes\na versatile paradigm for synergizing deep generative models with traditional\nsampling, which can be adopted to tackle complex inference problems in other\nfields, such as cosmology, biomedical imaging, and particle physics.",
      "authors": [
        {
          "name": "Bo Liang",
          "affiliation": null
        },
        {
          "name": "Hanlin Song",
          "affiliation": null
        },
        {
          "name": "Chang Liu",
          "affiliation": null
        },
        {
          "name": "Tianyu Zhao",
          "affiliation": null
        },
        {
          "name": "Yuxiang Xu",
          "affiliation": null
        },
        {
          "name": "Zihao Xiao",
          "affiliation": null
        },
        {
          "name": "Manjia Liang",
          "affiliation": null
        },
        {
          "name": "Minghui Du",
          "affiliation": null
        },
        {
          "name": "Wei-Liang Qian",
          "affiliation": null
        },
        {
          "name": "Li-e Qiang",
          "affiliation": null
        },
        {
          "name": "Peng Xu",
          "affiliation": null
        },
        {
          "name": "Ziren Luo",
          "affiliation": null
        }
      ],
      "categories": [
        "astro-ph.EP",
        "astro-ph.GA",
        "cs.LG"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17459v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17459v1",
      "primary_category": "astro-ph.EP",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17458v1",
      "title": "Explainable AI for microseismic event detection",
      "abstract": "Deep neural networks like PhaseNet show high accuracy in detecting\nmicroseismic events, but their black-box nature is a concern in critical\napplications. We apply explainable AI (XAI) techniques, such as\nGradient-weighted Class Activation Mapping (Grad-CAM) and Shapley Additive\nExplanations (SHAP), to interpret the PhaseNet model's decisions and improve\nits reliability. Grad-CAM highlights that the network's attention aligns with\nP- and S-wave arrivals. SHAP values quantify feature contributions, confirming\nthat vertical-component amplitudes drive P-phase picks while horizontal\ncomponents dominate S-phase picks, consistent with geophysical principles.\nLeveraging these insights, we introduce a SHAP-gated inference scheme that\ncombines the model's output with an explanation-based metric to reduce errors.\nOn a test set of 9,000 waveforms, the SHAP-gated model achieved an F1-score of\n0.98 (precision 0.99, recall 0.97), outperforming the baseline PhaseNet\n(F1-score 0.97) and demonstrating enhanced robustness to noise. These results\nshow that XAI can not only interpret deep learning models but also directly\nenhance their performance, providing a template for building trust in automated\nseismic detectors.",
      "authors": [
        {
          "name": "Ayrat Abdullin",
          "affiliation": null
        },
        {
          "name": "Denis Anikiev",
          "affiliation": null
        },
        {
          "name": "Umair bin Waheed",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "physics.geo-ph"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17458v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17458v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17457v1",
      "title": "Deeper with Riemannian Geometry: Overcoming Oversmoothing and Oversquashing for Graph Foundation Models",
      "abstract": "Message Passing Neural Networks (MPNNs) is the building block of graph\nfoundation models, but fundamentally suffer from oversmoothing and\noversquashing. There has recently been a surge of interest in fixing both\nissues. Existing efforts primarily adopt global approaches, which may be\nbeneficial in some regions but detrimental in others, ultimately leading to the\nsuboptimal expressiveness. In this paper, we begin by revisiting oversquashing\nthrough a global measure -- spectral gap $\\lambda$ -- and prove that the\nincrease of $\\lambda$ leads to gradient vanishing with respect to the input\nfeatures, thereby undermining the effectiveness of message passing. Motivated\nby such theoretical insights, we propose a \\textbf{local} approach that\nadaptively adjusts message passing based on local structures. To achieve this,\nwe connect local Riemannian geometry with MPNNs, and establish a novel\nnonhomogeneous boundary condition to address both oversquashing and\noversmoothing. Building on the Robin condition, we design a GBN network with\nlocal bottleneck adjustment, coupled with theoretical guarantees. Extensive\nexperiments on homophilic and heterophilic graphs show the expressiveness of\nGBN. Furthermore, GBN does not exhibit performance degradation even when the\nnetwork depth exceeds $256$ layers.",
      "authors": [
        {
          "name": "Li Sun",
          "affiliation": null
        },
        {
          "name": "Zhenhao Huang",
          "affiliation": null
        },
        {
          "name": "Ming Zhang",
          "affiliation": null
        },
        {
          "name": "Philip S. Yu",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17457v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17457v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17451v1",
      "title": "The Parameterized Complexity of Computing the VC-Dimension",
      "abstract": "The VC-dimension is a fundamental and well-studied measure of the complexity\nof a set system (or hypergraph) that is central to many areas of machine\nlearning. We establish several new results on the complexity of computing the\nVC-dimension. In particular, given a hypergraph\n$\\mathcal{H}=(\\mathcal{V},\\mathcal{E})$, we prove that the naive\n$2^{\\mathcal{O}(|\\mathcal{V}|)}$-time algorithm is asymptotically tight under\nthe Exponential Time Hypothesis (ETH). We then prove that the problem admits a\n1-additive fixed-parameter approximation algorithm when parameterized by the\nmaximum degree of $\\mathcal{H}$ and a fixed-parameter algorithm when\nparameterized by its dimension, and that these are essentially the only such\nexploitable structural parameters. Lastly, we consider a generalization of the\nproblem, formulated using graphs, which captures the VC-dimension of both set\nsystems and graphs. We show that it is fixed-parameter tractable parameterized\nby the treewidth of the graph (which, in the case of set systems, applies to\nthe treewidth of its incidence graph). In contrast with closely related\nproblems whose dependency on the treewidth is necessarily double-exponential\n(assuming the ETH), our algorithm has a relatively low dependency on the\ntreewidth.",
      "authors": [
        {
          "name": "Florent Foucaud",
          "affiliation": null
        },
        {
          "name": "Harmender Gahlawat",
          "affiliation": null
        },
        {
          "name": "Fionn Mc Inerney",
          "affiliation": null
        },
        {
          "name": "Prafullkumar Tale",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CC",
        "cs.AI",
        "cs.DM",
        "cs.LG",
        "math.CO"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17451v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17451v1",
      "primary_category": "cs.CC",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17450v1",
      "title": "Active Inference for an Intelligent Agent in Autonomous Reconnaissance Missions",
      "abstract": "We develop an active inference route-planning method for the autonomous\ncontrol of intelligent agents. The aim is to reconnoiter a geographical area to\nmaintain a common operational picture. To achieve this, we construct an\nevidence map that reflects our current understanding of the situation,\nincorporating both positive and \"negative\" sensor observations of possible\ntarget objects collected over time, and diffusing the evidence across the map\nas time progresses. The generative model of active inference uses\nDempster-Shafer theory and a Gaussian sensor model, which provides input to the\nagent. The generative process employs a Bayesian approach to update a posterior\nprobability distribution. We calculate the variational free energy for all\npositions within the area by assessing the divergence between a pignistic\nprobability distribution of the evidence map and a posterior probability\ndistribution of a target object based on the observations, including the level\nof surprise associated with receiving new observations. Using the free energy,\nwe direct the agents' movements in a simulation by taking an incremental step\ntoward a position that minimizes the free energy. This approach addresses the\nchallenge of exploration and exploitation, allowing agents to balance searching\nextensive areas of the geographical map while tracking identified target\nobjects.",
      "authors": [
        {
          "name": "Johan Schubert",
          "affiliation": null
        },
        {
          "name": "Farzad Kamrani",
          "affiliation": null
        },
        {
          "name": "Tove Gustavi",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.AI",
        "H.4.2; I.2.3; I.2.6; I.2.8; I.2.9; J.7"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17450v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17450v1",
      "primary_category": "cs.AI",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17440v1",
      "title": "Rethinking Nighttime Image Deraining via Learnable Color Space Transformation",
      "abstract": "Compared to daytime image deraining, nighttime image deraining poses\nsignificant challenges due to inherent complexities of nighttime scenarios and\nthe lack of high-quality datasets that accurately represent the coupling effect\nbetween rain and illumination. In this paper, we rethink the task of nighttime\nimage deraining and contribute a new high-quality benchmark, HQ-NightRain,\nwhich offers higher harmony and realism compared to existing datasets. In\naddition, we develop an effective Color Space Transformation Network (CST-Net)\nfor better removing complex rain from nighttime scenes. Specifically, we\npropose a learnable color space converter (CSC) to better facilitate rain\nremoval in the Y channel, as nighttime rain is more pronounced in the Y channel\ncompared to the RGB color space. To capture illumination information for\nguiding nighttime deraining, implicit illumination guidance is introduced\nenabling the learned features to improve the model's robustness in complex\nscenarios. Extensive experiments show the value of our dataset and the\neffectiveness of our method. The source code and datasets are available at\nhttps://github.com/guanqiyuan/CST-Net.",
      "authors": [
        {
          "name": "Qiyuan Guan",
          "affiliation": null
        },
        {
          "name": "Xiang Chen",
          "affiliation": null
        },
        {
          "name": "Guiyue Jin",
          "affiliation": null
        },
        {
          "name": "Jiyu Jin",
          "affiliation": null
        },
        {
          "name": "Shumin Fan",
          "affiliation": null
        },
        {
          "name": "Tianyu Song",
          "affiliation": null
        },
        {
          "name": "Jinshan Pan",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17440v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17440v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17439v1",
      "title": "From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Priors",
      "abstract": "Existing vision-language-action (VLA) models act in 3D real-world but are\ntypically built on 2D encoders, leaving a spatial reasoning gap that limits\ngeneralization and adaptability. Recent 3D integration techniques for VLAs\neither require specialized sensors and transfer poorly across modalities, or\ninject weak cues that lack geometry and degrade vision-language alignment. In\nthis work, we introduce FALCON (From Spatial to Action), a novel paradigm that\ninjects rich 3D spatial tokens into the action head. FALCON leverages spatial\nfoundation models to deliver strong geometric priors from RGB alone, and\nincludes an Embodied Spatial Model that can optionally fuse depth, or pose for\nhigher fidelity when available, without retraining or architectural changes. To\npreserve language reasoning, spatial tokens are consumed by a Spatial-Enhanced\nAction Head rather than being concatenated into the vision-language backbone.\nThese designs enable FALCON to address limitations in spatial representation,\nmodality transferability, and alignment. In comprehensive evaluations across\nthree simulation benchmarks and eleven real-world tasks, our proposed FALCON\nachieves state-of-the-art performance, consistently surpasses competitive\nbaselines, and remains robust under clutter, spatial-prompt conditioning, and\nvariations in object scale and height.",
      "authors": [
        {
          "name": "Zhengshen Zhang",
          "affiliation": null
        },
        {
          "name": "Hao Li",
          "affiliation": null
        },
        {
          "name": "Yalun Dai",
          "affiliation": null
        },
        {
          "name": "Zhengbang Zhu",
          "affiliation": null
        },
        {
          "name": "Lei Zhou",
          "affiliation": null
        },
        {
          "name": "Chenchen Liu",
          "affiliation": null
        },
        {
          "name": "Dong Wang",
          "affiliation": null
        },
        {
          "name": "Francis E. H. Tay",
          "affiliation": null
        },
        {
          "name": "Sijin Chen",
          "affiliation": null
        },
        {
          "name": "Ziwei Liu",
          "affiliation": null
        },
        {
          "name": "Yuxiao Liu",
          "affiliation": null
        },
        {
          "name": "Xinghang Li",
          "affiliation": null
        },
        {
          "name": "Pan Zhou",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17439v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17439v1",
      "primary_category": "cs.RO",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17437v1",
      "title": "Multilingual Clinical NER for Diseases and Medications Recognition in Cardiology Texts using BERT Embeddings",
      "abstract": "The rapidly increasing volume of electronic health record (EHR) data\nunderscores a pressing need to unlock biomedical knowledge from unstructured\nclinical texts to support advancements in data-driven clinical systems,\nincluding patient diagnosis, disease progression monitoring, treatment effects\nassessment, prediction of future clinical events, etc. While contextualized\nlanguage models have demonstrated impressive performance improvements for named\nentity recognition (NER) systems in English corpora, there remains a scarcity\nof research focused on clinical texts in low-resource languages. To bridge this\ngap, our study aims to develop multiple deep contextual embedding models to\nenhance clinical NER in the cardiology domain, as part of the BioASQ\nMultiCardioNER shared task. We explore the effectiveness of different\nmonolingual and multilingual BERT-based models, trained on general domain text,\nfor extracting disease and medication mentions from clinical case reports\nwritten in English, Spanish, and Italian. We achieved an F1-score of 77.88% on\nSpanish Diseases Recognition (SDR), 92.09% on Spanish Medications Recognition\n(SMR), 91.74% on English Medications Recognition (EMR), and 88.9% on Italian\nMedications Recognition (IMR). These results outperform the mean and median F1\nscores in the test leaderboard across all subtasks, with the mean/median values\nbeing: 69.61%/75.66% for SDR, 81.22%/90.18% for SMR, 89.2%/88.96% for EMR, and\n82.8%/87.76% for IMR.",
      "authors": [
        {
          "name": "Manuela Daniela Danu",
          "affiliation": null
        },
        {
          "name": "George Marica",
          "affiliation": null
        },
        {
          "name": "Constantin Suciu",
          "affiliation": null
        },
        {
          "name": "Lucian Mihai Itu",
          "affiliation": null
        },
        {
          "name": "Oladimeji Farri",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17437v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17437v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17434v1",
      "title": "Leveraging AV1 motion vectors for Fast and Dense Feature Matching",
      "abstract": "We repurpose AV1 motion vectors to produce dense sub-pixel correspondences\nand short tracks filtered by cosine consistency. On short videos, this\ncompressed-domain front end runs comparably to sequential SIFT while using far\nless CPU, and yields denser matches with competitive pairwise geometry. As a\nsmall SfM demo on a 117-frame clip, MV matches register all images and\nreconstruct 0.46-0.62M points at 0.51-0.53,px reprojection error; BA time grows\nwith match density. These results show compressed-domain correspondences are a\npractical, resource-efficient front end with clear paths to scaling in full\npipelines.",
      "authors": [
        {
          "name": "Julien Zouein",
          "affiliation": null
        },
        {
          "name": "Hossein Javidnia",
          "affiliation": null
        },
        {
          "name": "Fran\u00e7ois Piti\u00e9",
          "affiliation": null
        },
        {
          "name": "Anil Kokaram",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17434v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17434v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17431v1",
      "title": "Agentic Reinforcement Learning for Search is Unsafe",
      "abstract": "Agentic reinforcement learning (RL) trains large language models to\nautonomously call tools during reasoning, with search as the most common\napplication. These models excel at multi-step reasoning tasks, but their safety\nproperties are not well understood. In this study, we show that RL-trained\nsearch models inherit refusal from instruction tuning and often deflect harmful\nrequests by turning them into safe queries. However, this safety is fragile.\nTwo simple attacks, one that forces the model to begin response with search\n(Search attack), another that encourages models to repeatedly search\n(Multi-search attack), trigger cascades of harmful searches and answers. Across\ntwo model families (Qwen, Llama) with both local and web search, these attacks\nlower refusal rates by up to 60.0%, answer safety by 82.5%, and search-query\nsafety by 82.4%. The attacks succeed by triggering models to generate harmful,\nrequest-mirroring search queries before they can generate the inherited refusal\ntokens. This exposes a core weakness of current RL training: it rewards\ncontinued generation of effective queries without accounting for their\nharmfulness. As a result, RL search models have vulnerabilities that users can\neasily exploit, making it urgent to develop safety-aware agentic RL pipelines\noptimising for safe search.",
      "authors": [
        {
          "name": "Yushi Yang",
          "affiliation": null
        },
        {
          "name": "Shreyansh Padarha",
          "affiliation": null
        },
        {
          "name": "Andrew Lee",
          "affiliation": null
        },
        {
          "name": "Adam Mahdi",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17431v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17431v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17426v1",
      "title": "Navigating the Alignment-Calibration Trade-off: A Pareto-Superior Frontier via Model Merging",
      "abstract": "The \"alignment tax\" of post-training is typically framed as a drop in task\naccuracy. We show it also involves a severe loss of calibration, making models\noverconfident, less reliable, and model outputs less diverse. We show that this\ntrade-off can be navigated effectively via a simple post-hoc intervention:\ninterpolating between a model's weights before and after alignment. Crucially,\nthis is not a strict trade-off. We find that the process consistently reveals\nPareto-optimal interpolations - models that improve accuracy beyond both\nparents while substantially recovering the calibration lost during alignment.\nOur work demonstrates that simple model merging provides a computationally\nefficient method for mitigating the full scope of the alignment tax, yielding\nmodels that are more capable and more reliable.",
      "authors": [
        {
          "name": "Tiancheng Hu",
          "affiliation": null
        },
        {
          "name": "Benjamin Minixhofer",
          "affiliation": null
        },
        {
          "name": "Nigel Collier",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17426v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17426v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17425v1",
      "title": "Quantifying Climate Policy Action and Its Links to Development Outcomes: A Cross-National Data-Driven Analysis",
      "abstract": "Addressing climate change effectively requires more than cataloguing the\nnumber of policies in place; it calls for tools that can reveal their thematic\npriorities and their tangible impacts on development outcomes. Existing\nassessments often rely on qualitative descriptions or composite indices, which\ncan mask crucial differences between key domains such as mitigation,\nadaptation, disaster risk management, and loss and damage. To bridge this gap,\nwe develop a quantitative indicator of climate policy orientation by applying a\nmultilingual transformer-based language model to official national policy\ndocuments, achieving a classification accuracy of 0.90 (F1-score). Linking\nthese indicators with World Bank development data in panel regressions reveals\nthat mitigation policies are associated with higher GDP and GNI; disaster risk\nmanagement correlates with greater GNI and debt but reduced foreign direct\ninvestment; adaptation and loss and damage show limited measurable effects.\nThis integrated NLP-econometric framework enables comparable, theme-specific\nanalysis of climate governance, offering a scalable method to monitor progress,\nevaluate trade-offs, and align policy emphasis with development goals.",
      "authors": [
        {
          "name": "Aditi Dutta",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CY",
        "cs.LG"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17425v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17425v1",
      "primary_category": "cs.CY",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17422v1",
      "title": "DeepDetect: Learning All-in-One Dense Keypoints",
      "abstract": "Keypoint detection is the foundation of many computer vision tasks, including\nimage registration, structure-from motion, 3D reconstruction, visual odometry,\nand SLAM. Traditional detectors (SIFT, SURF, ORB, BRISK, etc.) and learning\nbased methods (SuperPoint, R2D2, LF-Net, D2-Net, etc.) have shown strong\nperformance yet suffer from key limitations: sensitivity to photometric\nchanges, low keypoint density and repeatability, limited adaptability to\nchallenging scenes, and lack of semantic understanding, often failing to\nprioritize visually important regions. We present DeepDetect, an intelligent,\nall-in-one, dense keypoint detector that unifies the strengths of classical\ndetectors using deep learning. Firstly, we create ground-truth masks by fusing\noutputs of 7 keypoint and 2 edge detectors, extracting diverse visual cues from\ncorners and blobs to prominent edges and textures in the images. Afterwards, a\nlightweight and efficient model: ESPNet, is trained using these masks as\nlabels, enabling DeepDetect to focus semantically on images while producing\nhighly dense keypoints, that are adaptable to diverse and visually degraded\nconditions. Evaluations on the Oxford Affine Covariant Regions dataset\ndemonstrate that DeepDetect surpasses other detectors in keypoint density,\nrepeatability, and the number of correct matches, achieving maximum values of\n0.5143 (average keypoint density), 0.9582 (average repeatability), and 59,003\n(correct matches).",
      "authors": [
        {
          "name": "Shaharyar Ahmed Khan Tareen",
          "affiliation": null
        },
        {
          "name": "Filza Khan Tareen",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17422v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17422v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17421v1",
      "title": "Diffusion Models as Dataset Distillation Priors",
      "abstract": "Dataset distillation aims to synthesize compact yet informative datasets from\nlarge ones. A significant challenge in this field is achieving a trifecta of\ndiversity, generalization, and representativeness in a single distilled\ndataset. Although recent generative dataset distillation methods adopt powerful\ndiffusion models as their foundation models, the inherent representativeness\nprior in diffusion models is overlooked. Consequently, these approaches often\nnecessitate the integration of external constraints to enhance data quality. To\naddress this, we propose Diffusion As Priors (DAP), which formalizes\nrepresentativeness by quantifying the similarity between synthetic and real\ndata in feature space using a Mercer kernel. We then introduce this prior as\nguidance to steer the reverse diffusion process, enhancing the\nrepresentativeness of distilled samples without any retraining. Extensive\nexperiments on large-scale datasets, such as ImageNet-1K and its subsets,\ndemonstrate that DAP outperforms state-of-the-art methods in generating\nhigh-fidelity datasets while achieving superior cross-architecture\ngeneralization. Our work not only establishes a theoretical connection between\ndiffusion priors and the objectives of dataset distillation but also provides a\npractical, training-free framework for improving the quality of the distilled\ndataset.",
      "authors": [
        {
          "name": "Duo Su",
          "affiliation": null
        },
        {
          "name": "Huyu Wu",
          "affiliation": null
        },
        {
          "name": "Huanran Chen",
          "affiliation": null
        },
        {
          "name": "Yiming Shi",
          "affiliation": null
        },
        {
          "name": "Yuzhu Wang",
          "affiliation": null
        },
        {
          "name": "Xi Ye",
          "affiliation": null
        },
        {
          "name": "Jun Zhu",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17421v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17421v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17418v1",
      "title": "Diverse Planning with Simulators via Linear Temporal Logic",
      "abstract": "Autonomous agents rely on automated planning algorithms to achieve their\nobjectives. Simulation-based planning offers a significant advantage over\ndeclarative models in modelling complex environments. However, relying solely\non a planner that produces a single plan may not be practical, as the generated\nplans may not always satisfy the agent's preferences. To address this\nlimitation, we introduce $\\texttt{FBI}_\\texttt{LTL}$, a diverse planner\nexplicitly designed for simulation-based planning problems.\n$\\texttt{FBI}_\\texttt{LTL}$ utilises Linear Temporal Logic (LTL) to define\nsemantic diversity criteria, enabling agents to specify what constitutes\nmeaningfully different plans. By integrating these LTL-based diversity models\ndirectly into the search process, $\\texttt{FBI}_\\texttt{LTL}$ ensures the\ngeneration of semantically diverse plans, addressing a critical limitation of\nexisting diverse planning approaches that may produce syntactically different\nbut semantically identical solutions. Extensive evaluations on various\nbenchmarks consistently demonstrate that $\\texttt{FBI}_\\texttt{LTL}$ generates\nmore diverse plans compared to a baseline approach. This work establishes the\nfeasibility of semantically-guided diverse planning in simulation-based\nenvironments, paving the way for innovative approaches in realistic,\nnon-symbolic domains where traditional model-based approaches fail.",
      "authors": [
        {
          "name": "Mustafa F. Abdelwahed",
          "affiliation": null
        },
        {
          "name": "Alice Toniolo",
          "affiliation": null
        },
        {
          "name": "Joan Espasa",
          "affiliation": null
        },
        {
          "name": "Ian P. Gent",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17418v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17418v1",
      "primary_category": "cs.AI",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17415v1",
      "title": "BenCao: An Instruction-Tuned Large Language Model for Traditional Chinese Medicine",
      "abstract": "Traditional Chinese Medicine (TCM), with a history spanning over two\nmillennia, plays a role in global healthcare. However, applying large language\nmodels (LLMs) to TCM remains challenging due to its reliance on holistic\nreasoning, implicit logic, and multimodal diagnostic cues. Existing TCM-domain\nLLMs have made progress in text-based understanding but lack multimodal\nintegration, interpretability, and clinical applicability. To address these\nlimitations, we developed BenCao, a ChatGPT-based multimodal assistant for TCM,\nintegrating structured knowledge bases, diagnostic data, and expert feedback\nrefinement. BenCao was trained through natural language instruction tuning\nrather than parameter retraining, aligning with expert-level reasoning and\nethical norms specific to TCM. The system incorporates a comprehensive\nknowledge base of over 1,000 classical and modern texts, a scenario-based\ninstruction framework for diverse interactions, a chain-of-thought simulation\nmechanism for interpretable reasoning, and a feedback refinement process\ninvolving licensed TCM practitioners. BenCao connects to external APIs for\ntongue-image classification and multimodal database retrieval, enabling dynamic\naccess to diagnostic resources. In evaluations across single-choice question\nbenchmarks and multimodal classification tasks, BenCao achieved superior\naccuracy to general-domain and TCM-domain models, particularly in diagnostics,\nherb recognition, and constitution classification. The model was deployed as an\ninteractive application on the OpenAI GPTs Store, accessed by nearly 1,000\nusers globally as of October 2025. This study demonstrates the feasibility of\ndeveloping a TCM-domain LLM through natural language-based instruction tuning\nand multimodal integration, offering a practical framework for aligning\ngenerative AI with traditional medical reasoning and a scalable pathway for\nreal-world deployment.",
      "authors": [
        {
          "name": "Jiacheng Xie",
          "affiliation": null
        },
        {
          "name": "Yang Yu",
          "affiliation": null
        },
        {
          "name": "Yibo Chen",
          "affiliation": null
        },
        {
          "name": "Hanyao Zhang",
          "affiliation": null
        },
        {
          "name": "Lening Zhao",
          "affiliation": null
        },
        {
          "name": "Jiaxuan He",
          "affiliation": null
        },
        {
          "name": "Lei Jiang",
          "affiliation": null
        },
        {
          "name": "Xiaoting Tang",
          "affiliation": null
        },
        {
          "name": "Guanghui An",
          "affiliation": null
        },
        {
          "name": "Dong Xu",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.MA",
        "cs.MM",
        "cs.SE"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17415v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17415v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17414v1",
      "title": "A Conditional Diffusion Model for Probabilistic Prediction of Battery Capacity Degradation",
      "abstract": "Accurate prediction of lithium-ion battery capacity and its associated\nuncertainty is essential for reliable battery management but remains\nchallenging due to the stochastic nature of aging. This paper presents a novel\nmethod, termed the Condition Diffusion U-Net with Attention (CDUA), which\nintegrates feature engineering and deep learning to address this challenge. The\nproposed approach employs a diffusion-based generative model for time-series\nforecasting and incorporates attention mechanisms to enhance predictive\nperformance. Battery capacity is first derived from real-world vehicle\noperation data. The most relevant features are then identified using the\nPearson correlation coefficient and the XGBoost algorithm. These features are\nused to train the CDUA model, which comprises two core components: (1) a\ncontextual U-Net with self-attention to capture complex temporal dependencies,\nand (2) a denoising network to reconstruct accurate capacity values from noisy\nobservations. Experimental validation on the real-world vehicle data\ndemonstrates that the proposed CDUA model achieves a relative Mean Absolute\nError (MAE) of 0.94% and a relative Root Mean Square Error (RMSE) of 1.14%,\nwith a narrow 95% confidence interval of 3.74% in relative width. These results\nconfirm that CDUA provides both accurate capacity estimation and reliable\nuncertainty quantification. Comparative experiments further verify its\nrobustness and superior performance over existing mainstream approaches.",
      "authors": [
        {
          "name": "Hequn Li",
          "affiliation": null
        },
        {
          "name": "Zhongwei Deng",
          "affiliation": null
        },
        {
          "name": "Chunlin Jiang",
          "affiliation": null
        },
        {
          "name": "Yvxin He andZhansheng Ning",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17414v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17414v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17409v1",
      "title": "Monitoring Horses in Stalls: From Object to Event Detection",
      "abstract": "Monitoring the behavior of stalled horses is essential for early detection of\nhealth and welfare issues but remains labor-intensive and time-consuming. In\nthis study, we present a prototype vision-based monitoring system that\nautomates the detection and tracking of horses and people inside stables using\nobject detection and multi-object tracking techniques. The system leverages\nYOLOv11 and BoT-SORT for detection and tracking, while event states are\ninferred based on object trajectories and spatial relations within the stall.\nTo support development, we constructed a custom dataset annotated with\nassistance from foundation models CLIP and GroundingDINO. The system\ndistinguishes between five event types and accounts for the camera's blind\nspots. Qualitative evaluation demonstrated reliable performance for\nhorse-related events, while highlighting limitations in detecting people due to\ndata scarcity. This work provides a foundation for real-time behavioral\nmonitoring in equine facilities, with implications for animal welfare and\nstable management.",
      "authors": [
        {
          "name": "Dmitrii Galimzianov",
          "affiliation": null
        },
        {
          "name": "Viacheslav Vyshegorodtsev",
          "affiliation": null
        },
        {
          "name": "Ivan Nezhivykh",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17409v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17409v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17406v1",
      "title": "S4ECG: Exploring the impact of long-range interactions for arrhythmia prediction",
      "abstract": "The electrocardiogram (ECG) exemplifies biosignal-based time series with\ncontinuous, temporally ordered structure reflecting cardiac physiological and\npathophysiological dynamics. Detailed analysis of these dynamics has proven\nchallenging, as conventional methods capture either global trends or local\nwaveform features but rarely their simultaneous interplay at high temporal\nresolution. To bridge global and local signal analysis, we introduce S4ECG, a\nnovel deep learning architecture leveraging structured state space models for\nmulti-epoch arrhythmia classification. Our joint multi-epoch predictions\nsignificantly outperform single-epoch approaches by 1.0-11.6% in macro-AUROC,\nwith atrial fibrillation specificity improving from 0.718-0.979 to 0.967-0.998,\ndemonstrating superior performance in-distribution and enhanced\nout-of-distribution robustness. Systematic investigation reveals optimal\ntemporal dependency windows spanning 10-20 minutes for peak performance. This\nwork contributes to a paradigm shift toward temporally-aware arrhythmia\ndetection algorithms, opening new possibilities for ECG interpretation, in\nparticular for complex arrhythmias like atrial fibrillation and atrial flutter.",
      "authors": [
        {
          "name": "Tiezhi Wang",
          "affiliation": null
        },
        {
          "name": "Wilhelm Haverkamp",
          "affiliation": null
        },
        {
          "name": "Nils Strodthoff",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "eess.SP"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17406v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17406v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17405v1",
      "title": "AFRICAPTION: Establishing a New Paradigm for Image Captioning in African Languages",
      "abstract": "Multimodal AI research has overwhelmingly focused on high-resource languages,\nhindering the democratization of advancements in the field. To address this, we\npresent AfriCaption, a comprehensive framework for multilingual image\ncaptioning in 20 African languages and our contributions are threefold: (i) a\ncurated dataset built on Flickr8k, featuring semantically aligned captions\ngenerated via a context-aware selection and translation process; (ii) a\ndynamic, context-preserving pipeline that ensures ongoing quality through model\nensembling and adaptive substitution; and (iii) the AfriCaption model, a 0.5B\nparameter vision-to-text architecture that integrates SigLIP and NLLB200 for\ncaption generation across under-represented languages. This unified framework\nensures ongoing data quality and establishes the first scalable\nimage-captioning resource for under-represented African languages, laying the\ngroundwork for truly inclusive multimodal AI.",
      "authors": [
        {
          "name": "Mardiyyah Oduwole",
          "affiliation": null
        },
        {
          "name": "Prince Mireku",
          "affiliation": null
        },
        {
          "name": "Fatimo Adebanjo",
          "affiliation": null
        },
        {
          "name": "Oluwatosin Olajide",
          "affiliation": null
        },
        {
          "name": "Mahi Aminu Aliyu",
          "affiliation": null
        },
        {
          "name": "Jekaterina Novikova",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17405v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17405v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17402v1",
      "title": "Leveraging Group Relative Policy Optimization to Advance Large Language Models in Traditional Chinese Medicine",
      "abstract": "Traditional Chinese Medicine (TCM) presents a rich and structurally unique\nknowledge system that challenges conventional applications of large language\nmodels (LLMs). Although previous TCM-specific LLMs have shown progress through\nsupervised fine-tuning, they often face limitations in alignment, data quality,\nand evaluation consistency. In this study, we introduce Ladder-base, the first\nTCM-focused LLM trained with Group Relative Policy Optimization (GRPO), a\nreinforcement learning method that improves reasoning and factual consistency\nby optimizing response selection based on intra-group comparisons. Ladder-base\nis built upon the Qwen2.5-7B-Instruct foundation model and trained exclusively\non the textual subset of the TCM-Ladder benchmark, using 80 percent of the data\nfor training and the remaining 20 percent split evenly between validation and\ntest sets. Through standardized evaluation, Ladder-base demonstrates superior\nperformance across multiple reasoning metrics when compared to both\nstate-of-the-art general-purpose LLMs such as GPT-4, Gemini 2.5, Claude 3, and\nQwen3 and domain-specific TCM models including BenTsao, HuatuoGPT2, and\nZhongjing. These findings suggest that GRPO provides an effective and efficient\nstrategy for aligning LLMs with expert-level reasoning in traditional medical\ndomains and supports the development of trustworthy and clinically grounded TCM\nartificial intelligence systems.",
      "authors": [
        {
          "name": "Jiacheng Xie",
          "affiliation": null
        },
        {
          "name": "Shuai Zeng",
          "affiliation": null
        },
        {
          "name": "Yang Yu",
          "affiliation": null
        },
        {
          "name": "Xiaoting Tang",
          "affiliation": null
        },
        {
          "name": "Guanghui An",
          "affiliation": null
        },
        {
          "name": "Dong Xu",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17402v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17402v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17396v1",
      "title": "RINS-T: Robust Implicit Neural Solvers for Time Series Linear Inverse Problems",
      "abstract": "Time series data are often affected by various forms of corruption, such as\nmissing values, noise, and outliers, which pose significant challenges for\ntasks such as forecasting and anomaly detection. To address these issues,\ninverse problems focus on reconstructing the original signal from corrupted\ndata by leveraging prior knowledge about its underlying structure. While deep\nlearning methods have demonstrated potential in this domain, they often require\nextensive pretraining and struggle to generalize under distribution shifts. In\nthis work, we propose RINS-T (Robust Implicit Neural Solvers for Time Series\nLinear Inverse Problems), a novel deep prior framework that achieves high\nrecovery performance without requiring pretraining data. RINS-T leverages\nneural networks as implicit priors and integrates robust optimization\ntechniques, making it resilient to outliers while relaxing the reliance on\nGaussian noise assumptions. To further improve optimization stability and\nrobustness, we introduce three key innovations: guided input initialization,\ninput perturbation, and convex output combination techniques. Each of these\ncontributions strengthens the framework's optimization stability and\nrobustness. These advancements make RINS-T a flexible and effective solution\nfor addressing complex real-world time series challenges. Our code is available\nat https://github.com/EPFL-IMOS/RINS-T.",
      "authors": [
        {
          "name": "Keivan Faghih Niresi",
          "affiliation": null
        },
        {
          "name": "Zepeng Zhang",
          "affiliation": null
        },
        {
          "name": "Olga Fink",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "eess.SP",
        "stat.ML"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17396v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17396v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17394v1",
      "title": "MILES: Modality-Informed Learning Rate Scheduler for Balancing Multimodal Learning",
      "abstract": "The aim of multimodal neural networks is to combine diverse data sources,\nreferred to as modalities, to achieve enhanced performance compared to relying\non a single modality. However, training of multimodal networks is typically\nhindered by modality overfitting, where the network relies excessively on one\nof the available modalities. This often yields sub-optimal performance,\nhindering the potential of multimodal learning and resulting in marginal\nimprovements relative to unimodal models. In this work, we present the\nModality-Informed Learning ratE Scheduler (MILES) for training multimodal joint\nfusion models in a balanced manner. MILES leverages the differences in\nmodality-wise conditional utilization rates during training to effectively\nbalance multimodal learning. The learning rate is dynamically adjusted during\ntraining to balance the speed of learning from each modality by the multimodal\nmodel, aiming for enhanced performance in both multimodal and unimodal\npredictions. We extensively evaluate MILES on four multimodal joint fusion\ntasks and compare its performance to seven state-of-the-art baselines. Our\nresults show that MILES outperforms all baselines across all tasks and fusion\nmethods considered in our study, effectively balancing modality usage during\ntraining. This results in improved multimodal performance and stronger modality\nencoders, which can be leveraged when dealing with unimodal samples or absent\nmodalities. Overall, our work highlights the impact of balancing multimodal\nlearning on improving model performance.",
      "authors": [
        {
          "name": "Alejandro Guerra-Manzanares",
          "affiliation": null
        },
        {
          "name": "Farah E. Shamout",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17394v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17394v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17391v1",
      "title": "Finite-Time Bounds for Average-Reward Fitted Q-Iteration",
      "abstract": "Although there is an extensive body of work characterizing the sample\ncomplexity of discounted-return offline RL with function approximations, prior\nwork on the average-reward setting has received significantly less attention,\nand existing approaches rely on restrictive assumptions, such as ergodicity or\nlinearity of the MDP. In this work, we establish the first sample complexity\nresults for average-reward offline RL with function approximation for weakly\ncommunicating MDPs, a much milder assumption. To this end, we introduce\nAnchored Fitted Q-Iteration, which combines the standard Fitted Q-Iteration\nwith an anchor mechanism. We show that the anchor, which can be interpreted as\na form of weight decay, is crucial for enabling finite-time analysis in the\naverage-reward setting. We also extend our finite-time analysis to the setup\nwhere the dataset is generated from a single-trajectory rather than IID\ntransitions, again leveraging the anchor mechanism.",
      "authors": [
        {
          "name": "Jongmin Lee",
          "affiliation": null
        },
        {
          "name": "Ernest K. Ryu",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17391v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17391v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17390v1",
      "title": "Exploration via Feature Perturbation in Contextual Bandits",
      "abstract": "We propose feature perturbation, a simple yet powerful technique that injects\nrandomness directly into feature inputs, instead of randomizing unknown\nparameters or adding noise to rewards. Remarkably, this algorithm achieves\n$\\tilde{\\mathcal{O}}(d\\sqrt{T})$ worst-case regret bound for generalized linear\nbandits, while avoiding the $\\tilde{\\mathcal{O}}(d^{3/2}\\sqrt{T})$ regret\ntypical of existing randomized bandit algorithms. Because our algorithm eschews\nparameter sampling, it is both computationally efficient and naturally extends\nto non-parametric or neural network models. We verify these advantages through\nempirical evaluations, demonstrating that feature perturbation not only\nsurpasses existing methods but also unifies strong practical performance with\nbest-known theoretical guarantees.",
      "authors": [
        {
          "name": "Seouh-won Yi",
          "affiliation": null
        },
        {
          "name": "Min-hwan Oh",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17390v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17390v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17389v1",
      "title": "EduAdapt: A Question Answer Benchmark Dataset for Evaluating Grade-Level Adaptability in LLMs",
      "abstract": "Large language models (LLMs) are transforming education by answering\nquestions, explaining complex concepts, and generating content across a wide\nrange of subjects. Despite strong performance on academic benchmarks, they\noften fail to tailor responses to students' grade levels. This is a critical\nneed in K-12 education, where age-appropriate vocabulary and explanation are\nessential for effective learning. Existing models frequently produce outputs\nthat are too advanced or vague for younger learners, and there are no\nstandardized benchmarks to evaluate their ability to adjust across cognitive\nand developmental stages. To address this gap, we introduce EduAdapt, a\nbenchmark of nearly 48k grade-labeled QA pairs across nine science subjects,\nspanning Grades 1-12 and grouped into four grade levels. We evaluate a diverse\nset of open-source LLMs on EduAdapt and find that while larger models generally\nperform better, they still struggle with generating suitable responses for\nearly-grade students (Grades 1-5). Our work presents the first dataset and\nevaluation framework for assessing grade-level adaptability in LLMs, aiming to\nfoster more developmentally aligned educational AI systems through better\ntraining and prompting strategies. EduAdapt code and datasets are publicly\navailable at https://github.com/NaumanNaeem/EduAdapt.",
      "authors": [
        {
          "name": "Numaan Naeem",
          "affiliation": null
        },
        {
          "name": "Abdellah El Mekki",
          "affiliation": null
        },
        {
          "name": "Muhammad Abdul-Mageed",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL",
        "cs.AI",
        "I.2.7"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17389v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17389v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17388v1",
      "title": "The Atomic Instruction Gap: Instruction-Tuned LLMs Struggle with Simple, Self-Contained Directives",
      "abstract": "Instruction-tuned large language models (IT-LLMs) exhibit strong zero-shot\nreasoning, yet their ability to execute simple, self-contained instructions\nremains underexplored, despite this being foundational to complex\ninstruction-following. We evaluate 20 IT-LLMs on modified MMLU and MMLU-Pro\nbenchmarks, by systematically varying the format of option labels (alphabetic,\nnumeric, Roman) while keeping their meaning identical under four paradigms,\nnamely: (1) With explicit instructions, label changes cause large performance\nshifts (e.g., -30.45\\% for Roman vs. numeric), revealing instruction-format\nbias. (2) Without instructions, performance drops further (up to -10.84\\%) and\nlabel sensitivity intensifies, underscoring the role of explicit guidance. (3)\nWhen option contents are removed, models fail random-choice baselines except\nwith numeric labels, suggesting weak adherence to atomic directives. (4)\nThree-shot exemplars yield no significant gains in robustness or fidelity, and\ngeneration analyses show persistent label errors, especially for non-numeric\nformats. Across model sizes, larger LLMs achieve higher accuracy but remain\ninconsistent in instruction adherence. These results expose the insufficiencies\nof current instruction-tuning paradigms and highlight the need for evaluation\nmethods and training strategies that explicitly target atomic\ninstruction-following.",
      "authors": [
        {
          "name": "Henry Lim",
          "affiliation": null
        },
        {
          "name": "Kwan Hui Lim",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17388v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17388v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17386v1",
      "title": "Inference of Deterministic Finite Automata via Q-Learning",
      "abstract": "Traditional approaches to inference of deterministic finite-state automata\n(DFA) stem from symbolic AI, including both active learning methods (e.g.,\nAngluin's L* algorithm and its variants) and passive techniques (e.g., Biermann\nand Feldman's method, RPNI). Meanwhile, sub-symbolic AI, particularly machine\nlearning, offers alternative paradigms for learning from data, such as\nsupervised, unsupervised, and reinforcement learning (RL). This paper\ninvestigates the use of Q-learning, a well-known reinforcement learning\nalgorithm, for the passive inference of deterministic finite automata. It\nbuilds on the core insight that the learned Q-function, which maps state-action\npairs to rewards, can be reinterpreted as the transition function of a DFA over\na finite domain. This provides a novel bridge between sub-symbolic learning and\nsymbolic representations. The paper demonstrates how Q-learning can be adapted\nfor automaton inference and provides an evaluation on several examples.",
      "authors": [
        {
          "name": "Elaheh Hosseinkhani",
          "affiliation": null
        },
        {
          "name": "Martin Leucker",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.FL",
        "cs.AI"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17386v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17386v1",
      "primary_category": "cs.FL",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17385v1",
      "title": "TabR1: Taming GRPO for tabular reasoning LLMs",
      "abstract": "Tabular prediction has traditionally relied on gradient-boosted decision\ntrees and specialized deep learning models, which excel within tasks but\nprovide limited interpretability and weak transfer across tables. Reasoning\nlarge language models (LLMs) promise cross-task adaptability with trans- parent\nreasoning traces, yet their potential has not been fully realized for tabular\ndata. This paper presents TabR1, the first reasoning LLM for tabular prediction\nwith multi-step reasoning. At its core is Permutation Relative Policy\nOptimization (PRPO), a simple yet efficient reinforcement learning method that\nencodes column-permutation invariance as a structural prior. By construct- ing\nmultiple label-preserving permutations per sample and estimating advantages\nboth within and across permutations, PRPO transforms sparse rewards into dense\nlearning signals and improves generalization. With limited supervision, PRPO\nactivates the reasoning ability of LLMs for tabular prediction, enhancing\nfew-shot and zero-shot performance as well as interpretability. Comprehensive\nexperiments demonstrate that TabR1 achieves performance comparable to strong\nbaselines under full-supervision fine-tuning. In the zero-shot setting, TabR1\napproaches the performance of strong baselines under the 32-shot setting.\nMoreover, TabR1 (8B) substantially outperforms much larger LLMs across various\ntasks, achieving up to 53.17% improvement over DeepSeek-R1 (685B).",
      "authors": [
        {
          "name": "Pengxiang Cai",
          "affiliation": null
        },
        {
          "name": "Zihao Gao",
          "affiliation": null
        },
        {
          "name": "Jintai Chen",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17385v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17385v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17384v1",
      "title": "Closed-Loop Transfer for Weakly-supervised Affordance Grounding",
      "abstract": "Humans can perform previously unexperienced interactions with novel objects\nsimply by observing others engage with them. Weakly-supervised affordance\ngrounding mimics this process by learning to locate object regions that enable\nactions on egocentric images, using exocentric interaction images with\nimage-level annotations. However, extracting affordance knowledge solely from\nexocentric images and transferring it one-way to egocentric images limits the\napplicability of previous works in complex interaction scenarios. Instead, this\nstudy introduces LoopTrans, a novel closed-loop framework that not only\ntransfers knowledge from exocentric to egocentric but also transfers back to\nenhance exocentric knowledge extraction. Within LoopTrans, several innovative\nmechanisms are introduced, including unified cross-modal localization and\ndenoising knowledge distillation, to bridge domain gaps between object-centered\negocentric and interaction-centered exocentric images while enhancing knowledge\ntransfer. Experiments show that LoopTrans achieves consistent improvements\nacross all metrics on image and video benchmarks, even handling challenging\nscenarios where object interaction regions are fully occluded by the human\nbody.",
      "authors": [
        {
          "name": "Jiajin Tang",
          "affiliation": null
        },
        {
          "name": "Zhengxuan Wei",
          "affiliation": null
        },
        {
          "name": "Ge Zheng",
          "affiliation": null
        },
        {
          "name": "Sibei Yang",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17384v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17384v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17383v1",
      "title": "Latent Spaces Beyond Synthesis: From GANs to Diffusion Models",
      "abstract": "This paper examines the evolving nature of internal representations in\ngenerative visual models, focusing on the conceptual and technical shift from\nGANs and VAEs to diffusion-based architectures. Drawing on Beatrice Fazi's\naccount of synthesis as the amalgamation of distributed representations, we\npropose a distinction between \"synthesis in a strict sense\", where a compact\nlatent space wholly determines the generative process, and \"synthesis in a\nbroad sense,\" which characterizes models whose representational labor is\ndistributed across layers. Through close readings of model architectures and a\ntargeted experimental setup that intervenes in layerwise representations, we\nshow how diffusion models fragment the burden of representation and thereby\nchallenge assumptions of unified internal space. By situating these findings\nwithin media theoretical frameworks and critically engaging with metaphors such\nas the latent space and the Platonic Representation Hypothesis, we argue for a\nreorientation of how generative AI is understood: not as a direct synthesis of\ncontent, but as an emergent configuration of specialized processes.",
      "authors": [
        {
          "name": "Ludovica Schaerf",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cs.CV",
        "cs.CY"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17383v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17383v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17382v1",
      "title": "Graph Attention-Guided Search for Dense Multi-Agent Pathfinding",
      "abstract": "Finding near-optimal solutions for dense multi-agent pathfinding (MAPF)\nproblems in real-time remains challenging even for state-of-the-art planners.\nTo this end, we develop a hybrid framework that integrates a learned heuristic\nderived from MAGAT, a neural MAPF policy with a graph attention scheme, into a\nleading search-based algorithm, LaCAM. While prior work has explored\nlearning-guided search in MAPF, such methods have historically underperformed.\nIn contrast, our approach, termed LaGAT, outperforms both purely search-based\nand purely learning-based methods in dense scenarios. This is achieved through\nan enhanced MAGAT architecture, a pre-train-then-fine-tune strategy on maps of\ninterest, and a deadlock detection scheme to account for imperfect neural\nguidance. Our results demonstrate that, when carefully designed, hybrid search\noffers a powerful solution for tightly coupled, challenging multi-agent\ncoordination problems.",
      "authors": [
        {
          "name": "Rishabh Jain",
          "affiliation": null
        },
        {
          "name": "Keisuke Okumura",
          "affiliation": null
        },
        {
          "name": "Michael Amir",
          "affiliation": null
        },
        {
          "name": "Amanda Prorok",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.MA",
        "cs.RO"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17382v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17382v1",
      "primary_category": "cs.AI",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17381v1",
      "title": "Beyond Binary Out-of-Distribution Detection: Characterizing Distributional Shifts with Multi-Statistic Diffusion Trajectories",
      "abstract": "Detecting out-of-distribution (OOD) data is critical for machine learning, be\nit for safety reasons or to enable open-ended learning. However, beyond mere\ndetection, choosing an appropriate course of action typically hinges on the\ntype of OOD data encountered. Unfortunately, the latter is generally not\ndistinguished in practice, as modern OOD detection methods collapse\ndistributional shifts into single scalar outlier scores. This work argues that\nscalar-based methods are thus insufficient for OOD data to be properly\ncontextualized and prospectively exploited, a limitation we overcome with the\nintroduction of DISC: Diffusion-based Statistical Characterization. DISC\nleverages the iterative denoising process of diffusion models to extract a\nrich, multi-dimensional feature vector that captures statistical discrepancies\nacross multiple noise levels. Extensive experiments on image and tabular\nbenchmarks show that DISC matches or surpasses state-of-the-art detectors for\nOOD detection and, crucially, also classifies OOD type, a capability largely\nabsent from prior work. As such, our work enables a shift from simple binary\nOOD detection to a more granular detection.",
      "authors": [
        {
          "name": "Achref Jaziri",
          "affiliation": null
        },
        {
          "name": "Martin Rogmann",
          "affiliation": null
        },
        {
          "name": "Martin Mundt",
          "affiliation": null
        },
        {
          "name": "Visvanathan Ramesh",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17381v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17381v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17380v1",
      "title": "Optimizing Energy Management of Smart Grid using Reinforcement Learning aided by Surrogate models built using Physics-informed Neural Networks",
      "abstract": "Optimizing the energy management within a smart grids scenario presents\nsignificant challenges, primarily due to the complexity of real-world systems\nand the intricate interactions among various components. Reinforcement Learning\n(RL) is gaining prominence as a solution for addressing the challenges of\nOptimal Power Flow in smart grids. However, RL needs to iterate compulsively\nthroughout a given environment to obtain the optimal policy. This means\nobtaining samples from a, most likely, costly simulator, which can lead to a\nsample efficiency problem. In this work, we address this problem by\nsubstituting costly smart grid simulators with surrogate models built using\nPhisics-informed Neural Networks (PINNs), optimizing the RL policy training\nprocess by arriving to convergent results in a fraction of the time employed by\nthe original environment.",
      "authors": [
        {
          "name": "Julen Cestero",
          "affiliation": null
        },
        {
          "name": "Carmine Delle Femine",
          "affiliation": null
        },
        {
          "name": "Kenji S. Muro",
          "affiliation": null
        },
        {
          "name": "Marco Quartulli",
          "affiliation": null
        },
        {
          "name": "Marcello Restelli",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17380v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17380v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17378v1",
      "title": "Model Metamers Reveal Invariances in Graph Neural Networks",
      "abstract": "In recent years, deep neural networks have been extensively employed in\nperceptual systems to learn representations endowed with invariances, aiming to\nemulate the invariance mechanisms observed in the human brain. However, studies\nin the visual and auditory domains have confirmed that significant gaps remain\nbetween the invariance properties of artificial neural networks and those of\nhumans. To investigate the invariance behavior within graph neural networks\n(GNNs), we introduce a model ``metamers'' generation technique. By optimizing\ninput graphs such that their internal node activations match those of a\nreference graph, we obtain graphs that are equivalent in the model's\nrepresentation space, yet differ significantly in both structure and node\nfeatures. Our theoretical analysis focuses on two aspects: the local metamer\ndimension for a single node and the activation-induced volume change of the\nmetamer manifold. Utilizing this approach, we uncover extreme levels of\nrepresentational invariance across several classic GNN architectures. Although\ntargeted modifications to model architecture and training strategies can\npartially mitigate this excessive invariance, they fail to fundamentally bridge\nthe gap to human-like invariance. Finally, we quantify the deviation between\nmetamer graphs and their original counterparts, revealing unique failure modes\nof current GNNs and providing a complementary benchmark for model evaluation.",
      "authors": [
        {
          "name": "Wei Xu",
          "affiliation": null
        },
        {
          "name": "Xiaoyi Jiang",
          "affiliation": null
        },
        {
          "name": "Lixiang Xu",
          "affiliation": null
        },
        {
          "name": "Dechao Tang",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17378v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17378v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17373v1",
      "title": "Facial Expression-based Parkinson's Disease Severity Diagnosis via Feature Fusion and Adaptive Class Balancing",
      "abstract": "Parkinson's disease (PD) severity diagnosis is crucial for early detecting\npotential patients and adopting tailored interventions. Diagnosing PD based on\nfacial expression is grounded in PD patients' \"masked face\" symptom and gains\ngrowing interest recently for its convenience and affordability. However,\ncurrent facial expression-based approaches often rely on single type of\nexpression which can lead to misdiagnosis, and ignore the class imbalance\nacross different PD stages which degrades the prediction performance. Moreover,\nmost existing methods focus on binary classification (i.e., PD / non-PD) rather\nthan diagnosing the severity of PD. To address these issues, we propose a new\nfacial expression-based method for PD severity diagnosis which integrates\nmultiple facial expression features through attention-based feature fusion.\nMoreover, we mitigate the class imbalance problem via an adaptive class\nbalancing strategy which dynamically adjusts the contribution of training\nsamples based on their class distribution and classification difficulty.\nExperimental results demonstrate the promising performance of the proposed\nmethod for PD severity diagnosis, as well as the efficacy of attention-based\nfeature fusion and adaptive class balancing.",
      "authors": [
        {
          "name": "Yintao Zhou",
          "affiliation": null
        },
        {
          "name": "Wei Huang",
          "affiliation": null
        },
        {
          "name": "Zhengyu Li",
          "affiliation": null
        },
        {
          "name": "Jing Huang",
          "affiliation": null
        },
        {
          "name": "Meng Pang",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17373v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17373v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17372v1",
      "title": "Beyond Real Faces: Synthetic Datasets Can Achieve Reliable Recognition Performance without Privacy Compromise",
      "abstract": "The deployment of facial recognition systems has created an ethical dilemma:\nachieving high accuracy requires massive datasets of real faces collected\nwithout consent, leading to dataset retractions and potential legal liabilities\nunder regulations like GDPR. While synthetic facial data presents a promising\nprivacy-preserving alternative, the field lacks comprehensive empirical\nevidence of its viability. This study addresses this critical gap through\nextensive evaluation of synthetic facial recognition datasets. We present a\nsystematic literature review identifying 25 synthetic facial recognition\ndatasets (2018-2025), combined with rigorous experimental validation. Our\nmethodology examines seven key requirements for privacy-preserving synthetic\ndata: identity leakage prevention, intra-class variability, identity\nseparability, dataset scale, ethical data sourcing, bias mitigation, and\nbenchmark reliability. Through experiments involving over 10 million synthetic\nsamples, extended by a comparison of results reported on five standard\nbenchmarks, we provide the first comprehensive empirical assessment of\nsynthetic data's capability to replace real datasets. Best-performing synthetic\ndatasets (VariFace, VIGFace) achieve recognition accuracies of 95.67% and\n94.91% respectively, surpassing established real datasets including\nCASIA-WebFace (94.70%). While those images remain private, publicly available\nalternatives Vec2Face (93.52%) and CemiFace (93.22%) come close behind. Our\nfindings reveal that they ensure proper intra-class variability while\nmaintaining identity separability. Demographic bias analysis shows that, even\nthough synthetic data inherits limited biases, it offers unprecedented control\nfor bias mitigation through generation parameters. These results establish\nsynthetic facial data as a scientifically viable and ethically imperative\nalternative for facial recognition research.",
      "authors": [
        {
          "name": "Pawe\u0142 Borsukiewicz",
          "affiliation": null
        },
        {
          "name": "Fadi Boutros",
          "affiliation": null
        },
        {
          "name": "Iyiola E. Olatunji",
          "affiliation": null
        },
        {
          "name": "Charles Beumier",
          "affiliation": null
        },
        {
          "name": "Wendk\u00fbuni C. Ouedraogo",
          "affiliation": null
        },
        {
          "name": "Jacques Klein",
          "affiliation": null
        },
        {
          "name": "Tegawend\u00e9 F. Bissyand\u00e9",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17372v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17372v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17369v1",
      "title": "Bridging Embodiment Gaps: Deploying Vision-Language-Action Models on Soft Robots",
      "abstract": "Robotic systems are increasingly expected to operate in human-centered,\nunstructured environments where safety, adaptability, and generalization are\nessential. Vision-Language-Action (VLA) models have been proposed as a language\nguided generalized control framework for real robots. However, their deployment\nhas been limited to conventional serial link manipulators. Coupled by their\nrigidity and unpredictability of learning based control, the ability to safely\ninteract with the environment is missing yet critical. In this work, we present\nthe deployment of a VLA model on a soft continuum manipulator to demonstrate\nautonomous safe human-robot interaction. We present a structured finetuning and\ndeployment pipeline evaluating two state-of-the-art VLA models (OpenVLA-OFT and\n$\\pi_0$) across representative manipulation tasks, and show while\nout-of-the-box policies fail due to embodiment mismatch, through targeted\nfinetuning the soft robot performs equally to the rigid counterpart. Our\nfindings highlight the necessity of finetuning for bridging embodiment gaps,\nand demonstrate that coupling VLA models with soft robots enables safe and\nflexible embodied AI in human-shared environments.",
      "authors": [
        {
          "name": "Haochen Su",
          "affiliation": null
        },
        {
          "name": "Cristian Meo",
          "affiliation": null
        },
        {
          "name": "Francesco Stella",
          "affiliation": null
        },
        {
          "name": "Andrea Peirone",
          "affiliation": null
        },
        {
          "name": "Kai Junge",
          "affiliation": null
        },
        {
          "name": "Josie Hughes",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17369v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17369v1",
      "primary_category": "cs.RO",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17364v1",
      "title": "Recurrent Attention-based Token Selection for Efficient Streaming Video-LLMs",
      "abstract": "Video Large Language Models (Video-LLMs) excel at understanding videos\nin-context, provided they have full access to the video when answering queries.\nHowever, these models face challenges in streaming scenarios where hour-long\nvideos must be processed online, and questions need timely responses. In this\nwork, we propose a training-free approach compatible with standard Video-LLMs,\nleveraging three key concepts: 1) LLM-informed selection of visual tokens to\nidentify those that the LLM has attended to and contributed to its\nunderstanding of each short clip. Our attention-based selection allows us to\ndiscard up to ~95% of unimportant visual tokens with minimal performance loss;\n2) Recurrent processing of past selected tokens to generate temporally coherent\nunderstanding of each processed clip; 3) Caption-based question answering for\nlightweight and accurate responses. Our method achieves state-of-the-art\nperformance on streaming video benchmarks, striking a balance between\nefficiency and effectiveness.",
      "authors": [
        {
          "name": "Vaggelis Dorovatas",
          "affiliation": null
        },
        {
          "name": "Soroush Seifi",
          "affiliation": null
        },
        {
          "name": "Gunshi Gupta",
          "affiliation": null
        },
        {
          "name": "Rahaf Aljundi",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17364v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17364v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17363v1",
      "title": "M2H: Multi-Task Learning with Efficient Window-Based Cross-Task Attention for Monocular Spatial Perception",
      "abstract": "Deploying real-time spatial perception on edge devices requires efficient\nmulti-task models that leverage complementary task information while minimizing\ncomputational overhead. This paper introduces Multi-Mono-Hydra (M2H), a novel\nmulti-task learning framework designed for semantic segmentation and depth,\nedge, and surface normal estimation from a single monocular image. Unlike\nconventional approaches that rely on independent single-task models or shared\nencoder-decoder architectures, M2H introduces a Window-Based Cross-Task\nAttention Module that enables structured feature exchange while preserving\ntask-specific details, improving prediction consistency across tasks. Built on\na lightweight ViT-based DINOv2 backbone, M2H is optimized for real-time\ndeployment and serves as the foundation for monocular spatial perception\nsystems supporting 3D scene graph construction in dynamic environments.\nComprehensive evaluations show that M2H outperforms state-of-the-art multi-task\nmodels on NYUDv2, surpasses single-task depth and semantic baselines on\nHypersim, and achieves superior performance on the Cityscapes dataset, all\nwhile maintaining computational efficiency on laptop hardware. Beyond\nbenchmarks, M2H is validated on real-world data, demonstrating its practicality\nin spatial perception tasks.",
      "authors": [
        {
          "name": "U. V. B. L Udugama",
          "affiliation": null
        },
        {
          "name": "George Vosselman",
          "affiliation": null
        },
        {
          "name": "Francesco Nex",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "cs.LG",
        "cs.RO"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17363v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17363v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17358v1",
      "title": "Localist LLMs with Recruitment Learning",
      "abstract": "We present a novel framework for training large language models with\ncontinuously adjustable internal representations that span the full spectrum\nfrom localist (interpretable, rule-based) to distributed (generalizable,\nefficient) encodings. The key innovations are (1) a locality dial, a tunable\nparameter that dynamically controls the degree of localization during both\ntraining and inference without requiring model retraining, (2) an\ninformation-theoretic recruitment mechanism that adaptively allocates semantic\nblocks as needed, eliminating the requirement for complete domain knowledge at\ninitialization, and (3) a hierarchical recruitment framework that extends\ncapacity allocation to entire specialized LLMs, enabling multi-granularity\narchitectural adaptation. This is achieved through group sparsity penalties on\nattention mechanisms, information-theoretic anchor design, dynamic rule\ninjection, and principled recruitment criteria based on penalized likelihood\nwith explicit units. We provide rigorous mathematical results establishing\nexplicit threshold conditions under which attention provably concentrates on\nsemantically relevant blocks at stationary points, with exact bounds on\nattention entropy and pointer fidelity. The hierarchical recruitment mechanism\nprovides convergence guarantees at both the block level (fine-grained,\nwithin-LLM) and the LLM level (coarse-grained, cross-domain), ensuring the\nsystem discovers semantic partitions that balance model complexity against data\nencoding efficiency. This framework enables practitioners to continuously\ninterpolate between interpretable and high-performance modes while adapting\narchitectural capacity at multiple granularities, supporting applications in\nregulated domains requiring both transparency and capability.",
      "authors": [
        {
          "name": "Joachim Diederich",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17358v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17358v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17354v1",
      "title": "Towards Mixed-Modal Retrieval for Universal Retrieval-Augmented Generation",
      "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for\nenhancing large language models (LLMs) by retrieving relevant documents from an\nexternal corpus. However, existing RAG systems primarily focus on unimodal text\ndocuments, and often fall short in real-world scenarios where both queries and\ndocuments may contain mixed modalities (such as text and images). In this\npaper, we address the challenge of Universal Retrieval-Augmented Generation\n(URAG), which involves retrieving and reasoning over mixed-modal information to\nimprove vision-language generation. To this end, we propose Nyx, a unified\nmixed-modal to mixed-modal retriever tailored for URAG scenarios. To mitigate\nthe scarcity of realistic mixed-modal data, we introduce a four-stage automated\npipeline for generation and filtering, leveraging web documents to construct\nNyxQA, a dataset comprising diverse mixed-modal question-answer pairs that\nbetter reflect real-world information needs. Building on this high-quality\ndataset, we adopt a two-stage training framework for Nyx: we first perform\npre-training on NyxQA along with a variety of open-source retrieval datasets,\nfollowed by supervised fine-tuning using feedback from downstream\nvision-language models (VLMs) to align retrieval outputs with generative\npreferences. Experimental results demonstrate that Nyx not only performs\ncompetitively on standard text-only RAG benchmarks, but also excels in the more\ngeneral and realistic URAG setting, significantly improving generation quality\nin vision-language tasks.",
      "authors": [
        {
          "name": "Chenghao Zhang",
          "affiliation": null
        },
        {
          "name": "Guanting Dong",
          "affiliation": null
        },
        {
          "name": "Xinyu Yang",
          "affiliation": null
        },
        {
          "name": "Zhicheng Dou",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17354v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17354v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17348v1",
      "title": "Optimal Best Arm Identification under Differential Privacy",
      "abstract": "Best Arm Identification (BAI) algorithms are deployed in data-sensitive\napplications, such as adaptive clinical trials or user studies. Driven by the\nprivacy concerns of these applications, we study the problem of\nfixed-confidence BAI under global Differential Privacy (DP) for Bernoulli\ndistributions. While numerous asymptotically optimal BAI algorithms exist in\nthe non-private setting, a significant gap remains between the best lower and\nupper bounds in the global DP setting. This work reduces this gap to a small\nmultiplicative constant, for any privacy budget $\\epsilon$. First, we provide a\ntighter lower bound on the expected sample complexity of any $\\delta$-correct\nand $\\epsilon$-global DP strategy. Our lower bound replaces the\nKullback-Leibler (KL) divergence in the transportation cost used by the\nnon-private characteristic time with a new information-theoretic quantity that\noptimally trades off between the KL divergence and the Total Variation distance\nscaled by $\\epsilon$. Second, we introduce a stopping rule based on these\ntransportation costs and a private estimator of the means computed using an\narm-dependent geometric batching. En route to proving the correctness of our\nstopping rule, we derive concentration results of independent interest for the\nLaplace distribution and for the sum of Bernoulli and Laplace distributions.\nThird, we propose a Top Two sampling rule based on these transportation costs.\nFor any budget $\\epsilon$, we show an asymptotic upper bound on its expected\nsample complexity that matches our lower bound to a multiplicative constant\nsmaller than $8$. Our algorithm outperforms existing $\\delta$-correct and\n$\\epsilon$-global DP BAI algorithms for different values of $\\epsilon$.",
      "authors": [
        {
          "name": "Marc Jourdan",
          "affiliation": null
        },
        {
          "name": "Achraf Azize",
          "affiliation": null
        }
      ],
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17348v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17348v1",
      "primary_category": "stat.ML",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17347v1",
      "title": "Exploring The Missing Semantics In Event Modality",
      "abstract": "Event cameras offer distinct advantages such as low latency, high dynamic\nrange, and efficient motion capture. However, event-to-video reconstruction\n(E2V), a fundamental event-based vision task, remains challenging, particularly\nfor reconstructing and recovering semantic information. This is primarily due\nto the nature of the event camera, as it only captures intensity changes,\nignoring static objects and backgrounds, resulting in a lack of semantic\ninformation in captured event modality. Further, semantic information plays a\ncrucial role in video and frame reconstruction, yet is often overlooked by\nexisting E2V approaches. To bridge this gap, we propose Semantic-E2VID, an E2V\nframework that explores the missing visual semantic knowledge in event modality\nand leverages it to enhance event-to-video reconstruction. Specifically,\nSemantic-E2VID introduces a cross-modal feature alignment (CFA) module to\ntransfer the robust visual semantics from a frame-based vision foundation\nmodel, the Segment Anything Model (SAM), to the event encoder, while aligning\nthe high-level features from distinct modalities. To better utilize the learned\nsemantic feature, we further propose a semantic-aware feature fusion (SFF)\nblock to integrate learned semantics in frame modality to form event\nrepresentations with rich semantics that can be decoded by the event decoder.\nFurther, to facilitate the reconstruction of semantic information, we propose a\nnovel Semantic Perceptual E2V Supervision that helps the model to reconstruct\nsemantic details by leveraging SAM-generated categorical labels. Extensive\nexperiments demonstrate that Semantic-E2VID significantly enhances frame\nquality, outperforming state-of-the-art E2V methods across multiple benchmarks.\nThe sample code is included in the supplementary material.",
      "authors": [
        {
          "name": "Jingqian Wu",
          "affiliation": null
        },
        {
          "name": "Shengpeng Xu",
          "affiliation": null
        },
        {
          "name": "Yunbo Jia",
          "affiliation": null
        },
        {
          "name": "Edmund Y. Lam",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17347v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17347v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17346v1",
      "title": "TopSeg: A Multi-Scale Topological Framework for Data-Efficient Heart Sound Segmentation",
      "abstract": "Deep learning approaches for heart-sound (PCG) segmentation built on\ntime--frequency features can be accurate but often rely on large expert-labeled\ndatasets, limiting robustness and deployment. We present TopSeg, a topological\nrepresentation-centric framework that encodes PCG dynamics with multi-scale\ntopological features and decodes them using a lightweight temporal\nconvolutional network (TCN) with an order- and duration-constrained inference\nstep. To evaluate data efficiency and generalization, we train exclusively on\nPhysioNet 2016 dataset with subject-level subsampling and perform external\nvalidation on CirCor dataset. Under matched-capacity decoders, the topological\nfeatures consistently outperform spectrogram and envelope inputs, with the\nlargest margins at low data budgets; as a full system, TopSeg surpasses\nrepresentative end-to-end baselines trained on their native inputs under the\nsame budgets while remaining competitive at full data. Ablations at 10%\ntraining confirm that all scales contribute and that combining H_0 and H_1\nyields more reliable S1/S2 localization and boundary stability. These results\nindicate that topology-aware representations provide a strong inductive bias\nfor data-efficient, cross-dataset PCG segmentation, supporting practical use\nwhen labeled data are limited.",
      "authors": [
        {
          "name": "Peihong Zhang",
          "affiliation": null
        },
        {
          "name": "Zhixin Li",
          "affiliation": null
        },
        {
          "name": "Yuxuan Liu",
          "affiliation": null
        },
        {
          "name": "Rui Sang",
          "affiliation": null
        },
        {
          "name": "Yiqiang Cai",
          "affiliation": null
        },
        {
          "name": "Yizhou Tan",
          "affiliation": null
        },
        {
          "name": "Shengchen Li",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17346v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17346v1",
      "primary_category": "cs.SD",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17345v1",
      "title": "DDSC: Dynamic Dual-Signal Curriculum for Data-Efficient Acoustic Scene Classification under Domain Shift",
      "abstract": "Acoustic scene classification (ASC) suffers from device-induced domain shift,\nespecially when labels are limited. Prior work focuses on curriculum-based\ntraining schedules that structure data presentation by ordering or reweighting\ntraining examples from easy-to-hard to facilitate learning; however, existing\ncurricula are static, fixing the ordering or the weights before training and\nignoring that example difficulty and marginal utility evolve with the learned\nrepresentation. To overcome this limitation, we propose the Dynamic Dual-Signal\nCurriculum (DDSC), a training schedule that adapts the curriculum online by\ncombining two signals computed each epoch: a domain-invariance signal and a\nlearning-progress signal. A time-varying scheduler fuses these signals into\nper-example weights that prioritize domain-invariant examples in early epochs\nand progressively emphasize device-specific cases. DDSC is lightweight,\narchitecture-agnostic, and introduces no additional inference overhead. Under\nthe official DCASE 2024 Task~1 protocol, DDSC consistently improves\ncross-device performance across diverse ASC baselines and label budgets, with\nthe largest gains on unseen-device splits.",
      "authors": [
        {
          "name": "Peihong Zhang",
          "affiliation": null
        },
        {
          "name": "Yuxuan Liu",
          "affiliation": null
        },
        {
          "name": "Rui Sang",
          "affiliation": null
        },
        {
          "name": "Zhixin Li",
          "affiliation": null
        },
        {
          "name": "Yiqiang Cai",
          "affiliation": null
        },
        {
          "name": "Yizhou Tan",
          "affiliation": null
        },
        {
          "name": "Shengchen Li",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17345v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17345v1",
      "primary_category": "cs.SD",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17338v1",
      "title": "Nearest-Class Mean and Logits Agreement for Wildlife Open-Set Recognition",
      "abstract": "Current state-of-the-art Wildlife classification models are trained under the\nclosed world setting. When exposed to unknown classes, they remain\noverconfident in their predictions. Open-set Recognition (OSR) aims to classify\nknown classes while rejecting unknown samples. Several OSR methods have been\nproposed to model the closed-set distribution by observing the feature, logit,\nor softmax probability space. A significant drawback of many existing\napproaches is the requirement to retrain the pre-trained classification model\nwith the OSR-specific strategy. This study contributes a post-processing OSR\nmethod that measures the agreement between the models' features and predicted\nlogits. We propose a probability distribution based on an input's distance to\nits Nearest Class Mean (NCM). The NCM-based distribution is then compared with\nthe softmax probabilities from the logit space to measure agreement between the\nNCM and the classification head. Our proposed strategy ranks within the top\nthree on two evaluated datasets, showing consistent performance across the two\ndatasets. In contrast, current state-of-the-art methods excel on a single\ndataset. We achieve an AUROC of 93.41 and 95.35 for African and Swedish\nanimals. The code can be found\nhttps://github.com/Applied-Representation-Learning-Lab/OSR.",
      "authors": [
        {
          "name": "Jiahao Huo",
          "affiliation": null
        },
        {
          "name": "Mufhumudzi Muthivhi",
          "affiliation": null
        },
        {
          "name": "Terence L. van Zyl",
          "affiliation": null
        },
        {
          "name": "Fredrik Gustafsson",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17338v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17338v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17332v1",
      "title": "iDETEX: Empowering MLLMs for Intelligent DETailed EXplainable IQA",
      "abstract": "Image Quality Assessment (IQA) has progressed from scalar quality prediction\nto more interpretable, human-aligned evaluation paradigms. In this work, we\naddress the emerging challenge of detailed and explainable IQA by proposing\niDETEX-a unified multimodal large language model (MLLM) capable of\nsimultaneously performing three key tasks: quality grounding, perception, and\ndescription. To facilitate efficient and generalizable training across these\nheterogeneous subtasks, we design a suite of task-specific offline augmentation\nmodules and a data mixing strategy. These are further complemented by online\nenhancement strategies to fully exploit multi-sourced supervision. We validate\nour approach on the large-scale ViDA-UGC benchmark, where iDETEX achieves\nstate-of-the-art performance across all subtasks. Our model ranks first in the\nICCV MIPI 2025 Detailed Image Quality Assessment Challenge, demonstrating its\neffectiveness and robustness in delivering accurate and interpretable quality\nassessments.",
      "authors": [
        {
          "name": "Zhaoran Zhao",
          "affiliation": null
        },
        {
          "name": "Xinli Yue",
          "affiliation": null
        },
        {
          "name": "Jianhui Sun",
          "affiliation": null
        },
        {
          "name": "Yuhao Xie",
          "affiliation": null
        },
        {
          "name": "Tao Shao",
          "affiliation": null
        },
        {
          "name": "Liangchao Yao",
          "affiliation": null
        },
        {
          "name": "Fan Xia",
          "affiliation": null
        },
        {
          "name": "Yuetang Deng",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17332v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17332v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17330v1",
      "title": "CharDiff: A Diffusion Model with Character-Level Guidance for License Plate Image Restoration",
      "abstract": "The significance of license plate image restoration goes beyond the\npreprocessing stage of License Plate Recognition (LPR) systems, as it also\nserves various purposes, including increasing evidential value, enhancing the\nclarity of visual interface, and facilitating further utilization of license\nplate images. We propose a novel diffusion-based framework with character-level\nguidance, CharDiff, which effectively restores and recognizes severely degraded\nlicense plate images captured under realistic conditions. CharDiff leverages\nfine-grained character-level priors extracted through external segmentation and\nOptical Character Recognition (OCR) modules tailored for low-quality license\nplate images. For precise and focused guidance, CharDiff incorporates a novel\nCharacter-guided Attention through Region-wise Masking (CHARM) module, which\nensures that each character's guidance is restricted to its own region, thereby\navoiding interference with other regions. In experiments, CharDiff\nsignificantly outperformed the baseline restoration models in both restoration\nquality and recognition accuracy, achieving a 28% relative reduction in CER on\nthe Roboflow-LP dataset, compared to the best-performing baseline model. These\nresults indicate that the structured character-guided conditioning effectively\nenhances the robustness of diffusion-based license plate restoration and\nrecognition in practical deployment scenarios.",
      "authors": [
        {
          "name": "Gyuhwan Park",
          "affiliation": null
        },
        {
          "name": "Kihyun Na",
          "affiliation": null
        },
        {
          "name": "Injung Kim",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17330v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17330v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17322v1",
      "title": "A Single Set of Adversarial Clothes Breaks Multiple Defense Methods in the Physical World",
      "abstract": "In recent years, adversarial attacks against deep learning-based object\ndetectors in the physical world have attracted much attention. To defend\nagainst these attacks, researchers have proposed various defense methods\nagainst adversarial patches, a typical form of physically-realizable attack.\nHowever, our experiments showed that simply enlarging the patch size could make\nthese defense methods fail. Motivated by this, we evaluated various defense\nmethods against adversarial clothes which have large coverage over the human\nbody. Adversarial clothes provide a good test case for adversarial defense\nagainst patch-based attacks because they not only have large sizes but also\nlook more natural than a large patch on humans. Experiments show that all the\ndefense methods had poor performance against adversarial clothes in both the\ndigital world and the physical world. In addition, we crafted a single set of\nclothes that broke multiple defense methods on Faster R-CNN. The set achieved\nan Attack Success Rate (ASR) of 96.06% against the undefended detector and over\n64.84% ASRs against nine defended models in the physical world, unveiling the\ncommon vulnerability of existing adversarial defense methods against\nadversarial clothes. Code is available at:\nhttps://github.com/weiz0823/adv-clothes-break-multiple-defenses.",
      "authors": [
        {
          "name": "Wei Zhang",
          "affiliation": null
        },
        {
          "name": "Zhanhao Hu",
          "affiliation": null
        },
        {
          "name": "Xiao Li",
          "affiliation": null
        },
        {
          "name": "Xiaopei Zhu",
          "affiliation": null
        },
        {
          "name": "Xiaolin Hu",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17322v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17322v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17318v1",
      "title": "CausalMamba: Scalable Conditional State Space Models for Neural Causal Inference",
      "abstract": "We introduce CausalMamba, a scalable framework that addresses fundamental\nlimitations in fMRI-based causal inference: the ill-posed nature of inferring\nneural causality from hemodynamically distorted BOLD signals and the\ncomputational intractability of existing methods like Dynamic Causal Modeling\n(DCM). Our approach decomposes this complex inverse problem into two tractable\nstages: BOLD deconvolution to recover latent neural activity, followed by\ncausal graph inference using a novel Conditional Mamba architecture. On\nsimulated data, CausalMamba achieves 37% higher accuracy than DCM. Critically,\nwhen applied to real task fMRI data, our method recovers well-established\nneural pathways with 88% fidelity, whereas conventional approaches fail to\nidentify these canonical circuits in over 99% of subjects. Furthermore, our\nnetwork analysis of working memory data reveals that the brain strategically\nshifts its primary causal hub-recruiting executive or salience networks\ndepending on the stimulus-a sophisticated reconfiguration that remains\nundetected by traditional methods. This work provides neuroscientists with a\npractical tool for large-scale causal inference that captures both fundamental\ncircuit motifs and flexible network dynamics underlying cognitive function.",
      "authors": [
        {
          "name": "Sangyoon Bae",
          "affiliation": null
        },
        {
          "name": "Jiook Cha",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17318v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17318v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17314v1",
      "title": "Auto-Rubric: Learning to Extract Generalizable Criteria for Reward Modeling",
      "abstract": "Reward models are essential for aligning Large Language Models (LLMs) with\nhuman values, yet their development is hampered by costly preference datasets\nand poor interpretability. While recent rubric-based approaches offer\ntransparency, they often lack systematic quality control and optimization,\ncreating a trade-off between scalability and reliability. We address these\nlimitations with a novel, training-free framework built on a key assumption:\n\\textit{evaluation rubrics underlying human preferences exhibit significant\ngeneralization ability across diverse queries}, a property that enables\nremarkable data efficiency. Our two-stage approach first infers high-quality,\nquery-specific rubrics using a validation-guided\n\\textbf{Propose-Evaluate-Revise} pipeline. Second, it generalizes these\ngranular rubrics into a compact, non-redundant core set by maximizing an\n\\textbf{information-theoretic coding rate}. The final output is an\ninterpretable, hierarchical \"Theme-Tips\" rubric set. Extensive experiments\ndemonstrate the framework's exceptional data efficiency and performance.\nCritically, using just 70 preference pairs (1.5\\% of the source data), our\nmethod also empowers smaller models like Qwen3-8B to outperform specialized,\nfully-trained counterparts. This work pioneers a scalable, interpretable, and\ndata-efficient path for reward modeling.",
      "authors": [
        {
          "name": "Lipeng Xie",
          "affiliation": null
        },
        {
          "name": "Sen Huang",
          "affiliation": null
        },
        {
          "name": "Zhuo Zhang",
          "affiliation": null
        },
        {
          "name": "Anni Zou",
          "affiliation": null
        },
        {
          "name": "Yunpeng Zhai",
          "affiliation": null
        },
        {
          "name": "Dingchao Ren",
          "affiliation": null
        },
        {
          "name": "Kezun Zhang",
          "affiliation": null
        },
        {
          "name": "Haoyuan Hu",
          "affiliation": null
        },
        {
          "name": "Boyin Liu",
          "affiliation": null
        },
        {
          "name": "Haoran Chen",
          "affiliation": null
        },
        {
          "name": "Zhaoyang Liu",
          "affiliation": null
        },
        {
          "name": "Bolin Ding",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17314v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17314v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17313v1",
      "title": "Disentanglement Beyond Static vs. Dynamic: A Benchmark and Evaluation Framework for Multi-Factor Sequential Representations",
      "abstract": "Learning disentangled representations in sequential data is a key goal in\ndeep learning, with broad applications in vision, audio, and time series. While\nreal-world data involves multiple interacting semantic factors over time, prior\nwork has mostly focused on simpler two-factor static and dynamic settings,\nprimarily because such settings make data collection easier, thereby\noverlooking the inherently multi-factor nature of real-world data. We introduce\nthe first standardized benchmark for evaluating multi-factor sequential\ndisentanglement across six diverse datasets spanning video, audio, and time\nseries. Our benchmark includes modular tools for dataset integration, model\ndevelopment, and evaluation metrics tailored to multi-factor analysis. We\nadditionally propose a post-hoc Latent Exploration Stage to automatically align\nlatent dimensions with semantic factors, and introduce a Koopman-inspired model\nthat achieves state-of-the-art results. Moreover, we show that Vision-Language\nModels can automate dataset annotation and serve as zero-shot disentanglement\nevaluators, removing the need for manual labels and human intervention.\nTogether, these contributions provide a robust and scalable foundation for\nadvancing multi-factor sequential disentanglement.",
      "authors": [
        {
          "name": "Tal Barami",
          "affiliation": null
        },
        {
          "name": "Nimrod Berman",
          "affiliation": null
        },
        {
          "name": "Ilan Naiman",
          "affiliation": null
        },
        {
          "name": "Amos H. Hason",
          "affiliation": null
        },
        {
          "name": "Rotem Ezra",
          "affiliation": null
        },
        {
          "name": "Omri Azencot",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17313v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17313v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17309v1",
      "title": "RubiSCoT: A Framework for AI-Supported Academic Assessment",
      "abstract": "The evaluation of academic theses is a cornerstone of higher education,\nensuring rigor and integrity. Traditional methods, though effective, are\ntime-consuming and subject to evaluator variability. This paper presents\nRubiSCoT, an AI-supported framework designed to enhance thesis evaluation from\nproposal to final submission. Using advanced natural language processing\ntechniques, including large language models, retrieval-augmented generation,\nand structured chain-of-thought prompting, RubiSCoT offers a consistent,\nscalable solution. The framework includes preliminary assessments,\nmultidimensional assessments, content extraction, rubric-based scoring, and\ndetailed reporting. We present the design and implementation of RubiSCoT,\ndiscussing its potential to optimize academic assessment processes through\nconsistent, scalable, and transparent evaluation.",
      "authors": [
        {
          "name": "Thorsten Fr\u00f6hlich",
          "affiliation": null
        },
        {
          "name": "Tim Schlippe",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.AI"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17309v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17309v1",
      "primary_category": "cs.AI",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17305v1",
      "title": "LongInsightBench: A Comprehensive Benchmark for Evaluating Omni-Modal Models on Human-Centric Long-Video Understanding",
      "abstract": "We introduce \\textbf{LongInsightBench}, the first benchmark designed to\nassess models' ability to understand long videos, with a focus on human\nlanguage, viewpoints, actions, and other contextual elements, while integrating\n\\textbf{visual, audio, and text} modalities. Our benchmark excels in three key\nareas: \\textbf{a) Long-Duration, Information-Dense Videos:} We carefully select\napproximately 1,000 videos from open-source datasets FineVideo based on\nduration limit and the information density of both visual and audio modalities,\nfocusing on content like lectures, interviews, and vlogs, which contain rich\nlanguage elements. \\textbf{b) Diverse and Challenging Task Scenarios:} We have\ndesigned six challenging task scenarios, including both Intra-Event and\nInter-Event Tasks. \\textbf{c) Rigorous and Comprehensive Quality Assurance\nPipelines:} We have developed a three-step, semi-automated data quality\nassurance pipeline to ensure the difficulty and validity of the synthesized\nquestions and answer options. Based on LongInsightBench, we designed a series\nof experiments. Experimental results shows that Omni-modal models(OLMs) still\nface challenge in tasks requiring precise temporal localization (T-Loc) and\nlong-range causal inference (CE-Caus). Extended experiments reveal the\ninformation loss and processing bias in multi-modal fusion of OLMs. Our dataset\nand code is available at\nhttps://anonymous.4open.science/r/LongInsightBench-910F/.",
      "authors": [
        {
          "name": "ZhaoYang Han",
          "affiliation": null
        },
        {
          "name": "Qihan Lin",
          "affiliation": null
        },
        {
          "name": "Hao Liang",
          "affiliation": null
        },
        {
          "name": "Bowen Chen",
          "affiliation": null
        },
        {
          "name": "Zhou Liu",
          "affiliation": null
        },
        {
          "name": "Wentao Zhang",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "cs.MM"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17305v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17305v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17303v1",
      "title": "Symmetries in PAC-Bayesian Learning",
      "abstract": "Symmetries are known to improve the empirical performance of machine learning\nmodels, yet theoretical guarantees explaining these gains remain limited. Prior\nwork has focused mainly on compact group symmetries and often assumes that the\ndata distribution itself is invariant, an assumption rarely satisfied in\nreal-world applications. In this work, we extend generalization guarantees to\nthe broader setting of non-compact symmetries, such as translations and to\nnon-invariant data distributions. Building on the PAC-Bayes framework, we adapt\nand tighten existing bounds, demonstrating the approach on McAllester's\nPAC-Bayes bound while showing that it applies to a wide range of PAC-Bayes\nbounds. We validate our theory with experiments on a rotated MNIST dataset with\na non-uniform rotation group, where the derived guarantees not only hold but\nalso improve upon prior results. These findings provide theoretical evidence\nthat, for symmetric data, symmetric models are preferable beyond the narrow\nsetting of compact groups and invariant distributions, opening the way to a\nmore general understanding of symmetries in machine learning.",
      "authors": [
        {
          "name": "Armin Beck",
          "affiliation": null
        },
        {
          "name": "Peter Ochs",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17303v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17303v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17301v1",
      "title": "Comprehending Spatio-temporal Data via Cinematic Storytelling using Large Language Models",
      "abstract": "Spatio-temporal data captures complex dynamics across both space and time,\nyet traditional visualizations are complex, require domain expertise and often\nfail to resonate with broader audiences. Here, we propose MapMuse, a\nstorytelling-based framework for interpreting spatio-temporal datasets,\ntransforming them into compelling, narrative-driven experiences. We utilize\nlarge language models and employ retrieval augmented generation (RAG) and\nagent-based techniques to generate comprehensive stories. Drawing on principles\ncommon in cinematic storytelling, we emphasize clarity, emotional connection,\nand audience-centric design. As a case study, we analyze a dataset of taxi\ntrajectories. Two perspectives are presented: a captivating story based on a\nheat map that visualizes millions of taxi trip endpoints to uncover urban\nmobility patterns; and a detailed narrative following a single long taxi\njourney, enriched with city landmarks and temporal shifts. By portraying\nlocations as characters and movement as plot, we argue that data storytelling\ndrives insight, engagement, and action from spatio-temporal information. The\ncase study illustrates how MapMuse can bridge the gap between data complexity\nand human understanding. The aim of this short paper is to provide a glimpse to\nthe potential of the cinematic storytelling technique as an effective\ncommunication tool for spatio-temporal data, as well as to describe open\nproblems and opportunities for future research.",
      "authors": [
        {
          "name": "Panos Kalnis. Shuo Shang",
          "affiliation": null
        },
        {
          "name": "Christian S. Jensen",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.DB",
        "cs.AI"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17301v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17301v1",
      "primary_category": "cs.DB",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17299v1",
      "title": "Exploring Structural Degradation in Dense Representations for Self-supervised Learning",
      "abstract": "In this work, we observe a counterintuitive phenomenon in self-supervised\nlearning (SSL): longer training may impair the performance of dense prediction\ntasks (e.g., semantic segmentation). We refer to this phenomenon as\nSelf-supervised Dense Degradation (SDD) and demonstrate its consistent presence\nacross sixteen state-of-the-art SSL methods with various losses, architectures,\nand datasets. When the model performs suboptimally on dense tasks at the end of\ntraining, measuring the performance during training becomes essential. However,\nevaluating dense performance effectively without annotations remains an open\nchallenge. To tackle this issue, we introduce a Dense representation Structure\nEstimator (DSE), composed of a class-relevance measure and an effective\ndimensionality measure. The proposed DSE is both theoretically grounded and\nempirically validated to be closely correlated with the downstream performance.\nBased on this metric, we introduce a straightforward yet effective model\nselection strategy and a DSE-based regularization method. Experiments on\nsixteen SSL methods across four benchmarks confirm that model selection\nimproves mIoU by $3.0\\%$ on average with negligible computational cost.\nAdditionally, DSE regularization consistently mitigates the effects of dense\ndegradation. Code is available at\nhttps://github.com/EldercatSAM/SSL-Degradation.",
      "authors": [
        {
          "name": "Siran Dai",
          "affiliation": null
        },
        {
          "name": "Qianqian Xu",
          "affiliation": null
        },
        {
          "name": "Peisong Wen",
          "affiliation": null
        },
        {
          "name": "Yang Liu",
          "affiliation": null
        },
        {
          "name": "Qingming Huang",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17299v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17299v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17289v1",
      "title": "Addressing Antisocial Behavior in Multi-Party Dialogs Through Multimodal Representation Learning",
      "abstract": "Antisocial behavior (ASB) on social media -- including hate speech,\nharassment, and cyberbullying -- poses growing risks to platform safety and\nsocietal well-being. Prior research has focused largely on networks such as X\nand Reddit, while \\textit{multi-party conversational settings} remain\nunderexplored due to limited data. To address this gap, we use\n\\textit{CyberAgressionAdo-Large}, a French open-access dataset simulating ASB\nin multi-party conversations, and evaluate three tasks: \\textit{abuse\ndetection}, \\textit{bullying behavior analysis}, and \\textit{bullying\npeer-group identification}. We benchmark six text-based and eight graph-based\n\\textit{representation-learning methods}, analyzing lexical cues, interactional\ndynamics, and their multimodal fusion. Results show that multimodal models\noutperform unimodal baselines. The late fusion model \\texttt{mBERT + WD-SGCN}\nachieves the best overall results, with top performance on abuse detection\n(0.718) and competitive scores on peer-group identification (0.286) and\nbullying analysis (0.606). Error analysis highlights its effectiveness in\nhandling nuanced ASB phenomena such as implicit aggression, role transitions,\nand context-dependent hostility.",
      "authors": [
        {
          "name": "Hajar Bakarou",
          "affiliation": null
        },
        {
          "name": "Mohamed Sinane El Messoussi",
          "affiliation": null
        },
        {
          "name": "Ana\u00efs Ollagnier",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17289v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17289v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17287v1",
      "title": "Machine Vision-Based Surgical Lighting System:Design and Implementation",
      "abstract": "Effortless and ergonomically designed surgical lighting is critical for\nprecision and safety during procedures. However, traditional systems often rely\non manual adjustments, leading to surgeon fatigue, neck strain, and\ninconsistent illumination due to drift and shadowing. To address these\nchallenges, we propose a novel surgical lighting system that leverages the\nYOLOv11 object detection algorithm to identify a blue marker placed above the\ntarget surgical site. A high-power LED light source is then directed to the\nidentified location using two servomotors equipped with tilt-pan brackets. The\nYOLO model achieves 96.7% mAP@50 on the validation set consisting of annotated\nimages simulating surgical scenes with the blue spherical marker. By automating\nthe lighting process, this machine vision-based solution reduces physical\nstrain on surgeons, improves consistency in illumination, and supports improved\nsurgical outcomes.",
      "authors": [
        {
          "name": "Amir Gharghabi",
          "affiliation": null
        },
        {
          "name": "Mahdi Hakiminezhad",
          "affiliation": null
        },
        {
          "name": "Maryam Shafaei",
          "affiliation": null
        },
        {
          "name": "Shaghayegh Gharghabi",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "cs.HC"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17287v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17287v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17281v1",
      "title": "MemoryBench: A Benchmark for Memory and Continual Learning in LLM Systems",
      "abstract": "Scaling up data, parameters, and test-time computation has been the\nmainstream methods to improve LLM systems (LLMsys), but their upper bounds are\nalmost reached due to the gradual depletion of high-quality data and marginal\ngains obtained from larger computational resource consumption. Inspired by the\nabilities of human and traditional AI systems in learning from practice,\nconstructing memory and continual learning frameworks for LLMsys has become an\nimportant and popular research direction in recent literature. Yet, existing\nbenchmarks for LLM memory often focus on evaluating the system on homogeneous\nreading comprehension tasks with long-form inputs rather than testing their\nabilities to learn from accumulated user feedback in service time. Therefore,\nwe propose a user feedback simulation framework and a comprehensive benchmark\ncovering multiple domains, languages, and types of tasks to evaluate the\ncontinual learning abilities of LLMsys. Experiments show that the effectiveness\nand efficiency of state-of-the-art baselines are far from satisfying, and we\nhope this benchmark could pave the way for future studies on LLM memory and\noptimization algorithms.",
      "authors": [
        {
          "name": "Qingyao Ai",
          "affiliation": null
        },
        {
          "name": "Yichen Tang",
          "affiliation": null
        },
        {
          "name": "Changyue Wang",
          "affiliation": null
        },
        {
          "name": "Jianming Long",
          "affiliation": null
        },
        {
          "name": "Weihang Su",
          "affiliation": null
        },
        {
          "name": "Yiqun Liu",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IR"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17281v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17281v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17278v1",
      "title": "SG-CLDFF: A Novel Framework for Automated White Blood Cell Classification and Segmentation",
      "abstract": "Accurate segmentation and classification of white blood cells (WBCs) in\nmicroscopic images are essential for diagnosis and monitoring of many\nhematological disorders, yet remain challenging due to staining variability,\ncomplex backgrounds, and class imbalance. In this paper, we introduce a novel\nSaliency-Guided Cross-Layer Deep Feature Fusion framework (SG-CLDFF) that\ntightly integrates saliency-driven preprocessing with multi-scale deep feature\naggregation to improve both robustness and interpretability for WBC analysis.\nSG-CLDFF first computes saliency priors to highlight candidate WBC regions and\nguide subsequent feature extraction. A lightweight hybrid backbone\n(EfficientSwin-style) produces multi-resolution representations, which are\nfused by a ResNeXt-CC-inspired cross-layer fusion module to preserve\ncomplementary information from shallow and deep layers. The network is trained\nin a multi-task setup with concurrent segmentation and cell-type classification\nheads, using class-aware weighted losses and saliency-alignment regularization\nto mitigate imbalance and suppress background activation. Interpretability is\nenforced through Grad-CAM visualizations and saliency consistency checks,\nallowing model decisions to be inspected at the regional level. We validate the\nframework on standard public benchmarks (BCCD, LISC, ALL-IDB), reporting\nconsistent gains in IoU, F1, and classification accuracy compared to strong CNN\nand transformer baselines. An ablation study also demonstrates the individual\ncontributions of saliency preprocessing and cross-layer fusion. SG-CLDFF offers\na practical and explainable path toward more reliable automated WBC analysis in\nclinical workflows.",
      "authors": [
        {
          "name": "Mehdi Zekriyapanah Gashti",
          "affiliation": null
        },
        {
          "name": "Mostafa Mohammadpour",
          "affiliation": null
        },
        {
          "name": "Ghasem Farjamnia",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "68T07, 92C55",
        "I.4.6; I.2.6"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17278v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17278v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17276v1",
      "title": "Breaking and Fixing Defenses Against Control-Flow Hijacking in Multi-Agent Systems",
      "abstract": "Control-flow hijacking attacks manipulate orchestration mechanisms in\nmulti-agent systems into performing unsafe actions that compromise the system\nand exfiltrate sensitive information. Recently proposed defenses, such as\nLlamaFirewall, rely on alignment checks of inter-agent communications to ensure\nthat all agent invocations are \"related to\" and \"likely to further\" the\noriginal objective.\n  We start by demonstrating control-flow hijacking attacks that evade these\ndefenses even if alignment checks are performed by advanced LLMs. We argue that\nthe safety and functionality objectives of multi-agent systems fundamentally\nconflict with each other. This conflict is exacerbated by the brittle\ndefinitions of \"alignment\" and the checkers' incomplete visibility into the\nexecution context.\n  We then propose, implement, and evaluate ControlValve, a new defense inspired\nby the principles of control-flow integrity and least privilege. ControlValve\n(1) generates permitted control-flow graphs for multi-agent systems, and (2)\nenforces that all executions comply with these graphs, along with contextual\nrules (generated in a zero-shot manner) for each agent invocation.",
      "authors": [
        {
          "name": "Rishi Jha",
          "affiliation": null
        },
        {
          "name": "Harold Triedman",
          "affiliation": null
        },
        {
          "name": "Justin Wagle",
          "affiliation": null
        },
        {
          "name": "Vitaly Shmatikov",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cs.CR",
        "cs.SY",
        "eess.SY"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17276v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17276v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17274v1",
      "title": "Enhanced Motion Forecasting with Plug-and-Play Multimodal Large Language Models",
      "abstract": "Current autonomous driving systems rely on specialized models for perceiving\nand predicting motion, which demonstrate reliable performance in standard\nconditions. However, generalizing cost-effectively to diverse real-world\nscenarios remains a significant challenge. To address this, we propose\nPlug-and-Forecast (PnF), a plug-and-play approach that augments existing motion\nforecasting models with multimodal large language models (MLLMs). PnF builds on\nthe insight that natural language provides a more effective way to describe and\nhandle complex scenarios, enabling quick adaptation to targeted behaviors. We\ndesign prompts to extract structured scene understanding from MLLMs and distill\nthis information into learnable embeddings to augment existing behavior\nprediction models. Our method leverages the zero-shot reasoning capabilities of\nMLLMs to achieve significant improvements in motion prediction performance,\nwhile requiring no fine-tuning -- making it practical to adopt. We validate our\napproach on two state-of-the-art motion forecasting models using the Waymo Open\nMotion Dataset and the nuScenes Dataset, demonstrating consistent performance\nimprovements across both benchmarks.",
      "authors": [
        {
          "name": "Katie Luo",
          "affiliation": null
        },
        {
          "name": "Jingwei Ji",
          "affiliation": null
        },
        {
          "name": "Tong He",
          "affiliation": null
        },
        {
          "name": "Runsheng Xu",
          "affiliation": null
        },
        {
          "name": "Yichen Xie",
          "affiliation": null
        },
        {
          "name": "Dragomir Anguelov",
          "affiliation": null
        },
        {
          "name": "Mingxing Tan",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17274v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17274v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17269v1",
      "title": "FineVision: Open Data Is All You Need",
      "abstract": "The advancement of vision-language models (VLMs) is hampered by a fragmented\nlandscape of inconsistent and contaminated public datasets. We introduce\nFineVision, a meticulously collected, curated, and unified corpus of 24 million\nsamples - the largest open resource of its kind. We unify more than 200 sources\ninto 185 subsets via a semi-automated, human-in-the-loop pipeline: automation\nperforms bulk ingestion and schema mapping, while reviewers audit mappings and\nspot-check outputs to verify faithful consumption of annotations, appropriate\nformatting and diversity, and safety; issues trigger targeted fixes and\nre-runs. The workflow further applies rigorous de-duplication within and across\nsources and decontamination against 66 public benchmarks. FineVision also\nencompasses agentic/GUI tasks with a unified action space; reviewers validate\nschemas and inspect a sample of trajectories to confirm executable fidelity.\nModels trained on FineVision consistently outperform those trained on existing\nopen mixtures across a broad evaluation suite, underscoring the benefits of\nscale, data hygiene, and balanced automation with human oversight. We release\nthe corpus and curation tools to accelerate data-centric VLM research.",
      "authors": [
        {
          "name": "Luis Wiedmann",
          "affiliation": null
        },
        {
          "name": "Orr Zohar",
          "affiliation": null
        },
        {
          "name": "Amir Mahla",
          "affiliation": null
        },
        {
          "name": "Xiaohan Wang",
          "affiliation": null
        },
        {
          "name": "Rui Li",
          "affiliation": null
        },
        {
          "name": "Thibaud Frere",
          "affiliation": null
        },
        {
          "name": "Leandro von Werra",
          "affiliation": null
        },
        {
          "name": "Aritra Roy Gosthipaty",
          "affiliation": null
        },
        {
          "name": "Andr\u00e9s Marafioti",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17269v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17269v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17268v1",
      "title": "Uncertainty-aware data assimilation through variational inference",
      "abstract": "Data assimilation, consisting in the combination of a dynamical model with a\nset of noisy and incomplete observations in order to infer the state of a\nsystem over time, involves uncertainty in most settings. Building upon an\nexisting deterministic machine learning approach, we propose a variational\ninference-based extension in which the predicted state follows a multivariate\nGaussian distribution. Using the chaotic Lorenz-96 dynamics as a testing\nground, we show that our new model enables to obtain nearly perfectly\ncalibrated predictions, and can be integrated in a wider variational data\nassimilation pipeline in order to achieve greater benefit from increasing\nlengths of data assimilation windows. Our code is available at\nhttps://github.com/anthony-frion/Stochastic_CODA.",
      "authors": [
        {
          "name": "Anthony Frion",
          "affiliation": null
        },
        {
          "name": "David S Greenberg",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17268v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17268v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17266v1",
      "title": "Adaptive Discretization for Consistency Models",
      "abstract": "Consistency Models (CMs) have shown promise for efficient one-step\ngeneration. However, most existing CMs rely on manually designed discretization\nschemes, which can cause repeated adjustments for different noise schedules and\ndatasets. To address this, we propose a unified framework for the automatic and\nadaptive discretization of CMs, formulating it as an optimization problem with\nrespect to the discretization step. Concretely, during the consistency training\nprocess, we propose using local consistency as the optimization objective to\nensure trainability by avoiding excessive discretization, and taking global\nconsistency as a constraint to ensure stability by controlling the denoising\nerror in the training target. We establish the trade-off between local and\nglobal consistency with a Lagrange multiplier. Building on this framework, we\nachieve adaptive discretization for CMs using the Gauss-Newton method. We refer\nto our approach as ADCMs. Experiments demonstrate that ADCMs significantly\nimprove the training efficiency of CMs, achieving superior generative\nperformance with minimal training overhead on both CIFAR-10 and ImageNet.\nMoreover, ADCMs exhibit strong adaptability to more advanced DM variants. Code\nis available at https://github.com/rainstonee/ADCM.",
      "authors": [
        {
          "name": "Jiayu Bai",
          "affiliation": null
        },
        {
          "name": "Zhanbo Feng",
          "affiliation": null
        },
        {
          "name": "Zhijie Deng",
          "affiliation": null
        },
        {
          "name": "Tianqi Hou",
          "affiliation": null
        },
        {
          "name": "Robert C. Qiu",
          "affiliation": null
        },
        {
          "name": "Zenan Ling",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17266v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17266v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17264v1",
      "title": "Fair and Interpretable Deepfake Detection in Videos",
      "abstract": "Existing deepfake detection methods often exhibit bias, lack transparency,\nand fail to capture temporal information, leading to biased decisions and\nunreliable results across different demographic groups. In this paper, we\npropose a fairness-aware deepfake detection framework that integrates temporal\nfeature learning and demographic-aware data augmentation to enhance fairness\nand interpretability. Our method leverages sequence-based clustering for\ntemporal modeling of deepfake videos and concept extraction to improve\ndetection reliability while also facilitating interpretable decisions for\nnon-expert users. Additionally, we introduce a demography-aware data\naugmentation method that balances underrepresented groups and applies\nfrequency-domain transformations to preserve deepfake artifacts, thereby\nmitigating bias and improving generalization. Extensive experiments on\nFaceForensics++, DFD, Celeb-DF, and DFDC datasets using state-of-the-art (SoTA)\narchitectures (Xception, ResNet) demonstrate the efficacy of the proposed\nmethod in obtaining the best tradeoff between fairness and accuracy when\ncompared to SoTA.",
      "authors": [
        {
          "name": "Akihito Yoshii",
          "affiliation": null
        },
        {
          "name": "Ryosuke Sonoda",
          "affiliation": null
        },
        {
          "name": "Ramya Srinivasan",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17264v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17264v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17263v1",
      "title": "TaxoAlign: Scholarly Taxonomy Generation Using Language Models",
      "abstract": "Taxonomies play a crucial role in helping researchers structure and navigate\nknowledge in a hierarchical manner. They also form an important part in the\ncreation of comprehensive literature surveys. The existing approaches to\nautomatic survey generation do not compare the structure of the generated\nsurveys with those written by human experts. To address this gap, we present\nour own method for automated taxonomy creation that can bridge the gap between\nhuman-generated and automatically-created taxonomies. For this purpose, we\ncreate the CS-TaxoBench benchmark which consists of 460 taxonomies that have\nbeen extracted from human-written survey papers. We also include an additional\ntest set of 80 taxonomies curated from conference survey papers. We propose\nTaxoAlign, a three-phase topic-based instruction-guided method for scholarly\ntaxonomy generation. Additionally, we propose a stringent automated evaluation\nframework that measures the structural alignment and semantic coherence of\nautomatically generated taxonomies in comparison to those created by human\nexperts. We evaluate our method and various baselines on CS-TaxoBench, using\nboth automated evaluation metrics and human evaluation studies. The results\nshow that TaxoAlign consistently surpasses the baselines on nearly all metrics.\nThe code and data can be found at https://github.com/AvishekLahiri/TaxoAlign.",
      "authors": [
        {
          "name": "Avishek Lahiri",
          "affiliation": null
        },
        {
          "name": "Yufang Hou",
          "affiliation": null
        },
        {
          "name": "Debarshi Kumar Sanyal",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17263v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17263v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17261v1",
      "title": "High-Level Multi-Robot Trajectory Planning And Spurious Behavior Detection",
      "abstract": "The reliable execution of high-level missions in multi-robot systems with\nheterogeneous agents, requires robust methods for detecting spurious behaviors.\nIn this paper, we address the challenge of identifying spurious executions of\nplans specified as a Linear Temporal Logic (LTL) formula, as incorrect task\nsequences, violations of spatial constraints, timing inconsis- tencies, or\ndeviations from intended mission semantics. To tackle this, we introduce a\nstructured data generation framework based on the Nets-within-Nets (NWN)\nparadigm, which coordinates robot actions with LTL-derived global mission\nspecifications. We further propose a Transformer-based anomaly detection\npipeline that classifies robot trajectories as normal or anomalous. Experi-\nmental evaluations show that our method achieves high accuracy (91.3%) in\nidentifying execution inefficiencies, and demonstrates robust detection\ncapabilities for core mission violations (88.3%) and constraint-based adaptive\nanomalies (66.8%). An ablation experiment of the embedding and architecture was\ncarried out, obtaining successful results where our novel proposition performs\nbetter than simpler representations.",
      "authors": [
        {
          "name": "Fernando Salanova",
          "affiliation": null
        },
        {
          "name": "Jes\u00fas Roche",
          "affiliation": null
        },
        {
          "name": "Cristian Mahuela",
          "affiliation": null
        },
        {
          "name": "Eduardo Montijano",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17261v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17261v1",
      "primary_category": "cs.RO",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17256v1",
      "title": "Explainability of Large Language Models: Opportunities and Challenges toward Generating Trustworthy Explanations",
      "abstract": "Large language models have exhibited impressive performance across a broad\nrange of downstream tasks in natural language processing. However, how a\nlanguage model predicts the next token and generates content is not generally\nunderstandable by humans. Furthermore, these models often make errors in\nprediction and reasoning, known as hallucinations. These errors underscore the\nurgent need to better understand and interpret the intricate inner workings of\nlanguage models and how they generate predictive outputs. Motivated by this\ngap, this paper investigates local explainability and mechanistic\ninterpretability within Transformer-based large language models to foster trust\nin such models. In this regard, our paper aims to make three key contributions.\nFirst, we present a review of local explainability and mechanistic\ninterpretability approaches and insights from relevant studies in the\nliterature. Furthermore, we describe experimental studies on explainability and\nreasoning with large language models in two critical domains -- healthcare and\nautonomous driving -- and analyze the trust implications of such explanations\nfor explanation receivers. Finally, we summarize current unaddressed issues in\nthe evolving landscape of LLM explainability and outline the opportunities,\ncritical challenges, and future directions toward generating human-aligned,\ntrustworthy LLM explanations.",
      "authors": [
        {
          "name": "Shahin Atakishiyev",
          "affiliation": null
        },
        {
          "name": "Housam K. B. Babiker",
          "affiliation": null
        },
        {
          "name": "Jiayi Dai",
          "affiliation": null
        },
        {
          "name": "Nawshad Farruque",
          "affiliation": null
        },
        {
          "name": "Teruaki Hayashi",
          "affiliation": null
        },
        {
          "name": "Nafisa Sadaf Hriti",
          "affiliation": null
        },
        {
          "name": "Md Abed Rahman",
          "affiliation": null
        },
        {
          "name": "Iain Smith",
          "affiliation": null
        },
        {
          "name": "Mi-Young Kim",
          "affiliation": null
        },
        {
          "name": "Osmar R. Za\u00efane",
          "affiliation": null
        },
        {
          "name": "Randy Goebel",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17256v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17256v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17253v1",
      "title": "Augmented Web Usage Mining and User Experience Optimization with CAWAL's Enriched Analytics Data",
      "abstract": "Understanding user behavior on the web is increasingly critical for\noptimizing user experience (UX). This study introduces Augmented Web Usage\nMining (AWUM), a methodology designed to enhance web usage mining and improve\nUX by enriching the interaction data provided by CAWAL (Combined Application\nLog and Web Analytics), a framework for advanced web analytics. Over 1.2\nmillion session records collected in one month (~8.5GB of data) were processed\nand transformed into enriched datasets. AWUM analyzes session structures, page\nrequests, service interactions, and exit methods. Results show that 87.16% of\nsessions involved multiple pages, contributing 98.05% of total pageviews; 40%\nof users accessed various services and 50% opted for secure exits. Association\nrule mining revealed patterns of frequently accessed services, highlighting\nCAWAL's precision and efficiency over conventional methods. AWUM offers a\ncomprehensive understanding of user behavior and strong potential for\nlarge-scale UX optimization.",
      "authors": [
        {
          "name": "\u00d6zkan Canay",
          "affiliation": null
        },
        {
          "name": "{\u00dc}mit Kocab\u0131cak",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.HC",
        "cs.AI",
        "68T09, 68U35",
        "H.5.2; H.3.3; I.2.6"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17253v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17253v1",
      "primary_category": "cs.HC",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17252v1",
      "title": "How News Feels: Understanding Affective Bias in Multilingual Headlines for Human-Centered Media Design",
      "abstract": "News media often shape the public mood not only by what they report but by\nhow they frame it. The same event can appear calm in one outlet and alarming in\nanother, reflecting subtle emotional bias in reporting. Negative or emotionally\ncharged headlines tend to attract more attention and spread faster, which in\nturn encourages outlets to frame stories in ways that provoke stronger\nreactions. This research explores that tendency through large-scale emotion\nanalysis of Bengali news. Using zero-shot inference with Gemma-3 4B, we\nanalyzed 300000 Bengali news headlines and their content to identify the\ndominant emotion and overall tone of each. The findings reveal a clear\ndominance of negative emotions, particularly anger, fear, and disappointment,\nand significant variation in how similar stories are emotionally portrayed\nacross outlets. Based on these insights, we propose design ideas for a\nhuman-centered news aggregator that visualizes emotional cues and helps readers\nrecognize hidden affective framing in daily news.",
      "authors": [
        {
          "name": "Mohd Ruhul Ameen",
          "affiliation": null
        },
        {
          "name": "Akif Islam",
          "affiliation": null
        },
        {
          "name": "Abu Saleh Musa Miah",
          "affiliation": null
        },
        {
          "name": "Ayesha Siddiqua",
          "affiliation": null
        },
        {
          "name": "Jungpil Shin",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17252v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17252v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17250v1",
      "title": "A Prototypical Network with an Attention-based Encoder for Drivers Identification Application",
      "abstract": "Driver identification has become an area of increasing interest in recent\nyears, especially for data- driven applications, because biometric-based\ntechnologies may incur privacy issues. This study proposes a deep learning\nneural network architecture, an attention-based encoder (AttEnc), which uses an\nattention mechanism for driver identification and uses fewer model parameters\nthan current methods. Most studies do not address the issue of data shortages\nfor driver identification, and most of them are inflexible when encountering\nunknown drivers. In this study, an architecture that combines a prototypical\nnetwork and an attention-based encoder (P-AttEnc) is proposed. It applies\nfew-shot learning to overcome the data shortage issues and to enhance model\ngeneralizations. The experiments showed that the attention-based encoder can\nidentify drivers with accuracies of 99.3%, 99.0% and 99.9% in three different\ndatasets and has a prediction time that is 44% to 79% faster because it\nsignificantly reduces, on average, 87.6% of the model parameters. P-AttEnc\nidentifies drivers based on few shot data, extracts driver fingerprints to\naddress the issue of data shortages, and is able to classify unknown drivers.\nThe first experiment showed that P-AttEnc can identify drivers with an accuracy\nof 69.8% in the one-shot scenario. The second experiment showed that P-AttEnc,\nin the 1-shot scenario, can classify unknown drivers with an average accuracy\nof 65.7%.",
      "authors": [
        {
          "name": "Wei-Hsun Lee",
          "affiliation": null
        },
        {
          "name": "Che-Yu Chang",
          "affiliation": null
        },
        {
          "name": "Kuang-Yu Li",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17250v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17250v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17247v1",
      "title": "From Preferences to Prejudice: The Role of Alignment Tuning in Shaping Social Bias in Video Diffusion Models",
      "abstract": "Recent advances in video diffusion models have significantly enhanced\ntext-to-video generation, particularly through alignment tuning using reward\nmodels trained on human preferences. While these methods improve visual\nquality, they can unintentionally encode and amplify social biases. To\nsystematically trace how such biases evolve throughout the alignment pipeline,\nwe introduce VideoBiasEval, a comprehensive diagnostic framework for evaluating\nsocial representation in video generation. Grounded in established social bias\ntaxonomies, VideoBiasEval employs an event-based prompting strategy to\ndisentangle semantic content (actions and contexts) from actor attributes\n(gender and ethnicity). It further introduces multi-granular metrics to\nevaluate (1) overall ethnicity bias, (2) gender bias conditioned on ethnicity,\n(3) distributional shifts in social attributes across model variants, and (4)\nthe temporal persistence of bias within videos. Using this framework, we\nconduct the first end-to-end analysis connecting biases in human preference\ndatasets, their amplification in reward models, and their propagation through\nalignment-tuned video diffusion models. Our results reveal that alignment\ntuning not only strengthens representational biases but also makes them\ntemporally stable, producing smoother yet more stereotyped portrayals. These\nfindings highlight the need for bias-aware evaluation and mitigation throughout\nthe alignment process to ensure fair and socially responsible video generation.",
      "authors": [
        {
          "name": "Zefan Cai",
          "affiliation": null
        },
        {
          "name": "Haoyi Qiu",
          "affiliation": null
        },
        {
          "name": "Haozhe Zhao",
          "affiliation": null
        },
        {
          "name": "Ke Wan",
          "affiliation": null
        },
        {
          "name": "Jiachen Li",
          "affiliation": null
        },
        {
          "name": "Jiuxiang Gu",
          "affiliation": null
        },
        {
          "name": "Wen Xiao",
          "affiliation": null
        },
        {
          "name": "Nanyun Peng",
          "affiliation": null
        },
        {
          "name": "Junjie Hu",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL",
        "cs.CV"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17247v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17247v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17241v1",
      "title": "Visibility Allocation Systems: How Algorithmic Design Shapes Online Visibility and Societal Outcomes",
      "abstract": "Throughout application domains, we now rely extensively on algorithmic\nsystems to engage with ever-expanding datasets of information. Despite their\nbenefits, these systems are often complex (comprising of many intricate tools,\ne.g., moderation, recommender systems, prediction models), of unknown structure\n(due to the lack of accompanying documentation), and having hard-to-predict yet\npotentially severe downstream consequences (due to the extensive use,\nsystematic enactment of existing errors, and many comprising feedback loops).\nAs such, understanding and evaluating these systems as a whole remains a\nchallenge for both researchers and legislators. To aid ongoing efforts, we\nintroduce a formal framework for such visibility allocation systems (VASs)\nwhich we define as (semi-)automated systems deciding which (processed) data to\npresent a human user with. We review typical tools comprising VASs and define\nthe associated computational problems they solve. By doing so, VASs can be\ndecomposed into sub-processes and illustrated via data flow diagrams. Moreover,\nwe survey metrics for evaluating VASs throughout the pipeline, thus aiding\nsystem diagnostics. Using forecasting-based recommendations in school choice as\na case study, we demonstrate how our framework can support VAS evaluation. We\nalso discuss how our framework can support ongoing AI-legislative efforts to\nlocate obligations, quantify systemic risks, and enable adaptive compliance.",
      "authors": [
        {
          "name": "Stefania Ionescu",
          "affiliation": null
        },
        {
          "name": "Robin Forsberg",
          "affiliation": null
        },
        {
          "name": "Elsa Lichtenegger",
          "affiliation": null
        },
        {
          "name": "Salima Jaoua",
          "affiliation": null
        },
        {
          "name": "Kshitijaa Jaglan",
          "affiliation": null
        },
        {
          "name": "Florian Dorfler",
          "affiliation": null
        },
        {
          "name": "Aniko Hannak",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17241v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17241v1",
      "primary_category": "cs.CY",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17238v1",
      "title": "StreamingThinker: Large Language Models Can Think While Reading",
      "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in\nchain of thought (CoT) reasoning. However, the current LLM reasoning paradigm\ninitiates thinking only after the entire input is available, which introduces\nunnecessary latency and weakens attention to earlier information in dynamic\nscenarios. Inspired by human cognition of thinking while reading, we first\ndesign a \\textit{\\textbf{streaming thinking}} paradigm for LLMs, where\nreasoning unfolds in the order of input and further adjusts its depth once\nreading is complete. We instantiate this paradigm with\n\\textit{StreamingThinker}, a framework that enables LLMs to think while reading\nthrough the integration of streaming CoT generation, streaming-constraint\ntraining, and streaming parallel inference. Specifically, StreamingThinker\nemploys streaming reasoning units with quality control for CoT generation,\nenforces order-preserving reasoning through streaming attention masks and\nposition encoding, and leverages parallel KV caches that decouple input\nencoding from reasoning generation, thereby ensuring alignment and enabling\ntrue concurrency. We evaluate StreamingThinker on the Qwen3 model family across\nmath reasoning, logical reasoning, and context-based QA reasoning tasks.\nExperimental results show that the StreamingThinker preserves performance\ncomparable to batch thinking, while yielding an 80\\% reduction in token waiting\nbefore the onset of reasoning and a more than 60\\% reduction in time-level\nlatency for producing the final answer, demonstrating the effectiveness of the\nstreaming paradigm for LLM reasoning. Code will be released at\n\\href{https://github.com/EIT-NLP/StreamingLLM/tree/main/StreamingThinker}{this\nrepository.}",
      "authors": [
        {
          "name": "Junlong Tong",
          "affiliation": null
        },
        {
          "name": "Yingqi Fan",
          "affiliation": null
        },
        {
          "name": "Anhao Zhao",
          "affiliation": null
        },
        {
          "name": "Yunpu Ma",
          "affiliation": null
        },
        {
          "name": "Xiaoyu Shen",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17238v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17238v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17235v1",
      "title": "Coinvisor: An RL-Enhanced Chatbot Agent for Interactive Cryptocurrency Investment Analysis",
      "abstract": "The cryptocurrency market offers significant investment opportunities but\nfaces challenges including high volatility and fragmented information. Data\nintegration and analysis are essential for informed investment decisions.\nCurrently, investors use three main approaches: (1) Manual analysis across\nvarious sources, which depends heavily on individual experience and is\ntime-consuming and prone to bias; (2) Data aggregation platforms-limited in\nfunctionality and depth of analysis; (3) Large language model agents-based on\nstatic pretrained models, lacking real-time data integration and multi-step\nreasoning capabilities. To address these limitations, we present Coinvisor, a\nreinforcement learning-based chatbot that provides comprehensive analytical\nsupport for cryptocurrency investment through a multi-agent framework.\nCoinvisor integrates diverse analytical capabilities through specialized tools.\nIts key innovation is a reinforcement learning-based tool selection mechanism\nthat enables multi-step planning and flexible integration of diverse data\nsources. This design supports real-time interaction and adaptive analysis of\ndynamic content, delivering accurate and actionable investment insights. We\nevaluated Coinvisor through automated benchmarks on tool calling accuracy and\nuser studies with 20 cryptocurrency investors using our interface. Results show\nthat Coinvisor improves recall by 40.7% and F1 score by 26.6% over the base\nmodel in tool orchestration. User studies show high satisfaction (4.64/5), with\nparticipants preferring Coinvisor to both general LLMs and existing crypto\nplatforms (4.62/5).",
      "authors": [
        {
          "name": "Chong Chen",
          "affiliation": null
        },
        {
          "name": "Ze Liu",
          "affiliation": null
        },
        {
          "name": "Lingfeng Bao",
          "affiliation": null
        },
        {
          "name": "Yanlin Wang",
          "affiliation": null
        },
        {
          "name": "Ting Chen",
          "affiliation": null
        },
        {
          "name": "Daoyuan Wu",
          "affiliation": null
        },
        {
          "name": "Jiachi Chen",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.AI"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17235v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17235v1",
      "primary_category": "cs.AI",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17234v1",
      "title": "Taming Modality Entanglement in Continual Audio-Visual Segmentation",
      "abstract": "Recently, significant progress has been made in multi-modal continual\nlearning, aiming to learn new tasks sequentially in multi-modal settings while\npreserving performance on previously learned ones. However, existing methods\nmainly focus on coarse-grained tasks, with limitations in addressing modality\nentanglement in fine-grained continual learning settings. To bridge this gap,\nwe introduce a novel Continual Audio-Visual Segmentation (CAVS) task, aiming to\ncontinuously segment new classes guided by audio. Through comprehensive\nanalysis, two critical challenges are identified: 1) multi-modal semantic\ndrift, where a sounding objects is labeled as background in sequential tasks;\n2) co-occurrence confusion, where frequent co-occurring classes tend to be\nconfused. In this work, a Collision-based Multi-modal Rehearsal (CMR) framework\nis designed to address these challenges. Specifically, for multi-modal semantic\ndrift, a Multi-modal Sample Selection (MSS) strategy is proposed to select\nsamples with high modal consistency for rehearsal. Meanwhile, for co-occurence\nconfusion, a Collision-based Sample Rehearsal (CSR) mechanism is designed,\nallowing for the increase of rehearsal sample frequency of those confusable\nclasses during training process. Moreover, we construct three audio-visual\nincremental scenarios to verify effectiveness of our method. Comprehensive\nexperiments demonstrate that our method significantly outperforms single-modal\ncontinual learning methods.",
      "authors": [
        {
          "name": "Yuyang Hong",
          "affiliation": null
        },
        {
          "name": "Qi Yang",
          "affiliation": null
        },
        {
          "name": "Tao Zhang",
          "affiliation": null
        },
        {
          "name": "Zili Wang",
          "affiliation": null
        },
        {
          "name": "Zhaojin Fu",
          "affiliation": null
        },
        {
          "name": "Kun Ding",
          "affiliation": null
        },
        {
          "name": "Bin Fan",
          "affiliation": null
        },
        {
          "name": "Shiming Xiang",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.MM",
        "cs.AI",
        "cs.CV"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17234v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17234v1",
      "primary_category": "cs.MM",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17218v1",
      "title": "When One Moment Isn't Enough: Multi-Moment Retrieval with Cross-Moment Interactions",
      "abstract": "Existing Moment retrieval (MR) methods focus on Single-Moment Retrieval\n(SMR). However, one query can correspond to multiple relevant moments in\nreal-world applications. This makes the existing datasets and methods\ninsufficient for video temporal grounding. By revisiting the gap between\ncurrent MR tasks and real-world applications, we introduce a high-quality\ndatasets called QVHighlights Multi-Moment Dataset (QV-M$^2$), along with new\nevaluation metrics tailored for multi-moment retrieval (MMR). QV-M$^2$ consists\nof 2,212 annotations covering 6,384 video segments. Building on existing\nefforts in MMR, we propose a framework called FlashMMR. Specifically, we\npropose a Multi-moment Post-verification module to refine the moment\nboundaries. We introduce constrained temporal adjustment and subsequently\nleverage a verification module to re-evaluate the candidate segments. Through\nthis sophisticated filtering pipeline, low-confidence proposals are pruned, and\nrobust multi-moment alignment is achieved. We retrain and evaluate 6 existing\nMR methods on QV-M$^2$ and QVHighlights under both SMR and MMR settings.\nResults show that QV-M$^2$ serves as an effective benchmark for training and\nevaluating MMR models, while FlashMMR provides a strong baseline. Specifically,\non QV-M$^2$, it achieves improvements over prior SOTA method by 3.00% on G-mAP,\n2.70% on mAP@3+tgt, and 2.56% on mR@3. The proposed benchmark and method\nestablish a foundation for advancing research in more realistic and challenging\nvideo temporal grounding scenarios. Code is released at\nhttps://github.com/Zhuo-Cao/QV-M2.",
      "authors": [
        {
          "name": "Zhuo Cao",
          "affiliation": null
        },
        {
          "name": "Heming Du",
          "affiliation": null
        },
        {
          "name": "Bingqing Zhang",
          "affiliation": null
        },
        {
          "name": "Xin Yu",
          "affiliation": null
        },
        {
          "name": "Xue Li",
          "affiliation": null
        },
        {
          "name": "Sen Wang",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17218v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17218v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17214v1",
      "title": "Diagnosis of Fuel Cell Health Status with Deep Sparse Auto-Encoder Neural Network",
      "abstract": "Effective and accurate diagnosis of fuel cell health status is crucial for\nensuring the stable operation of fuel cell stacks. Among various parameters,\nhigh-frequency impedance serves as a critical indicator for assessing fuel cell\nstate and health conditions. However, its online testing is prohibitively\ncomplex and costly. This paper employs a deep sparse auto-encoding network for\nthe prediction and classification of high-frequency impedance in fuel cells,\nachieving metric of accuracy rate above 92\\%. The network is further deployed\non an FPGA, attaining a hardware-based recognition rate almost 90\\%.",
      "authors": [
        {
          "name": "Chenyan Fei",
          "affiliation": null
        },
        {
          "name": "Dalin Zhang",
          "affiliation": null
        },
        {
          "name": "Chen Melinda Dang",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17214v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17214v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17212v1",
      "title": "D2C-HRHR: Discrete Actions with Double Distributional Critics for High-Risk-High-Return Tasks",
      "abstract": "Tasks involving high-risk-high-return (HRHR) actions, such as obstacle\ncrossing, often exhibit multimodal action distributions and stochastic returns.\nMost reinforcement learning (RL) methods assume unimodal Gaussian policies and\nrely on scalar-valued critics, which limits their effectiveness in HRHR\nsettings. We formally define HRHR tasks and theoretically show that Gaussian\npolicies cannot guarantee convergence to the optimal solution. To address this,\nwe propose a reinforcement learning framework that (i) discretizes continuous\naction spaces to approximate multimodal distributions, (ii) employs\nentropy-regularized exploration to improve coverage of risky but rewarding\nactions, and (iii) introduces a dual-critic architecture for more accurate\ndiscrete value distribution estimation. The framework scales to\nhigh-dimensional action spaces, supporting complex control domains. Experiments\non locomotion and manipulation benchmarks with high risks of failure\ndemonstrate that our method outperforms baselines, underscoring the importance\nof explicitly modeling multimodality and risk in RL.",
      "authors": [
        {
          "name": "Jundong Zhang",
          "affiliation": null
        },
        {
          "name": "Yuhui Situ",
          "affiliation": null
        },
        {
          "name": "Fanji Zhang",
          "affiliation": null
        },
        {
          "name": "Rongji Deng",
          "affiliation": null
        },
        {
          "name": "Tianqi Wei",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17212v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17212v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17211v1",
      "title": "Temporally Detailed Hypergraph Neural ODEs for Type 2 Diabetes Progression Modeling",
      "abstract": "Disease progression modeling aims to characterize and predict how a patient's\ndisease complications worsen over time based on longitudinal electronic health\nrecords (EHRs). Accurate modeling of disease progression, such as type 2\ndiabetes, can enhance patient sub-phenotyping and inform effective and timely\ninterventions. However, the problem is challenging due to the need to learn\ncontinuous-time dynamics of progression patterns based on irregular-time event\nsamples and patient heterogeneity (\\eg different progression rates and\npathways). Existing mechanistic and data-driven methods either lack\nadaptability to learn from real-world data or fail to capture complex\ncontinuous-time dynamics on progression trajectories. To address these\nlimitations, we propose Temporally Detailed Hypergraph Neural Ordinary\nDifferential Equation (TD-HNODE), which represents disease progression on\nclinically recognized trajectories as a temporally detailed hypergraph and\nlearns the continuous-time progression dynamics via a neural ODE framework.\nTD-HNODE contains a learnable TD-Hypergraph Laplacian that captures the\ninterdependency of disease complication markers within both intra- and\ninter-progression trajectories. Experiments on two real-world clinical datasets\ndemonstrate that TD-HNODE outperforms multiple baselines in modeling the\nprogression of type 2 diabetes and related cardiovascular diseases.",
      "authors": [
        {
          "name": "Tingsong Xiao",
          "affiliation": null
        },
        {
          "name": "Yao An Lee",
          "affiliation": null
        },
        {
          "name": "Zelin Xu",
          "affiliation": null
        },
        {
          "name": "Yupu Zhang",
          "affiliation": null
        },
        {
          "name": "Zibo Liu",
          "affiliation": null
        },
        {
          "name": "Yu Huang",
          "affiliation": null
        },
        {
          "name": "Jiang Bian",
          "affiliation": null
        },
        {
          "name": "Serena Jingchuan Guo",
          "affiliation": null
        },
        {
          "name": "Zhe Jiang",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17211v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17211v1",
      "primary_category": "cs.AI",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17210v1",
      "title": "Wisdom is Knowing What not to Say: Hallucination-Free LLMs Unlearning via Attention Shifting",
      "abstract": "The increase in computing power and the necessity of AI-assisted\ndecision-making boost the growing application of large language models (LLMs).\nAlong with this, the potential retention of sensitive data of LLMs has spurred\nincreasing research into machine unlearning. However, existing unlearning\napproaches face a critical dilemma: Aggressive unlearning compromises model\nutility, while conservative strategies preserve utility but risk hallucinated\nresponses. This significantly limits LLMs' reliability in knowledge-intensive\napplications. To address this, we introduce a novel Attention-Shifting (AS)\nframework for selective unlearning. AS is driven by two design objectives: (1)\ncontext-preserving suppression that attenuates attention to fact-bearing tokens\nwithout disrupting LLMs' linguistic structure; and (2) hallucination-resistant\nresponse shaping that discourages fabricated completions when queried about\nunlearning content. AS realizes these objectives through two attention-level\ninterventions, which are importance-aware suppression applied to the unlearning\nset to reduce reliance on memorized knowledge and attention-guided retention\nenhancement that reinforces attention toward semantically essential tokens in\nthe retained dataset to mitigate unintended degradation. These two components\nare jointly optimized via a dual-loss objective, which forms a soft boundary\nthat localizes unlearning while preserving unrelated knowledge under\nrepresentation superposition. Experimental results show that AS improves\nperformance preservation over the state-of-the-art unlearning methods,\nachieving up to 15% higher accuracy on the ToFU benchmark and 10% on the TDEC\nbenchmark, while maintaining competitive hallucination-free unlearning\neffectiveness. Compared to existing methods, AS demonstrates a superior balance\nbetween unlearning effectiveness, generalization, and response reliability.",
      "authors": [
        {
          "name": "Chenchen Tan",
          "affiliation": null
        },
        {
          "name": "Youyang Qu",
          "affiliation": null
        },
        {
          "name": "Xinghao Li",
          "affiliation": null
        },
        {
          "name": "Hui Zhang",
          "affiliation": null
        },
        {
          "name": "Shujie Cui",
          "affiliation": null
        },
        {
          "name": "Cunjian Chen",
          "affiliation": null
        },
        {
          "name": "Longxiang Gao",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17210v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17210v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17206v1",
      "title": "Soft-Masked Diffusion Language Models",
      "abstract": "Diffusion models have demonstrated strong potential in language modeling,\noffering various advantages over traditional autoregressive approaches. Their\nability to generate and revise entire responses in parallel enables faster\ngeneration and built-in self-correction mechanisms. Most modern diffusion-based\nlanguage models employ masked diffusion, where decoding involves iteratively\nprocessing masked tokens based on a binary decision: either retaining the mask\nor replacing it with the predicted token. However, this binary choice discards\nvaluable predictive information when the mask is retained. To address this\nlimitation, we introduce soft-masking (SM), a novel method that dynamically\nblends the embedding of the mask token with the embeddings of the top-$k$\npredicted tokens from the previous decoding step, for each retained mask. This\nprovides the model with a more informative prior, preserving context from\nearlier computations and allowing partial information about masked tokens to\npropagate beyond a single step. We propose a training methodology that adapts a\npretrained masked diffusion language model to incorporate SM. We demonstrate\nthat continuing pretraining a 169M parameter model with SM leads to improved\nperplexity and MAUVE scores. Furthermore, we finetune two state-of-the-art\ndiffusion models, Dream-7B and Dream-Coder-7B, with SM. SM consistently\nimproves performance across multiple coding benchmarks, particularly in\nhigh-throughput settings.",
      "authors": [
        {
          "name": "Michael Hersche",
          "affiliation": null
        },
        {
          "name": "Samuel Moor-Smith",
          "affiliation": null
        },
        {
          "name": "Thomas Hofmann",
          "affiliation": null
        },
        {
          "name": "Abbas Rahimi",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17206v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17206v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17205v1",
      "title": "$\\mathcal{V}isi\\mathcal{P}runer$: Decoding Discontinuous Cross-Modal Dynamics for Efficient Multimodal LLMs",
      "abstract": "Multimodal Large Language Models (MLLMs) have achieved strong performance\nacross vision-language tasks, but suffer from significant computational\noverhead due to the quadratic growth of attention computations with the number\nof multimodal tokens. Though efforts have been made to prune tokens in MLLMs,\n\\textit{they lack a fundamental understanding of how MLLMs process and fuse\nmultimodal information.} Through systematic analysis, we uncover a\n\\textbf{three-stage} cross-modal interaction process: (1) Shallow layers\nrecognize task intent, with visual tokens acting as passive attention sinks;\n(2) Cross-modal fusion occurs abruptly in middle layers, driven by a few\ncritical visual tokens; (3) Deep layers discard vision tokens, focusing solely\non linguistic refinement. Based on these findings, we propose\n\\emph{VisiPruner}, a training-free pruning framework that reduces up to 99\\% of\nvision-related attention computations and 53.9\\% of FLOPs on LLaVA-v1.5 7B. It\nsignificantly outperforms existing token pruning methods and generalizes across\ndiverse MLLMs. Beyond pruning, our insights further provide actionable\nguidelines for training efficient MLLMs by aligning model architecture with its\nintrinsic layer-wise processing dynamics. Our code is available at:\nhttps://github.com/EIT-NLP/VisiPruner.",
      "authors": [
        {
          "name": "Yingqi Fan",
          "affiliation": null
        },
        {
          "name": "Anhao Zhao",
          "affiliation": null
        },
        {
          "name": "Jinlan Fu",
          "affiliation": null
        },
        {
          "name": "Junlong Tong",
          "affiliation": null
        },
        {
          "name": "Hui Su",
          "affiliation": null
        },
        {
          "name": "Yijie Pan",
          "affiliation": null
        },
        {
          "name": "Wei Zhang",
          "affiliation": null
        },
        {
          "name": "Xiaoyu Shen",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "cs.CL"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17205v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17205v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17201v1",
      "title": "Optimizing DINOv2 with Registers for Face Anti-Spoofing",
      "abstract": "Face recognition systems are designed to be robust against variations in head\npose, illumination, and image blur during capture. However, malicious actors\ncan exploit these systems by presenting a face photo of a registered user,\npotentially bypassing the authentication process. Such spoofing attacks must be\ndetected prior to face recognition. In this paper, we propose a DINOv2-based\nspoofing attack detection method to discern minute differences between live and\nspoofed face images. Specifically, we employ DINOv2 with registers to extract\ngeneralizable features and to suppress perturbations in the attention\nmechanism, which enables focused attention on essential and minute features. We\ndemonstrate the effectiveness of the proposed method through experiments\nconducted on the dataset provided by ``The 6th Face Anti-Spoofing Workshop:\nUnified Physical-Digital Attacks Detection@ICCV2025'' and SiW dataset.",
      "authors": [
        {
          "name": "Mika Feng",
          "affiliation": null
        },
        {
          "name": "Pierre Gallin-Martel",
          "affiliation": null
        },
        {
          "name": "Koichi Ito",
          "affiliation": null
        },
        {
          "name": "Takafumi Aoki",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17201v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17201v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17200v1",
      "title": "EndoCIL: A Class-Incremental Learning Framework for Endoscopic Image Classification",
      "abstract": "Class-incremental learning (CIL) for endoscopic image analysis is crucial for\nreal-world clinical applications, where diagnostic models should continuously\nadapt to evolving clinical data while retaining performance on previously\nlearned ones. However, existing replay-based CIL methods fail to effectively\nmitigate catastrophic forgetting due to severe domain discrepancies and class\nimbalance inherent in endoscopic imaging. To tackle these challenges, we\npropose EndoCIL, a novel and unified CIL framework specifically tailored for\nendoscopic image diagnosis. EndoCIL incorporates three key components: Maximum\nMean Discrepancy Based Replay (MDBR), employing a distribution-aligned greedy\nstrategy to select diverse and representative exemplars, Prior Regularized\nClass Balanced Loss (PRCBL), designed to alleviate both inter-phase and\nintra-phase class imbalance by integrating prior class distributions and\nbalance weights into the loss function, and Calibration of Fully-Connected\nGradients (CFG), which adjusts the classifier gradients to mitigate bias toward\nnew classes. Extensive experiments conducted on four public endoscopic datasets\ndemonstrate that EndoCIL generally outperforms state-of-the-art CIL methods\nacross varying buffer sizes and evaluation metrics. The proposed framework\neffectively balances stability and plasticity in lifelong endoscopic diagnosis,\nshowing promising potential for clinical scalability and deployment.",
      "authors": [
        {
          "name": "Bingrong Liu",
          "affiliation": null
        },
        {
          "name": "Jun Shi",
          "affiliation": null
        },
        {
          "name": "Yushan Zheng",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17200v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17200v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17199v1",
      "title": "Round Outcome Prediction in VALORANT Using Tactical Features from Video Analysis",
      "abstract": "Recently, research on predicting match outcomes in esports has been actively\nconducted, but much of it is based on match log data and statistical\ninformation. This research targets the FPS game VALORANT, which requires\ncomplex strategies, and aims to build a round outcome prediction model by\nanalyzing minimap information in match footage. Specifically, based on the\nvideo recognition model TimeSformer, we attempt to improve prediction accuracy\nby incorporating detailed tactical features extracted from minimap information,\nsuch as character position information and other in-game events. This paper\nreports preliminary results showing that a model trained on a dataset augmented\nwith such tactical event labels achieved approximately 81% prediction accuracy,\nespecially from the middle phases of a round onward, significantly\noutperforming a model trained on a dataset with the minimap information itself.\nThis suggests that leveraging tactical features from match footage is highly\neffective for predicting round outcomes in VALORANT.",
      "authors": [
        {
          "name": "Nirai Hayakawa",
          "affiliation": null
        },
        {
          "name": "Kazumasa Shimari",
          "affiliation": null
        },
        {
          "name": "Kazuma Yamasaki",
          "affiliation": null
        },
        {
          "name": "Hirotatsu Hoshikawa",
          "affiliation": null
        },
        {
          "name": "Rikuto Tsuchida",
          "affiliation": null
        },
        {
          "name": "Kenichi Matsumoto",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17199v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17199v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17198v1",
      "title": "From Pixels to People: Satellite-Based Mapping and Quantification of Riverbank Erosion and Lost Villages in Bangladesh",
      "abstract": "The great rivers of Bangladesh, arteries of commerce and sustenance, are also\nagents of relentless destruction. Each year, they swallow whole villages and\nvast tracts of farmland, erasing communities from the map and displacing\nthousands of families. To track this slow-motion catastrophe has, until now,\nbeen a Herculean task for human analysts. Here we show how a powerful\ngeneral-purpose vision model, the Segment Anything Model (SAM), can be adapted\nto this task with remarkable precision. To do this, we assembled a new dataset\n- a digital chronicle of loss compiled from historical Google Earth imagery of\nBangladesh's most vulnerable regions, including Mokterer Char Union, Kedarpur\nUnion, Balchipara village, and Chowhali Upazila, from 2003 to 2025. Crucially,\nthis dataset is the first to include manually annotated data on the settlements\nthat have vanished beneath the water. Our method first uses a simple\ncolor-channel analysis to provide a rough segmentation of land and water, and\nthen fine-tunes SAM's mask decoder to recognize the subtle signatures of\nriverbank erosion. The resulting model demonstrates a keen eye for this\ndestructive process, achieving a mean Intersection over Union of 86.30% and a\nDice score of 92.60% - a performance that significantly surpasses traditional\nmethods and off-the-shelf deep learning models. This work delivers three key\ncontributions: the first annotated dataset of disappeared settlements in\nBangladesh due to river erosion; a specialized AI model fine-tuned for this\ncritical task; and a method for quantifying land loss with compelling visual\nevidence. Together, these tools provide a powerful new lens through which\npolicymakers and disaster management agencies can monitor erosion, anticipate\nits trajectory, and ultimately protect the vulnerable communities in its path.",
      "authors": [
        {
          "name": "M Saifuzzaman Rafat",
          "affiliation": null
        },
        {
          "name": "Mohd Ruhul Ameen",
          "affiliation": null
        },
        {
          "name": "Akif Islam",
          "affiliation": null
        },
        {
          "name": "Abu Saleh Musa Miah",
          "affiliation": null
        },
        {
          "name": "Jungpil Shin",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17198v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17198v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17197v1",
      "title": "ZSPAPrune: Zero-Shot Prompt-Aware Token Pruning for Vision-Language Models",
      "abstract": "As the capabilities of Vision-Language Models (VLMs) advance, they can\nprocess increasingly large inputs, which, unlike in LLMs, generates significant\nvisual token redundancy and leads to prohibitive inference costs. While many\nmethods aim to reduce these costs by pruning visual tokens, existing\napproaches, whether based on attention or diversity, typically neglect the\nguidance of the text prompt and thus fail to prioritize task relevance. In this\nwork, we propose a novel, zero-shot method that reframes the problem by\nintroducing a prompt-aware perspective, explicitly modeling visual token\npruning as a balance between task relevance and information diversity. Our\nhierarchical approach first selects a core set of task-relevant visual tokens\nand then supplements them with diversity tokens to preserve broader context.\nExperiments across multiple models and benchmarks show that our method achieves\nperformance that matches or surpasses the state-of-the-art with only minimal\naccuracy loss, even when pruning up to 90\\% of the tokens. Furthermore, these\ngains are accompanied by significant reductions in GPU memory footprint and\ninference latency.",
      "authors": [
        {
          "name": "Pu Zhang",
          "affiliation": null
        },
        {
          "name": "Yuwei Li",
          "affiliation": null
        },
        {
          "name": "Xingyuan Xian",
          "affiliation": null
        },
        {
          "name": "Guoming Tang",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17197v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17197v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17196v1",
      "title": "Understanding and Improving Length Generalization in Hierarchical Sparse Attention Models",
      "abstract": "Effectively processing long contexts is a critical challenge for language\nmodels. While standard Transformers are limited by quadratic complexity and\npoor length extrapolation, alternative architectures like sliding window\nattention and state space models sacrifice the ability to effectively utilize\nthe full context due to their fixed-size memory. Chunk-based sparse attention\nhas emerged as a promising paradigm for extreme length generalization, yet the\nkey architectural principles underpinning its success are not yet fully\nunderstood. In this work, we present a systematic dissection of these models to\nidentify the core components driving their performance. Through a unified\nframework and comprehensive ablation studies, we demonstrate that a combination\nof three design principles is critical: (1) an expressive, non-linear Chunk\nEncoder with a dedicated CLS token to produce representations for retrieval;\n(2) a Bypassing Residual Path to stably integrate retrieved global information\nwithout it being overridden by the local residual stream; and (3) enforced\nselection sparsity during pre-training to bridge the train-test distribution\ngap. We provide a theoretical motivation for intra-chunk information processing\nand landmark generation. By combining these principles, we establish a new\nstate-of-the-art for training-free length extrapolation, successfully\ngeneralizing models trained on a 4K context to 32 million tokens on RULER and\nBABILong. Our findings provide a clear and empirically-grounded set of design\nprinciples for developing future, highly-capable long-context language models.",
      "authors": [
        {
          "name": "Jiaqi Leng",
          "affiliation": null
        },
        {
          "name": "Xiang Hu",
          "affiliation": null
        },
        {
          "name": "Junxiong Wang",
          "affiliation": null
        },
        {
          "name": "Jianguo Li",
          "affiliation": null
        },
        {
          "name": "Wei Wu",
          "affiliation": null
        },
        {
          "name": "Yucheng Lu",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17196v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17196v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17191v1",
      "title": "SimpleVSF: VLM-Scoring Fusion for Trajectory Prediction of End-to-End Autonomous Driving",
      "abstract": "End-to-end autonomous driving has emerged as a promising paradigm for\nachieving robust and intelligent driving policies. However, existing end-to-end\nmethods still face significant challenges, such as suboptimal decision-making\nin complex scenarios. In this paper,we propose SimpleVSF (Simple VLM-Scoring\nFusion), a novel framework that enhances end-to-end planning by leveraging the\ncognitive capabilities of Vision-Language Models (VLMs) and advanced trajectory\nfusion techniques. We utilize the conventional scorers and the novel\nVLM-enhanced scorers. And we leverage a robust weight fusioner for quantitative\naggregation and a powerful VLM-based fusioner for qualitative, context-aware\ndecision-making. As the leading approach in the ICCV 2025 NAVSIM v2 End-to-End\nDriving Challenge, our SimpleVSF framework demonstrates state-of-the-art\nperformance, achieving a superior balance between safety, comfort, and\nefficiency.",
      "authors": [
        {
          "name": "Peiru Zheng",
          "affiliation": null
        },
        {
          "name": "Yun Zhao",
          "affiliation": null
        },
        {
          "name": "Zhan Gong",
          "affiliation": null
        },
        {
          "name": "Hong Zhu",
          "affiliation": null
        },
        {
          "name": "Shaohua Wu",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17191v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17191v1",
      "primary_category": "cs.RO",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17189v1",
      "title": "SOLE: Hardware-Software Co-design of Softmax and LayerNorm for Efficient Transformer Inference",
      "abstract": "Transformers have shown remarkable performance in both natural language\nprocessing (NLP) and computer vision (CV) tasks. However, their real-time\ninference speed and efficiency are limited due to the inefficiency in Softmax\nand Layer Normalization (LayerNorm). Previous works based on function\napproximation suffer from inefficient implementation as they place emphasis on\ncomputation while disregarding memory overhead concerns. Moreover, such methods\nrely on retraining to compensate for approximation error which can be costly\nand inconvenient.\n  In this paper, we present SOLE, a hardware-software co-design for Softmax and\nLayerNorm which is composed of E2Softmax and AILayerNorm. E2Softmax utilizes\nlog2 quantization of exponent function and log-based division to approximate\nSoftmax while AILayerNorm adopts low-precision statistic calculation. Compared\nwith state-of-the-art designs, we achieve both low-precision calculation and\nlow bit-width storage on Softmax and LayerNorm. Experiments show that SOLE\nmaintains inference accuracy without retraining while offering orders of\nmagnitude speedup and energy savings over GPU, achieving 3.04x, 3.86x\nenergy-efficiency improvements and 2.82x, 3.32x area-efficiency improvements\nover prior state-of-the-art custom hardware for Softmax and LayerNorm,\nrespectively.",
      "authors": [
        {
          "name": "Wenxun Wang",
          "affiliation": null
        },
        {
          "name": "Shuchang Zhou",
          "affiliation": null
        },
        {
          "name": "Wenyu Sun",
          "affiliation": null
        },
        {
          "name": "Peiqin Sun",
          "affiliation": null
        },
        {
          "name": "Yongpan Liu",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cs.AR"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17189v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17189v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17188v1",
      "title": "HIDISC: A Hyperbolic Framework for Domain Generalization with Generalized Category Discovery",
      "abstract": "Generalized Category Discovery (GCD) aims to classify test-time samples into\neither seen categories** -- available during training -- or novel ones, without\nrelying on label supervision. Most existing GCD methods assume simultaneous\naccess to labeled and unlabeled data during training and arising from the same\ndomain, limiting applicability in open-world scenarios involving distribution\nshifts. Domain Generalization with GCD (DG-GCD) lifts this constraint by\nrequiring models to generalize to unseen domains containing novel categories,\nwithout accessing targetdomain data during training. The only prior DG-GCD\nmethod, DG2CD-Net, relies on episodic training with multiple synthetic domains\nand task vector aggregation, incurring high computational cost and error\naccumulation. We propose HIDISC, a hyperbolic representation learning framework\nthat achieves domain and category-level generalization without episodic\nsimulation. To expose the model to minimal but diverse domain variations, we\naugment the source domain using GPT-guided diffusion, avoiding overfitting\nwhile maintaining efficiency. To structure the representation space, we\nintroduce Tangent CutMix, a curvature-aware interpolation that synthesizes\npseudo-novel samples in tangent space, preserving manifold consistency. A\nunified loss -- combining penalized Busemann alignment, hybrid hyperbolic\ncontrastive regularization, and adaptive outlier repulsion -- **facilitates\ncompact, semantically structured embeddings. A learnable curvature parameter\nfurther adapts the geometry to dataset complexity. HIDISC achieves\nstate-of-the-art results on PACS , Office-Home , and DomainNet, consistently\noutperforming the existing Euclidean and hyperbolic (DG)-GCD baselines.",
      "authors": [
        {
          "name": "Vaibhav Rathore",
          "affiliation": null
        },
        {
          "name": "Divyam Gupta",
          "affiliation": null
        },
        {
          "name": "Biplab Banerjee",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17188v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17188v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17187v1",
      "title": "A Standardized Benchmark for Machine-Learned Molecular Dynamics using Weighted Ensemble Sampling",
      "abstract": "The rapid evolution of molecular dynamics (MD) methods, including\nmachine-learned dynamics, has outpaced the development of standardized tools\nfor method validation. Objective comparison between simulation approaches is\noften hindered by inconsistent evaluation metrics, insufficient sampling of\nrare conformational states, and the absence of reproducible benchmarks. To\naddress these challenges, we introduce a modular benchmarking framework that\nsystematically evaluates protein MD methods using enhanced sampling analysis.\nOur approach uses weighted ensemble (WE) sampling via The Weighted Ensemble\nSimulation Toolkit with Parallelization and Analysis (WESTPA), based on\nprogress coordinates derived from Time-lagged Independent Component Analysis\n(TICA), enabling fast and efficient exploration of protein conformational\nspace. The framework includes a flexible, lightweight propagator interface that\nsupports arbitrary simulation engines, allowing both classical force fields and\nmachine learning-based models. Additionally, the framework offers a\ncomprehensive evaluation suite capable of computing more than 19 different\nmetrics and visualizations across a variety of domains. We further contribute a\ndataset of nine diverse proteins, ranging from 10 to 224 residues, that span a\nvariety of folding complexities and topologies. Each protein has been\nextensively simulated at 300K for one million MD steps per starting point (4\nns). To demonstrate the utility of our framework, we perform validation tests\nusing classic MD simulations with implicit solvent and compare protein\nconformational sampling using a fully trained versus under-trained CGSchNet\nmodel. By standardizing evaluation protocols and enabling direct, reproducible\ncomparisons across MD approaches, our open-source platform lays the groundwork\nfor consistent, rigorous benchmarking across the molecular simulation\ncommunity.",
      "authors": [
        {
          "name": "Alexander Aghili",
          "affiliation": null
        },
        {
          "name": "Andy Bruce",
          "affiliation": null
        },
        {
          "name": "Daniel Sabo",
          "affiliation": null
        },
        {
          "name": "Sanya Murdeshwar",
          "affiliation": null
        },
        {
          "name": "Kevin Bachelor",
          "affiliation": null
        },
        {
          "name": "Ionut Mistreanu",
          "affiliation": null
        },
        {
          "name": "Ashwin Lokapally",
          "affiliation": null
        },
        {
          "name": "Razvan Marinescu",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "q-bio.BM",
        "92B20"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17187v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17187v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17185v1",
      "title": "Robustness in Text-Attributed Graph Learning: Insights, Trade-offs, and New Defenses",
      "abstract": "While Graph Neural Networks (GNNs) and Large Language Models (LLMs) are\npowerful approaches for learning on Text-Attributed Graphs (TAGs), a\ncomprehensive understanding of their robustness remains elusive. Current\nevaluations are fragmented, failing to systematically investigate the distinct\neffects of textual and structural perturbations across diverse models and\nattack scenarios. To address these limitations, we introduce a unified and\ncomprehensive framework to evaluate robustness in TAG learning. Our framework\nevaluates classical GNNs, robust GNNs (RGNNs), and GraphLLMs across ten\ndatasets from four domains, under diverse text-based, structure-based, and\nhybrid perturbations in both poisoning and evasion scenarios. Our extensive\nanalysis reveals multiple findings, among which three are particularly\nnoteworthy: 1) models have inherent robustness trade-offs between text and\nstructure, 2) the performance of GNNs and RGNNs depends heavily on the text\nencoder and attack type, and 3) GraphLLMs are particularly vulnerable to\ntraining data corruption. To overcome the identified trade-offs, we introduce\nSFT-auto, a novel framework that delivers superior and balanced robustness\nagainst both textual and structural attacks within a single model. Our work\nestablishes a foundation for future research on TAG security and offers\npractical solutions for robust TAG learning in adversarial environments. Our\ncode is available at: https://github.com/Leirunlin/TGRB.",
      "authors": [
        {
          "name": "Runlin Lei",
          "affiliation": null
        },
        {
          "name": "Lu Yi",
          "affiliation": null
        },
        {
          "name": "Mingguo He",
          "affiliation": null
        },
        {
          "name": "Pengyu Qiu",
          "affiliation": null
        },
        {
          "name": "Zhewei Wei",
          "affiliation": null
        },
        {
          "name": "Yongchao Liu",
          "affiliation": null
        },
        {
          "name": "Chuntao Hong",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17185v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17185v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17181v1",
      "title": "Capturing Head Avatar with Hand Contacts from a Monocular Video",
      "abstract": "Photorealistic 3D head avatars are vital for telepresence, gaming, and VR.\nHowever, most methods focus solely on facial regions, ignoring natural\nhand-face interactions, such as a hand resting on the chin or fingers gently\ntouching the cheek, which convey cognitive states like pondering. In this work,\nwe present a novel framework that jointly learns detailed head avatars and the\nnon-rigid deformations induced by hand-face interactions.\n  There are two principal challenges in this task. First, naively tracking hand\nand face separately fails to capture their relative poses. To overcome this, we\npropose to combine depth order loss with contact regularization during pose\ntracking, ensuring correct spatial relationships between the face and hand.\nSecond, no publicly available priors exist for hand-induced deformations,\nmaking them non-trivial to learn from monocular videos. To address this, we\nlearn a PCA basis specific to hand-induced facial deformations from a face-hand\ninteraction dataset. This reduces the problem to estimating a compact set of\nPCA parameters rather than a full spatial deformation field. Furthermore,\ninspired by physics-based simulation, we incorporate a contact loss that\nprovides additional supervision, significantly reducing interpenetration\nartifacts and enhancing the physical plausibility of the results.\n  We evaluate our approach on RGB(D) videos captured by an iPhone.\nAdditionally, to better evaluate the reconstructed geometry, we construct a\nsynthetic dataset of avatars with various types of hand interactions. We show\nthat our method can capture better appearance and more accurate deforming\ngeometry of the face than SOTA surface reconstruction methods.",
      "authors": [
        {
          "name": "Haonan He",
          "affiliation": null
        },
        {
          "name": "Yufeng Zheng",
          "affiliation": null
        },
        {
          "name": "Jie Song",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17181v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17181v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17179v1",
      "title": "Benchmarking Out-of-Distribution Detection for Plankton Recognition: A Systematic Evaluation of Advanced Methods in Marine Ecological Monitoring",
      "abstract": "Automated plankton recognition models face significant challenges during\nreal-world deployment due to distribution shifts (Out-of-Distribution, OoD)\nbetween training and test data. This stems from plankton's complex\nmorphologies, vast species diversity, and the continuous discovery of novel\nspecies, which leads to unpredictable errors during inference. Despite rapid\nadvancements in OoD detection methods in recent years, the field of plankton\nrecognition still lacks a systematic integration of the latest computer vision\ndevelopments and a unified benchmark for large-scale evaluation. To address\nthis, this paper meticulously designed a series of OoD benchmarks simulating\nvarious distribution shift scenarios based on the DYB-PlanktonNet dataset\n\\cite{875n-f104-21}, and systematically evaluated twenty-two OoD detection\nmethods. Extensive experimental results demonstrate that the ViM\n\\cite{wang2022vim} method significantly outperforms other approaches in our\nconstructed benchmarks, particularly excelling in Far-OoD scenarios with\nsubstantial improvements in key metrics. This comprehensive evaluation not only\nprovides a reliable reference for algorithm selection in automated plankton\nrecognition but also lays a solid foundation for future research in plankton\nOoD detection. To our knowledge, this study marks the first large-scale,\nsystematic evaluation and analysis of Out-of-Distribution data detection\nmethods in plankton recognition. Code is available at\nhttps://github.com/BlackJack0083/PlanktonOoD.",
      "authors": [
        {
          "name": "Yingzi Han",
          "affiliation": null
        },
        {
          "name": "Jiakai He",
          "affiliation": null
        },
        {
          "name": "Chuanlong Xie",
          "affiliation": null
        },
        {
          "name": "Jianping Li",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17179v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17179v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17175v1",
      "title": "QR\u00efS: A Preemptive Novel Method for Quishing Detection Through Structural Features of QR",
      "abstract": "Globally, individuals and organizations employ Quick Response (QR) codes for\nswift and convenient communication. Leveraging this, cybercriminals embed\nfalsify and misleading information in QR codes to launch various phishing\nattacks which termed as Quishing. Many former studies have introduced defensive\napproaches to preclude Quishing such as by classifying the embedded content of\nQR codes and then label the QR codes accordingly, whereas other studies\nclassify them using visual features (i.e., deep features, histogram density\nanalysis features). However, these approaches mainly rely on black-box\ntechniques which do not clearly provide interpretability and transparency to\nfully comprehend and reproduce the intrinsic decision process; therefore,\nhaving certain obvious limitations includes the approaches' trust,\naccountability, issues in bias detection, and many more. We proposed QR\\\"iS,\nthe pioneer method to classify QR codes through the comprehensive structural\nanalysis of a QR code which helps to identify phishing QR codes beforehand. Our\nclassification method is clearly transparent which makes it reproducible,\nscalable, and easy to comprehend. First, we generated QR codes dataset (i.e.\n400,000 samples) using recently published URLs datasets [1], [2]. Then, unlike\nblack-box models, we developed a simple algorithm to extract 24 structural\nfeatures from layout patterns present in QR codes. Later, we train the machine\nlearning models on the harvested features and obtained accuracy of up to\n83.18%. To further evaluate the effectiveness of our approach, we perform the\ncomparative analysis of proposed method with relevant contemporary studies.\nLastly, for real-world deployment and validation, we developed a mobile app\nwhich assures the feasibility of the proposed solution in real-world scenarios\nwhich eventually strengthen the applicability of the study.",
      "authors": [
        {
          "name": "Muhammad Wahid Akram",
          "affiliation": null
        },
        {
          "name": "Keshav Sood",
          "affiliation": null
        },
        {
          "name": "Muneeb Ul Hassan",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CR",
        "cs.LG"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17175v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17175v1",
      "primary_category": "cs.CR",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17173v1",
      "title": "Offline Policy Evaluation of Multi-Turn LLM Health Coaching with Real Users",
      "abstract": "We study a web-deployed, tool-augmented LLM health coach with real users. In\na pilot with seven users (280 rated turns), offline policy evaluation (OPE)\nover factorized decision heads (Tool/Style) shows that a uniform heavy-tool\npolicy raises average value on logs but harms specific subgroups, most notably\nlow-health-literacy/high-self-efficacy users. A lightweight simulator with\nhidden archetypes further shows that adding a small early information-gain\nbonus reliably shortens trait identification and improves goal success and\npass@3. Together, these early findings indicate an evaluation-first path to\npersonalization: freeze the generator, learn subgroup-aware decision heads on\ntyped rewards (objective tool outcomes and satisfaction), and always report\nper-archetype metrics to surface subgroup harms that averages obscure.",
      "authors": [
        {
          "name": "Melik Ozolcer",
          "affiliation": null
        },
        {
          "name": "Sang Won Bae",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17173v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17173v1",
      "primary_category": "cs.AI",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17172v1",
      "title": "Combining ECG Foundation Model and XGBoost to Predict In-Hospital Malignant Ventricular Arrhythmias in AMI Patients",
      "abstract": "Malignant ventricular arrhythmias (VT/VF) following acute myocardial\ninfarction (AMI) are a major cause of in-hospital death, yet early\nidentification remains a clinical challenge. While traditional risk scores have\nlimited performance, end-to-end deep learning models often lack the\ninterpretability needed for clinical trust. This study aimed to develop a\nhybrid predictive framework that integrates a large-scale electrocardiogram\n(ECG) foundation model (ECGFounder) with an interpretable XGBoost classifier to\nimprove both accuracy and interpretability. We analyzed 6,634 ECG recordings\nfrom AMI patients, among whom 175 experienced in-hospital VT/VF. The ECGFounder\nmodel was used to extract 150-dimensional diagnostic probability features ,\nwhich were then refined through feature selection to train the XGBoost\nclassifier. Model performance was evaluated using AUC and F1-score , and the\nSHAP method was used for interpretability. The ECGFounder + XGBoost hybrid\nmodel achieved an AUC of 0.801 , outperforming KNN (AUC 0.677), RNN (AUC\n0.676), and an end-to-end 1D-CNN (AUC 0.720). SHAP analysis revealed that\nmodel-identified key features, such as \"premature ventricular complexes\" (risk\npredictor) and \"normal sinus rhythm\" (protective factor), were highly\nconsistent with clinical knowledge. We conclude that this hybrid framework\nprovides a novel paradigm for VT/VF risk prediction by validating the use of\nfoundation model outputs as effective, automated feature engineering for\nbuilding trustworthy, explainable AI-based clinical decision support systems.",
      "authors": [
        {
          "name": "Shun Huang",
          "affiliation": null
        },
        {
          "name": "Wenlu Xing",
          "affiliation": null
        },
        {
          "name": "Shijia Geng",
          "affiliation": null
        },
        {
          "name": "Hailong Wang",
          "affiliation": null
        },
        {
          "name": "Guangkun Nie",
          "affiliation": null
        },
        {
          "name": "Gongzheng Tang",
          "affiliation": null
        },
        {
          "name": "Chenyang He",
          "affiliation": null
        },
        {
          "name": "Shenda Hong",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.AI"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17172v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17172v1",
      "primary_category": "cs.AI",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17171v1",
      "title": "Generation then Reconstruction: Accelerating Masked Autoregressive Models via Two-Stage Sampling",
      "abstract": "Masked Autoregressive (MAR) models promise better efficiency in visual\ngeneration than autoregressive (AR) models for the ability of parallel\ngeneration, yet their acceleration potential remains constrained by the\nmodeling complexity of spatially correlated visual tokens in a single step. To\naddress this limitation, we introduce Generation then Reconstruction (GtR), a\ntraining-free hierarchical sampling strategy that decomposes generation into\ntwo stages: structure generation establishing global semantic scaffolding,\nfollowed by detail reconstruction efficiently completing remaining tokens.\nAssuming that it is more difficult to create an image from scratch than to\ncomplement images based on a basic image framework, GtR is designed to achieve\nacceleration by computing the reconstruction stage quickly while maintaining\nthe generation quality by computing the generation stage slowly. Moreover,\nobserving that tokens on the details of an image often carry more semantic\ninformation than tokens in the salient regions, we further propose\nFrequency-Weighted Token Selection (FTS) to offer more computation budget to\ntokens on image details, which are localized based on the energy of high\nfrequency information. Extensive experiments on ImageNet class-conditional and\ntext-to-image generation demonstrate 3.72x speedup on MAR-H while maintaining\ncomparable quality (e.g., FID: 1.59, IS: 304.4 vs. original 1.59, 299.1),\nsubstantially outperforming existing acceleration methods across various model\nscales and generation tasks. Our codes will be released in\nhttps://github.com/feihongyan1/GtR.",
      "authors": [
        {
          "name": "Feihong Yan",
          "affiliation": null
        },
        {
          "name": "Peiru Wang",
          "affiliation": null
        },
        {
          "name": "Yao Zhu",
          "affiliation": null
        },
        {
          "name": "Kaiyu Pang",
          "affiliation": null
        },
        {
          "name": "Qingyan Wei",
          "affiliation": null
        },
        {
          "name": "Huiqi Li",
          "affiliation": null
        },
        {
          "name": "Linfeng Zhang",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17171v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17171v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17169v1",
      "title": "Investigating Adversarial Robustness against Preprocessing used in Blackbox Face Recognition",
      "abstract": "Face Recognition (FR) models have been shown to be vulnerable to adversarial\nexamples that subtly alter benign facial images, exposing blind spots in these\nsystems, as well as protecting user privacy. End-to-end FR systems first obtain\npreprocessed faces from diverse facial imagery prior to computing the\nsimilarity of the deep feature embeddings. Whilst face preprocessing is a\ncritical component of FR systems, and hence adversarial attacks against them,\nwe observe that this preprocessing is often overlooked in blackbox settings.\nOur study seeks to investigate the transferability of several out-of-the-box\nstate-of-the-art adversarial attacks against FR when applied against different\npreprocessing techniques used in a blackbox setting. We observe that the choice\nof face detection model can degrade the attack success rate by up to 78%,\nwhereas choice of interpolation method during downsampling has relatively\nminimal impacts. Furthermore, we find that the requirement for facial\npreprocessing even degrades attack strength in a whitebox setting, due to the\nunintended interaction of produced noise vectors against face detection models.\nBased on these findings, we propose a preprocessing-invariant method using\ninput transformations that improves the transferability of the studied attacks\nby up to 27%. Our findings highlight the importance of preprocessing in FR\nsystems, and the need for its consideration towards improving the adversarial\ngeneralisation of facial adversarial examples.",
      "authors": [
        {
          "name": "Roland Croft",
          "affiliation": null
        },
        {
          "name": "Brian Du",
          "affiliation": null
        },
        {
          "name": "Darcy Joseph",
          "affiliation": null
        },
        {
          "name": "Sharath Kumar",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17169v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17169v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17168v1",
      "title": "When AI companions become witty: Can human brain recognize AI-generated irony?",
      "abstract": "As Large Language Models (LLMs) are increasingly deployed as social agents\nand trained to produce humor and irony, a question emerges: when encountering\nwitty AI remarks, do people interpret these as intentional communication or\nmere computational output? This study investigates whether people adopt the\nintentional stance, attributing mental states to explain behavior,toward AI\nduring irony comprehension. Irony provides an ideal paradigm because it\nrequires distinguishing intentional contradictions from unintended errors\nthrough effortful semantic reanalysis. We compared behavioral and neural\nresponses to ironic statements from AI versus human sources using established\nERP components: P200 reflecting early incongruity detection and P600 indexing\ncognitive efforts in reinterpreting incongruity as deliberate irony. Results\ndemonstrate that people do not fully adopt the intentional stance toward\nAI-generated irony. Behaviorally, participants attributed incongruity to\ndeliberate communication for both sources, though significantly less for AI\nthan human, showing greater tendency to interpret AI incongruities as\ncomputational errors. Neural data revealed attenuated P200 and P600 effects for\nAI-generated irony, suggesting reduced effortful detection and reanalysis\nconsistent with diminished attribution of communicative intent. Notably, people\nwho perceived AI as more sincere showed larger P200 and P600 effects for\nAI-generated irony, suggesting that intentional stance adoption is calibrated\nby specific mental models of artificial agents. These findings reveal that\nsource attribution shapes neural processing of social-communicative phenomena.\nDespite current LLMs' linguistic sophistication, achieving genuine social\nagency requires more than linguistic competence, it necessitates a shift in how\nhumans perceive and attribute intentionality to artificial agents.",
      "authors": [
        {
          "name": "Xiaohui Rao",
          "affiliation": null
        },
        {
          "name": "Hanlin Wu",
          "affiliation": null
        },
        {
          "name": "Zhenguang G. Cai",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17168v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17168v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17167v1",
      "title": "Discovering Causal Relationships using Proxy Variables under Unmeasured Confounding",
      "abstract": "Inferring causal relationships between variable pairs in the observational\nstudy is crucial but challenging, due to the presence of unmeasured\nconfounding. While previous methods employed the negative controls to adjust\nfor the confounding bias, they were either restricted to the discrete setting\n(i.e., all variables are discrete) or relied on strong assumptions for\nidentification. To address these problems, we develop a general nonparametric\napproach that accommodates both discrete and continuous settings for testing\ncausal hypothesis under unmeasured confounders. By using only a single negative\ncontrol outcome (NCO), we establish a new identification result based on a\nnewly proposed integral equation that links the outcome and NCO, requiring only\nthe completeness and mild regularity conditions. We then propose a kernel-based\ntesting procedure that is more efficient than existing moment-restriction\nmethods. We derive the asymptotic level and power properties for our tests.\nFurthermore, we examine cases where our procedure using only NCO fails to\nachieve identification, and introduce a new procedure that incorporates a\nnegative control exposure (NCE) to restore identifiability. We demonstrate the\neffectiveness of our approach through extensive simulations and real-world data\nfrom the Intensive Care Data and World Values Survey.",
      "authors": [
        {
          "name": "Yong Wu",
          "affiliation": null
        },
        {
          "name": "Yanwei Fu",
          "affiliation": null
        },
        {
          "name": "Shouyan Wang",
          "affiliation": null
        },
        {
          "name": "Yizhou Wang",
          "affiliation": null
        },
        {
          "name": "Xinwei Sun",
          "affiliation": null
        }
      ],
      "categories": [
        "stat.ME",
        "math.ST",
        "stat.ML",
        "stat.TH"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17167v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17167v1",
      "primary_category": "stat.ME",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17163v1",
      "title": "TREAT: A Code LLMs Trustworthiness / Reliability Evaluation and Testing Framework",
      "abstract": "Large foundation models are fundamentally transforming the software\nengineering landscape, demonstrating exceptional capabilities across diverse\ntasks such as code generation, debugging, and testing. Despite this rapid\nprogress, a significant gap remains in how to comprehensively evaluate these\nmodels' trustworthiness in real-world software engineering scenarios. Existing\nbenchmarks suffer from limited task scope and fail to incorporate critical\nevaluation aspects such as the robustness and reliability of models. To bridge\nthis gap, we present an evaluation framework called TREAT (Code LLMs\nTrustworthiness / Reliability Evaluation And Testing) that provides a holistic\nassessment of model performance in code intelligence tasks. Our evaluation\nframework addresses key limitations in existing approaches with four main\nimprovements: (1) Multi-Task Holistic Evaluation that spans diverse software\nengineering activities rather than limited coding tasks; (2) Multi-Language and\nMulti-Modality Assessment that extends beyond traditional single-language,\ntext-only benchmarks to include multi-modality coding tasks; (3) Robustness\nAssessment that evaluates model reliability under semantically-preserving code\ntransformations; and (4) Rigorous Evaluation Methodology that enhances the\ntrustworthiness of evaluation results through diverse evaluation prompts and\nadaptive solution extraction. Based on this evaluation framework, we assess 26\nstate-of-the-art models and uncover both their strengths and limitations,\nyielding several key insights:(1) Current models show substantial performance\nvariation across programming tasks; (2) Multi-modal language models demonstrate\nspecific performance limitations in UI code generation and edit;",
      "authors": [
        {
          "name": "Shuzheng Gao",
          "affiliation": null
        },
        {
          "name": "Eric John Li",
          "affiliation": null
        },
        {
          "name": "Man Ho Lam",
          "affiliation": null
        },
        {
          "name": "Jingyu Xiao",
          "affiliation": null
        },
        {
          "name": "Yuxuan Wan",
          "affiliation": null
        },
        {
          "name": "Chaozheng Wang",
          "affiliation": null
        },
        {
          "name": "Ng Man Tik",
          "affiliation": null
        },
        {
          "name": "Michael R. Lyu",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17163v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17163v1",
      "primary_category": "cs.SE",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17162v1",
      "title": "ALPINE: A Lightweight and Adaptive Privacy-Decision Agent Framework for Dynamic Edge Crowdsensing",
      "abstract": "Mobile edge crowdsensing (MECS) systems continuously generate and transmit\nuser data in dynamic, resource-constrained environments, exposing users to\nsignificant privacy threats. In practice, many privacy-preserving mechanisms\nbuild on differential privacy (DP). However, static DP mechanisms often fail to\nadapt to evolving risks, for example, shifts in adversarial capabilities,\nresource constraints and task requirements, resulting in either excessive noise\nor inadequate protection. To address this challenge, we propose ALPINE, a\nlightweight, adaptive framework that empowers terminal devices to autonomously\nadjust differential privacy levels in real time. ALPINE operates as a\nclosed-loop control system consisting of four modules: dynamic risk perception,\nprivacy decision via twin delayed deep deterministic policy gradient (TD3),\nlocal privacy execution and performance verification from edge nodes. Based on\nenvironmental risk assessments, we design a reward function that balances\nprivacy gains, data utility and energy cost, guiding the TD3 agent to\nadaptively tune noise magnitude across diverse risk scenarios and achieve a\ndynamic equilibrium among privacy, utility and cost. Both the collaborative\nrisk model and pretrained TD3-based agent are designed for low-overhead\ndeployment. Extensive theoretical analysis and real-world simulations\ndemonstrate that ALPINE effectively mitigates inference attacks while\npreserving utility and cost, making it practical for large-scale edge\napplications.",
      "authors": [
        {
          "name": "Guanjie Cheng",
          "affiliation": null
        },
        {
          "name": "Siyang Liu",
          "affiliation": null
        },
        {
          "name": "Junqin Huang",
          "affiliation": null
        },
        {
          "name": "Xinkui Zhao",
          "affiliation": null
        },
        {
          "name": "Yin Wang",
          "affiliation": null
        },
        {
          "name": "Mengying Zhu",
          "affiliation": null
        },
        {
          "name": "Linghe Kong",
          "affiliation": null
        },
        {
          "name": "Shuiguang Deng",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17162v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17162v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17160v1",
      "title": "Learning After Model Deployment",
      "abstract": "In classic supervised learning, once a model is deployed in an application,\nit is fixed. No updates will be made to it during the application. This is\ninappropriate for many dynamic and open environments, where unexpected samples\nfrom unseen classes may appear. In such an environment, the model should be\nable to detect these novel samples from unseen classes and learn them after\nthey are labeled. We call this paradigm Autonomous Learning after Model\nDeployment (ALMD). The learning here is continuous and involves no human\nengineers. Labeling in this scenario is performed by human co-workers or other\nknowledgeable agents, which is similar to what humans do when they encounter an\nunfamiliar object and ask another person for its name. In ALMD, the detection\nof novel samples is dynamic and differs from traditional out-of-distribution\n(OOD) detection in that the set of in-distribution (ID) classes expands as new\nclasses are learned during application, whereas ID classes is fixed in\ntraditional OOD detection. Learning is also different from classic supervised\nlearning because in ALMD, we learn the encountered new classes immediately and\nincrementally. It is difficult to retrain the model from scratch using all the\npast data from the ID classes and the novel samples from newly discovered\nclasses, as this would be resource- and time-consuming. Apart from these two\nchallenges, ALMD faces the data scarcity issue because instances of new classes\noften appear sporadically in real-life applications. To address these issues,\nwe propose a novel method, PLDA, which performs dynamic OOD detection and\nincremental learning of new classes on the fly. Empirical evaluations will\ndemonstrate the effectiveness of PLDA.",
      "authors": [
        {
          "name": "Derda Kaymak",
          "affiliation": null
        },
        {
          "name": "Gyuhak Kim",
          "affiliation": null
        },
        {
          "name": "Tomoya Kaichi",
          "affiliation": null
        },
        {
          "name": "Tatsuya Konishi",
          "affiliation": null
        },
        {
          "name": "Bing Liu",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17160v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17160v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17157v1",
      "title": "GACO-CAD: Geometry-Augmented and Conciseness-Optimized CAD Model Generation from Single Image",
      "abstract": "Generating editable, parametric CAD models from a single image holds great\npotential to lower the barriers of industrial concept design. However, current\nmulti-modal large language models (MLLMs) still struggle with accurately\ninferring 3D geometry from 2D images due to limited spatial reasoning\ncapabilities. We address this limitation by introducing GACO-CAD, a novel\ntwo-stage post-training framework. It is designed to achieve a joint objective:\nsimultaneously improving the geometric accuracy of the generated CAD models and\nencouraging the use of more concise modeling procedures. First, during\nsupervised fine-tuning, we leverage depth and surface normal maps as dense\ngeometric priors, combining them with the RGB image to form a multi-channel\ninput. In the context of single-view reconstruction, these priors provide\ncomplementary spatial cues that help the MLLM more reliably recover 3D geometry\nfrom 2D observations. Second, during reinforcement learning, we introduce a\ngroup length reward that, while preserving high geometric fidelity, promotes\nthe generation of more compact and less redundant parametric modeling\nsequences. A simple dynamic weighting strategy is adopted to stabilize\ntraining. Experiments on the DeepCAD and Fusion360 datasets show that GACO-CAD\nachieves state-of-the-art performance under the same MLLM backbone,\nconsistently outperforming existing methods in terms of code validity,\ngeometric accuracy, and modeling conciseness.",
      "authors": [
        {
          "name": "Yinghui Wang",
          "affiliation": null
        },
        {
          "name": "Xinyu Zhang",
          "affiliation": null
        },
        {
          "name": "Peng Du",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17157v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17157v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17153v1",
      "title": "HyperSearch: Prediction of New Hyperedges through Unconstrained yet Efficient Search",
      "abstract": "Higher-order interactions (HOIs) in complex systems, such as scientific\ncollaborations, multi-protein complexes, and multi-user communications, are\ncommonly modeled as hypergraphs, where each hyperedge (i.e., a subset of nodes)\nrepresents an HOI among the nodes. Given a hypergraph, hyperedge prediction\naims to identify hyperedges that are either missing or likely to form in the\nfuture, and it has broad applications, including recommending interest-based\nsocial groups, predicting collaborations, and uncovering functional complexes\nin biological systems. However, the vast search space of hyperedge candidates\n(i.e., all possible subsets of nodes) poses a significant computational\nchallenge, making naive exhaustive search infeasible. As a result, existing\napproaches rely on either heuristic sampling to obtain constrained candidate\nsets or ungrounded assumptions on hypergraph structure to select promising\nhyperedges.\n  In this work, we propose HyperSearch, a search-based algorithm for hyperedge\nprediction that efficiently evaluates unconstrained candidate sets, by\nincorporating two key components: (1) an empirically grounded scoring function\nderived from observations in real-world hypergraphs and (2) an efficient search\nmechanism, where we derive and use an anti-monotonic upper bound of the\noriginal scoring function (which is not antimonotonic) to prune the search\nspace. This pruning comes with theoretical guarantees, ensuring that discarded\ncandidates are never better than the kept ones w.r.t. the original scoring\nfunction. In extensive experiments on 10 real-world hypergraphs across five\ndomains, HyperSearch consistently outperforms state-of-the-art baselines,\nachieving higher accuracy in predicting new (i.e., not in the training set)\nhyperedges.",
      "authors": [
        {
          "name": "Hyunjin Choo",
          "affiliation": null
        },
        {
          "name": "Fanchen Bu",
          "affiliation": null
        },
        {
          "name": "Hyunjin Hwang",
          "affiliation": null
        },
        {
          "name": "Young-Gyu Yoon",
          "affiliation": null
        },
        {
          "name": "Kijung Shin",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.SI",
        "cs.LG"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17153v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17153v1",
      "primary_category": "cs.SI",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17149v1",
      "title": "Which LLM Multi-Agent Protocol to Choose?",
      "abstract": "As large-scale multi-agent systems evolve, the communication protocol layer\nhas become a critical yet under-evaluated factor shaping performance and\nreliability. Despite the existence of diverse protocols (A2A, ACP, ANP, Agora,\netc.), selection is often intuition-driven and lacks standardized guidance. We\nintroduce ProtocolBench, a benchmark that systematically compares agent\nprotocols along four measurable axes: task success, end-to-end latency, message\nor byte overhead, and robustness under failures. On ProtocolBench, protocol\nchoice significantly influences system behavior. In the Streaming Queue\nscenario, overall completion time varies by up to 36.5% across protocols, and\nmean end-to-end latency differs by 3.48 s. Under Fail-Storm Recovery,\nresilience also differs consistently across protocols. Beyond evaluation, we\npresent ProtocolRouter, a learnable protocol router that selects per-scenario\n(or per-module) protocols from requirement and runtime signals. ProtocolRouter\nreduces Fail-Storm recovery time by up to 18.1% versus the best single-protocol\nbaseline, and achieves scenario-specific gains such as higher success in GAIA.\nWe also release ProtocolRouterBench to standardize protocol evaluation and\nimprove reliability at scale.",
      "authors": [
        {
          "name": "Hongyi Du",
          "affiliation": null
        },
        {
          "name": "Jiaqi Su",
          "affiliation": null
        },
        {
          "name": "Jisen Li",
          "affiliation": null
        },
        {
          "name": "Lijie Ding",
          "affiliation": null
        },
        {
          "name": "Yingxuan Yang",
          "affiliation": null
        },
        {
          "name": "Peixuan Han",
          "affiliation": null
        },
        {
          "name": "Xiangru Tang",
          "affiliation": null
        },
        {
          "name": "Kunlun Zhu",
          "affiliation": null
        },
        {
          "name": "Jiaxuan You",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.AI",
        "I.2.11"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17149v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17149v1",
      "primary_category": "cs.AI",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17148v1",
      "title": "DiffVLA++: Bridging Cognitive Reasoning and End-to-End Driving through Metric-Guided Alignment",
      "abstract": "Conventional end-to-end (E2E) driving models are effective at generating\nphysically plausible trajectories, but often fail to generalize to long-tail\nscenarios due to the lack of essential world knowledge to understand and reason\nabout surrounding environments. In contrast, Vision-Language-Action (VLA)\nmodels leverage world knowledge to handle challenging cases, but their limited\n3D reasoning capability can lead to physically infeasible actions. In this work\nwe introduce DiffVLA++, an enhanced autonomous driving framework that\nexplicitly bridges cognitive reasoning and E2E planning through metric-guided\nalignment. First, we build a VLA module directly generating semantically\ngrounded driving trajectories. Second, we design an E2E module with a dense\ntrajectory vocabulary that ensures physical feasibility. Third, and most\ncritically, we introduce a metric-guided trajectory scorer that guides and\naligns the outputs of the VLA and E2E modules, thereby integrating their\ncomplementary strengths. The experiment on the ICCV 2025 Autonomous Grand\nChallenge leaderboard shows that DiffVLA++ achieves EPDMS of 49.12.",
      "authors": [
        {
          "name": "Yu Gao",
          "affiliation": null
        },
        {
          "name": "Yiru Wang",
          "affiliation": null
        },
        {
          "name": "Anqing Jiang",
          "affiliation": null
        },
        {
          "name": "Heng Yuwen",
          "affiliation": null
        },
        {
          "name": "Wang Shuo",
          "affiliation": null
        },
        {
          "name": "Sun Hao",
          "affiliation": null
        },
        {
          "name": "Wang Jijun",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17148v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17148v1",
      "primary_category": "cs.RO",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17146v1",
      "title": "Physics-Informed Large Language Models for HVAC Anomaly Detection with Autonomous Rule Generation",
      "abstract": "Heating, Ventilation, and Air-Conditioning (HVAC) systems account for a\nsubstantial share of global building energy use, making reliable anomaly\ndetection essential for improving efficiency and reducing emissions. Classical\nrule-based approaches offer explainability but lack adaptability, while deep\nlearning methods provide predictive power at the cost of transparency,\nefficiency, and physical plausibility. Recent attempts to use Large Language\nModels (LLMs) for anomaly detection improve interpretability but largely ignore\nthe physical principles that govern HVAC operations. We present PILLM, a\nPhysics-Informed LLM framework that operates within an evolutionary loop to\nautomatically generate, evaluate, and refine anomaly detection rules. Our\napproach introduces physics-informed reflection and crossover operators that\nembed thermodynamic and control-theoretic constraints, enabling rules that are\nboth adaptive and physically grounded. Experiments on the public Building Fault\nDetection dataset show that PILLM achieves state-of-the-art performance while\nproducing diagnostic rules that are interpretable and actionable, advancing\ntrustworthy and deployable AI for smart building systems.",
      "authors": [
        {
          "name": "Subin Lin",
          "affiliation": null
        },
        {
          "name": "Chuanbo Hua",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.AI",
        "cs.CE"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17146v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17146v1",
      "primary_category": "cs.AI",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17145v1",
      "title": "Enhanced Fish Freshness Classification with Incremental Handcrafted Feature Fusion",
      "abstract": "Accurate assessment of fish freshness remains a major challenge in the food\nindustry, with direct consequences for product quality, market value, and\nconsumer health. Conventional sensory evaluation is inherently subjective,\ninconsistent, and difficult to standardize across contexts, often limited by\nsubtle, species-dependent spoilage cues. To address these limitations, we\npropose a handcrafted feature-based approach that systematically extracts and\nincrementally fuses complementary descriptors, including color statistics,\nhistograms across multiple color spaces, and texture features such as Local\nBinary Patterns (LBP) and Gray-Level Co-occurrence Matrices (GLCM), from fish\neye images. Our method captures global chromatic variations from full images\nand localized degradations from ROI segments, fusing each independently to\nevaluate their effectiveness in assessing freshness. Experiments on the\nFreshness of the Fish Eyes (FFE) dataset demonstrate the approach's\neffectiveness: in a standard train-test setting, a LightGBM classifier achieved\n77.56% accuracy, a 14.35% improvement over the previous deep learning baseline\nof 63.21%. With augmented data, an Artificial Neural Network (ANN) reached\n97.16% accuracy, surpassing the prior best of 77.3% by 19.86%. These results\ndemonstrate that carefully engineered, handcrafted features, when strategically\nprocessed, yield a robust, interpretable, and reliable solution for automated\nfish freshness assessment, providing valuable insights for practical\napplications in food quality monitoring.",
      "authors": [
        {
          "name": "Phi-Hung Hoang",
          "affiliation": null
        },
        {
          "name": "Nam-Thuan Trinh",
          "affiliation": null
        },
        {
          "name": "Van-Manh Tran",
          "affiliation": null
        },
        {
          "name": "Thi-Thu-Hong Phan",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.AI",
        "68T05, 62H30"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17145v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17145v1",
      "primary_category": "cs.AI",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17139v1",
      "title": "Rethinking On-policy Optimization for Query Augmentation",
      "abstract": "Recent advances in large language models (LLMs) have led to a surge of\ninterest in query augmentation for information retrieval (IR). Two main\napproaches have emerged. The first prompts LLMs to generate answers or\npseudo-documents that serve as new queries, relying purely on the model's\nparametric knowledge or contextual information. The second applies\nreinforcement learning (RL) to fine-tune LLMs for query rewriting, directly\noptimizing retrieval metrics. While having respective advantages and\nlimitations, the two approaches have not been compared under consistent\nexperimental conditions. In this work, we present the first systematic\ncomparison of prompting-based and RL-based query augmentation across diverse\nbenchmarks, including evidence-seeking, ad hoc, and tool retrieval. Our key\nfinding is that simple, training-free query augmentation often performs on par\nwith, or even surpasses, more expensive RL-based counterparts, especially when\nusing powerful LLMs. Motivated by this discovery, we introduce a novel hybrid\nmethod, On-policy Pseudo-document Query Expansion (OPQE), which, instead of\nrewriting a query, the LLM policy learns to generate a pseudo-document that\nmaximizes retrieval performance, thus merging the flexibility and generative\nstructure of prompting with the targeted optimization of RL. We show OPQE\noutperforms both standalone prompting and RL-based rewriting, demonstrating\nthat a synergistic approach yields the best results. Our implementation is made\navailable to facilitate reproducibility.",
      "authors": [
        {
          "name": "Zhichao Xu",
          "affiliation": null
        },
        {
          "name": "Shengyao Zhuang",
          "affiliation": null
        },
        {
          "name": "Xueguang Ma",
          "affiliation": null
        },
        {
          "name": "Bingsen Chen",
          "affiliation": null
        },
        {
          "name": "Yijun Tian",
          "affiliation": null
        },
        {
          "name": "Fengran Mo",
          "affiliation": null
        },
        {
          "name": "Jie Cao",
          "affiliation": null
        },
        {
          "name": "Vivek Srikumar",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL",
        "cs.IR"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17139v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17139v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17137v1",
      "title": "KineDiff3D: Kinematic-Aware Diffusion for Category-Level Articulated Object Shape Reconstruction and Generation",
      "abstract": "Articulated objects, such as laptops and drawers, exhibit significant\nchallenges for 3D reconstruction and pose estimation due to their multi-part\ngeometries and variable joint configurations, which introduce structural\ndiversity across different states. To address these challenges, we propose\nKineDiff3D: Kinematic-Aware Diffusion for Category-Level Articulated Object\nShape Reconstruction and Generation, a unified framework for reconstructing\ndiverse articulated instances and pose estimation from single view input.\nSpecifically, we first encode complete geometry (SDFs), joint angles, and part\nsegmentation into a structured latent space via a novel Kinematic-Aware VAE\n(KA-VAE). In addition, we employ two conditional diffusion models: one for\nregressing global pose (SE(3)) and joint parameters, and another for generating\nthe kinematic-aware latent code from partial observations. Finally, we produce\nan iterative optimization module that bidirectionally refines reconstruction\naccuracy and kinematic parameters via Chamfer-distance minimization while\npreserving articulation constraints. Experimental results on synthetic,\nsemi-synthetic, and real-world datasets demonstrate the effectiveness of our\napproach in accurately reconstructing articulated objects and estimating their\nkinematic properties.",
      "authors": [
        {
          "name": "WenBo Xu",
          "affiliation": null
        },
        {
          "name": "Liu Liu",
          "affiliation": null
        },
        {
          "name": "Li Zhang",
          "affiliation": null
        },
        {
          "name": "Ran Zhang",
          "affiliation": null
        },
        {
          "name": "Hao Wu",
          "affiliation": null
        },
        {
          "name": "Dan Guo",
          "affiliation": null
        },
        {
          "name": "Meng Wang",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17137v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17137v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17136v1",
      "title": "In-situ Autoguidance: Eliciting Self-Correction in Diffusion Models",
      "abstract": "The generation of high-quality, diverse, and prompt-aligned images is a\ncentral goal in image-generating diffusion models. The popular classifier-free\nguidance (CFG) approach improves quality and alignment at the cost of reduced\nvariation, creating an inherent entanglement of these effects. Recent work has\nsuccessfully disentangled these properties by guiding a model with a separately\ntrained, inferior counterpart; however, this solution introduces the\nconsiderable overhead of requiring an auxiliary model. We challenge this\nprerequisite by introducing In-situ Autoguidance, a method that elicits\nguidance from the model itself without any auxiliary components. Our approach\ndynamically generates an inferior prediction on the fly using a stochastic\nforward pass, reframing guidance as a form of inference-time self-correction.\nWe demonstrate that this zero-cost approach is not only viable but also\nestablishes a powerful new baseline for cost-efficient guidance, proving that\nthe benefits of self-guidance can be achieved without external models.",
      "authors": [
        {
          "name": "Enhao Gu",
          "affiliation": null
        },
        {
          "name": "Haolin Hou",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "I.2.6; I.5.1; I.5.4"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17136v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17136v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17132v1",
      "title": "Do LLMs Recognize Your Latent Preferences? A Benchmark for Latent Information Discovery in Personalized Interaction",
      "abstract": "Large Language Models (LLMs) excel at producing broadly relevant text, but\nthis generality becomes a limitation when user-specific preferences are\nrequired, such as recommending restaurants or planning travel. In these\nscenarios, users rarely articulate every preference explicitly; instead, much\nof what they care about remains latent, waiting to be inferred. This raises a\nfundamental question: Can LLMs uncover and reason about such latent information\nthrough conversation?\n  We address this problem by introducing a unified benchmark for evaluating\nlatent information discovery - the ability of LLMs to reveal and utilize hidden\nuser attributes through multi-turn interaction. The benchmark spans three\nprogressively realistic settings: the classic 20 Questions game, Personalized\nQuestion Answering, and Personalized Text Summarization. All tasks share a\ntri-agent framework (User, Assistant, Judge) enabling turn-level evaluation of\nelicitation and adaptation. Our results reveal that while LLMs can indeed\nsurface latent information through dialogue, their success varies dramatically\nwith context: from 32% to 98%, depending on task complexity, topic, and number\nof hidden attributes. This benchmark provides the first systematic framework\nfor studying latent information discovery in personalized interaction,\nhighlighting that effective preference inference remains an open frontier for\nbuilding truly adaptive AI systems.",
      "authors": [
        {
          "name": "Ioannis Tsaknakis",
          "affiliation": null
        },
        {
          "name": "Bingqing Song",
          "affiliation": null
        },
        {
          "name": "Shuyu Gan",
          "affiliation": null
        },
        {
          "name": "Dongyeop Kang",
          "affiliation": null
        },
        {
          "name": "Alfredo Garcia",
          "affiliation": null
        },
        {
          "name": "Gaowen Liu",
          "affiliation": null
        },
        {
          "name": "Charles Fleming",
          "affiliation": null
        },
        {
          "name": "Mingyi Hong",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17132v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17132v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17131v1",
      "title": "GOOD: Training-Free Guided Diffusion Sampling for Out-of-Distribution Detection",
      "abstract": "Recent advancements have explored text-to-image diffusion models for\nsynthesizing out-of-distribution (OOD) samples, substantially enhancing the\nperformance of OOD detection. However, existing approaches typically rely on\nperturbing text-conditioned embeddings, resulting in semantic instability and\ninsufficient shift diversity, which limit generalization to realistic OOD. To\naddress these challenges, we propose GOOD, a novel and flexible framework that\ndirectly guides diffusion sampling trajectories towards OOD regions using\noff-the-shelf in-distribution (ID) classifiers. GOOD incorporates dual-level\nguidance: (1) Image-level guidance based on the gradient of log partition to\nreduce input likelihood, drives samples toward low-density regions in pixel\nspace. (2) Feature-level guidance, derived from k-NN distance in the\nclassifier's latent space, promotes sampling in feature-sparse regions. Hence,\nthis dual-guidance design enables more controllable and diverse OOD sample\ngeneration. Additionally, we introduce a unified OOD score that adaptively\ncombines image and feature discrepancies, enhancing detection robustness. We\nperform thorough quantitative and qualitative analyses to evaluate the\neffectiveness of GOOD, demonstrating that training with samples generated by\nGOOD can notably enhance OOD detection performance.",
      "authors": [
        {
          "name": "Xin Gao",
          "affiliation": null
        },
        {
          "name": "Jiyao Liu",
          "affiliation": null
        },
        {
          "name": "Guanghao Li",
          "affiliation": null
        },
        {
          "name": "Yueming Lyu",
          "affiliation": null
        },
        {
          "name": "Jianxiong Gao",
          "affiliation": null
        },
        {
          "name": "Weichen Yu",
          "affiliation": null
        },
        {
          "name": "Ningsheng Xu",
          "affiliation": null
        },
        {
          "name": "Liang Wang",
          "affiliation": null
        },
        {
          "name": "Caifeng Shan",
          "affiliation": null
        },
        {
          "name": "Ziwei Liu",
          "affiliation": null
        },
        {
          "name": "Chenyang Si",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17131v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17131v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17122v1",
      "title": "Continuous Q-Score Matching: Diffusion Guided Reinforcement Learning for Continuous-Time Control",
      "abstract": "Reinforcement learning (RL) has achieved significant success across a wide\nrange of domains, however, most existing methods are formulated in discrete\ntime. In this work, we introduce a novel RL method for continuous-time control,\nwhere stochastic differential equations govern state-action dynamics. Departing\nfrom traditional value function-based approaches, our key contribution is the\ncharacterization of continuous-time Q-functions via a martingale condition and\nthe linking of diffusion policy scores to the action gradient of a learned\ncontinuous Q-function by the dynamic programming principle. This insight\nmotivates Continuous Q-Score Matching (CQSM), a score-based policy improvement\nalgorithm. Notably, our method addresses a long-standing challenge in\ncontinuous-time RL: preserving the action-evaluation capability of Q-functions\nwithout relying on time discretization. We further provide theoretical\nclosed-form solutions for linear-quadratic (LQ) control problems within our\nframework. Numerical results in simulated environments demonstrate the\neffectiveness of our proposed method and compare it to popular baselines.",
      "authors": [
        {
          "name": "Chengxiu Hua",
          "affiliation": null
        },
        {
          "name": "Jiawen Gu",
          "affiliation": null
        },
        {
          "name": "Yushun Tang",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "math.OC"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17122v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17122v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17120v1",
      "title": "Matricial Free Energy as a Gaussianizing Regularizer: Enhancing Autoencoders for Gaussian Code Generation",
      "abstract": "We introduce a novel regularization scheme for autoencoders based on\nmatricial free energy. Our approach defines a differentiable loss function in\nterms of the singular values of the code matrix (code dimension x batch size).\nFrom the standpoint of free probability an d random matrix theory, this loss\nachieves its minimum when the singular value distribution of the code matrix\ncoincides with that of an appropriately sculpted random metric with i.i.d.\nGaussian entries. Empirical simulations demonstrate that minimizing the\nnegative matricial free energy through standard stochastic gradient-based\ntraining yields Gaussian-like codes that generalize across training and test\nsets. Building on this foundation, we propose a matricidal free energy\nmaximizing autoencoder that reliably produces Gaussian codes and show its\napplication to underdetermined inverse problems.",
      "authors": [
        {
          "name": "Rishi Sonthalia",
          "affiliation": null
        },
        {
          "name": "Raj Rao Nadakuditi",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cs.CV",
        "stat.ML"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17120v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17120v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17115v1",
      "title": "DVAGen: Dynamic Vocabulary Augmented Generation",
      "abstract": "Language models trained with a fixed vocabulary struggle to generalize to\nnovel or out-of-vocabulary words, limiting their flexibility in handling\ndiverse token combinations. Existing dynamic vocabulary approaches attempt to\naddress this limitation but face challenges such as fragmented codebases, lack\nof support for modern LLMs, and limited inference scalability. To overcome\nthese issues, we introduce DVAGen, a fully open-source, unified framework\ndesigned for training, evaluation, and visualization of dynamic\nvocabulary-augmented language models. Our framework modularizes the pipeline\nfor ease of customization, integrates seamlessly with open-source LLMs, and is\nthe first to provide both CLI and WebUI tools for real-time result inspection.\nWe validate the effectiveness of dynamic vocabulary methods on modern LLMs and\ndemonstrate support for batch inference, significantly improving inference\nthroughput.",
      "authors": [
        {
          "name": "Wei Du",
          "affiliation": null
        },
        {
          "name": "Nuowei Liu",
          "affiliation": null
        },
        {
          "name": "Jie Wang",
          "affiliation": null
        },
        {
          "name": "Jiahao Kuang",
          "affiliation": null
        },
        {
          "name": "Tao Ji",
          "affiliation": null
        },
        {
          "name": "Xiaoling Wang",
          "affiliation": null
        },
        {
          "name": "Yuanbin Wu",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17115v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17115v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17114v1",
      "title": "Towards Imperceptible Watermarking Via Environment Illumination for Consumer Cameras",
      "abstract": "This paper introduces a method for using LED-based environmental lighting to\nproduce visually imperceptible watermarks for consumer cameras. Our approach\noptimizes an LED light source's spectral profile to be minimally visible to the\nhuman eye while remaining highly detectable by typical consumer cameras. The\nmethod jointly considers the human visual system's sensitivity to visible\nspectra, modern consumer camera sensors' spectral sensitivity, and narrowband\nLEDs' ability to generate broadband spectra perceived as \"white light\"\n(specifically, D65 illumination). To ensure imperceptibility, we employ\nspectral modulation rather than intensity modulation. Unlike conventional\nvisible light communication, our approach enables watermark extraction at\nstandard low frame rates (30-60 fps). While the information transfer rate is\nmodest-embedding 128 bits within a 10-second video clip-this capacity is\nsufficient for essential metadata supporting privacy protection and content\nverification.",
      "authors": [
        {
          "name": "Hodaka Kawachi",
          "affiliation": null
        },
        {
          "name": "Tomoya Nakamura",
          "affiliation": null
        },
        {
          "name": "Hiroaki Santo",
          "affiliation": null
        },
        {
          "name": "SaiKiran Kumar Tedla",
          "affiliation": null
        },
        {
          "name": "Trevor Dalton Canham",
          "affiliation": null
        },
        {
          "name": "Yasushi Yagi",
          "affiliation": null
        },
        {
          "name": "Michael S. Brown",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17114v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17114v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17111v1",
      "title": "Efficient Vision-Language-Action Models for Embodied Manipulation: A Systematic Survey",
      "abstract": "Vision-Language-Action (VLA) models extend vision-language models to embodied\ncontrol by mapping natural-language instructions and visual observations to\nrobot actions. Despite their capabilities, VLA systems face significant\nchallenges due to their massive computational and memory demands, which\nconflict with the constraints of edge platforms such as on-board mobile\nmanipulators that require real-time performance. Addressing this tension has\nbecome a central focus of recent research. In light of the growing efforts\ntoward more efficient and scalable VLA systems, this survey provides a\nsystematic review of approaches for improving VLA efficiency, with an emphasis\non reducing latency, memory footprint, and training and inference costs. We\ncategorize existing solutions into four dimensions: model architecture,\nperception feature, action generation, and training/inference strategies,\nsummarizing representative techniques within each category. Finally, we discuss\nfuture trends and open challenges, highlighting directions for advancing\nefficient embodied intelligence.",
      "authors": [
        {
          "name": "Weifan Guan",
          "affiliation": null
        },
        {
          "name": "Qinghao Hu",
          "affiliation": null
        },
        {
          "name": "Aosheng Li",
          "affiliation": null
        },
        {
          "name": "Jian Cheng",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17111v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17111v1",
      "primary_category": "cs.RO",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17109v1",
      "title": "Verification-Aware Planning for Multi-Agent Systems",
      "abstract": "Large language model (LLM) agents are increasingly deployed to tackle complex\ntasks, often necessitating collaboration among multiple specialized agents.\nHowever, multi-agent collaboration introduces new challenges in planning,\ncoordination, and verification. Execution failures frequently arise not from\nflawed reasoning alone, but from subtle misalignments in task interpretation,\noutput format, or inter-agent handoffs. To address these challenges, we present\nVeriMAP, a framework for multi-agent collaboration with verification-aware\nplanning. The VeriMAP planner decomposes tasks, models subtask dependencies,\nand encodes planner-defined passing criteria as subtask verification functions\n(VFs) in Python and natural language. We evaluate VeriMAP on diverse datasets,\ndemonstrating that it outperforms both single- and multi-agent baselines while\nenhancing system robustness and interpretability. Our analysis highlights how\nverification-aware planning enables reliable coordination and iterative\nrefinement in multi-agent systems, without relying on external labels or\nannotations.",
      "authors": [
        {
          "name": "Tianyang Xu",
          "affiliation": null
        },
        {
          "name": "Dan Zhang",
          "affiliation": null
        },
        {
          "name": "Kushan Mitra",
          "affiliation": null
        },
        {
          "name": "Estevam Hruschka",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.MA"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17109v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17109v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17108v1",
      "title": "Structured Debate Improves Corporate Credit Reasoning in Financial AI",
      "abstract": "Despite advances in financial AI, the automation of evidence-based reasoning\nremains unresolved in corporate credit assessment, where qualitative\nnon-financial indicators exert decisive influence on loan repayment outcomes\nyet resist formalization. Existing approaches focus predominantly on numerical\nprediction and provide limited support for the interpretive judgments required\nin professional loan evaluation. This study develops and evaluates two\noperational large language model (LLM)-based systems designed to generate\nstructured reasoning from non-financial evidence. The first is a\nnon-adversarial single-agent system (NAS) that produces bidirectional analysis\nthrough a single-pass reasoning pipeline. The second is a debate-based\nmulti-agent system (KPD-MADS) that operationalizes adversarial verification\nthrough a ten-step structured interaction protocol grounded in Karl Popper's\ncritical dialogue framework. Both systems were applied to three real corporate\ncases and evaluated by experienced credit risk professionals. Compared to\nmanual expert reporting, both systems achieved substantial productivity gains\n(NAS: 11.55 s per case; KPD-MADS: 91.97 s; human baseline: 1920 s). The\nKPD-MADS demonstrated superior reasoning quality, receiving higher median\nratings in explanatory adequacy (4.0 vs. 3.0), practical applicability (4.0 vs.\n3.0), and usability (62.5 vs. 52.5). These findings show that structured\nmulti-agent interaction can enhance reasoning rigor and interpretability in\nfinancial AI, advancing scalable and defensible automation in corporate credit\nassessment.",
      "authors": [
        {
          "name": "Yoonjin Lee",
          "affiliation": null
        },
        {
          "name": "Munhee Kim",
          "affiliation": null
        },
        {
          "name": "Hanbi Choi",
          "affiliation": null
        },
        {
          "name": "Juhyeon Park",
          "affiliation": null
        },
        {
          "name": "Seungho Lyoo",
          "affiliation": null
        },
        {
          "name": "Woojin Park",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.AI"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17108v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17108v1",
      "primary_category": "cs.AI",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17106v1",
      "title": "Fighter: Unveiling the Graph Convolutional Nature of Transformers in Time Series Modeling",
      "abstract": "Transformers have achieved remarkable success in time series modeling, yet\ntheir internal mechanisms remain opaque. This work demystifies the Transformer\nencoder by establishing its fundamental equivalence to a Graph Convolutional\nNetwork (GCN). We show that in the forward pass, the attention distribution\nmatrix serves as a dynamic adjacency matrix, and its composition with\nsubsequent transformations performs computations analogous to graph\nconvolution. Moreover, we demonstrate that in the backward pass, the update\ndynamics of value and feed-forward projections mirror those of GCN parameters.\nBuilding on this unified theoretical reinterpretation, we propose\n\\textbf{Fighter} (Flexible Graph Convolutional Transformer), a streamlined\narchitecture that removes redundant linear projections and incorporates\nmulti-hop graph aggregation. This perspective yields an explicit and\ninterpretable representation of temporal dependencies across different scales,\nnaturally expressed as graph edges. Experiments on standard forecasting\nbenchmarks confirm that Fighter achieves competitive performance while\nproviding clearer mechanistic interpretability of its predictions.",
      "authors": [
        {
          "name": "Chen Zhang",
          "affiliation": null
        },
        {
          "name": "Weixin Bu",
          "affiliation": null
        },
        {
          "name": "Wendong Xu",
          "affiliation": null
        },
        {
          "name": "Runsheng Yu",
          "affiliation": null
        },
        {
          "name": "Yik-Chung Wu",
          "affiliation": null
        },
        {
          "name": "Ngai Wong",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17106v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17106v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17105v1",
      "title": "Boosting Fidelity for Pre-Trained-Diffusion-Based Low-Light Image Enhancement via Condition Refinement",
      "abstract": "Diffusion-based methods, leveraging pre-trained large models like Stable\nDiffusion via ControlNet, have achieved remarkable performance in several\nlow-level vision tasks. However, Pre-Trained Diffusion-Based (PTDB) methods\noften sacrifice content fidelity to attain higher perceptual realism. This\nissue is exacerbated in low-light scenarios, where severely degraded\ninformation caused by the darkness limits effective control. We identify two\nprimary causes of fidelity loss: the absence of suitable conditional latent\nmodeling and the lack of bidirectional interaction between the conditional\nlatent and noisy latent in the diffusion process. To address this, we propose a\nnovel optimization strategy for conditioning in pre-trained diffusion models,\nenhancing fidelity while preserving realism and aesthetics. Our method\nintroduces a mechanism to recover spatial details lost during VAE encoding,\ni.e., a latent refinement pipeline incorporating generative priors.\nAdditionally, the refined latent condition interacts dynamically with the noisy\nlatent, leading to improved restoration performance. Our approach is\nplug-and-play, seamlessly integrating into existing diffusion networks to\nprovide more effective control. Extensive experiments demonstrate significant\nfidelity improvements in PTDB methods.",
      "authors": [
        {
          "name": "Xiaogang Xu",
          "affiliation": null
        },
        {
          "name": "Jian Wang",
          "affiliation": null
        },
        {
          "name": "Yunfan Lu",
          "affiliation": null
        },
        {
          "name": "Ruihang Chu",
          "affiliation": null
        },
        {
          "name": "Ruixing Wang",
          "affiliation": null
        },
        {
          "name": "Jiafei Wu",
          "affiliation": null
        },
        {
          "name": "Bei Yu",
          "affiliation": null
        },
        {
          "name": "Liang Lin",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17105v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17105v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17103v1",
      "title": "Adapting to Stochastic and Adversarial Losses in Episodic MDPs with Aggregate Bandit Feedback",
      "abstract": "We study online learning in finite-horizon episodic Markov decision processes\n(MDPs) under the challenging aggregate bandit feedback model, where the learner\nobserves only the cumulative loss incurred in each episode, rather than\nindividual losses at each state-action pair. While prior work in this setting\nhas focused exclusively on worst-case analysis, we initiate the study of\nbest-of-both-worlds (BOBW) algorithms that achieve low regret in both\nstochastic and adversarial environments. We propose the first BOBW algorithms\nfor episodic tabular MDPs with aggregate bandit feedback. In the case of known\ntransitions, our algorithms achieve $O(\\log T)$ regret in stochastic settings\nand ${O}(\\sqrt{T})$ regret in adversarial ones. Importantly, we also establish\nmatching lower bounds, showing the optimality of our algorithms in this\nsetting. We further extend our approach to unknown-transition settings by\nincorporating confidence-based techniques. Our results rely on a combination of\nFTRL over occupancy measures, self-bounding techniques, and new loss estimators\ninspired by recent advances in online shortest path problems. Along the way, we\nalso provide the first individual-gap-dependent lower bounds and demonstrate\nnear-optimal BOBW algorithms for shortest path problems with bandit feedback.",
      "authors": [
        {
          "name": "Shinji Ito",
          "affiliation": null
        },
        {
          "name": "Kevin Jamieson",
          "affiliation": null
        },
        {
          "name": "Haipeng Luo",
          "affiliation": null
        },
        {
          "name": "Arnab Maiti",
          "affiliation": null
        },
        {
          "name": "Taira Tsuchiya",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17103v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17103v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17101v1",
      "title": "Shape-aware Inertial Poser: Motion Tracking for Humans with Diverse Shapes Using Sparse Inertial Sensors",
      "abstract": "Human motion capture with sparse inertial sensors has gained significant\nattention recently. However, existing methods almost exclusively rely on a\ntemplate adult body shape to model the training data, which poses challenges\nwhen generalizing to individuals with largely different body shapes (such as a\nchild). This is primarily due to the variation in IMU-measured acceleration\ncaused by changes in body shape. To fill this gap, we propose Shape-aware\nInertial Poser (SAIP), the first solution considering body shape differences in\nsparse inertial-based motion capture. Specifically, we decompose the sensor\nmeasurements related to shape and pose in order to effectively model their\njoint correlations. Firstly, we train a regression model to transfer the\nIMU-measured accelerations of a real body to match the template adult body\nmodel, compensating for the shape-related sensor measurements. Then, we can\neasily follow the state-of-the-art methods to estimate the full body motions of\nthe template-shaped body. Finally, we utilize a second regression model to map\nthe joint velocities back to the real body, combined with a shape-aware\nphysical optimization strategy to calculate global motions on the subject.\nFurthermore, our method relies on body shape awareness, introducing the first\ninertial shape estimation scheme. This is accomplished by modeling the\nshape-conditioned IMU-pose correlation using an MLP-based network. To validate\nthe effectiveness of SAIP, we also present the first IMU motion capture dataset\ncontaining individuals of different body sizes. This dataset features 10\nchildren and 10 adults, with heights ranging from 110 cm to 190 cm, and a total\nof 400 minutes of paired IMU-Motion samples. Extensive experimental results\ndemonstrate that SAIP can effectively handle motion capture tasks for diverse\nbody shapes. The code and dataset are available at\nhttps://github.com/yinlu5942/SAIP.",
      "authors": [
        {
          "name": "Lu Yin",
          "affiliation": null
        },
        {
          "name": "Ziying Shi",
          "affiliation": null
        },
        {
          "name": "Yinghao Wu",
          "affiliation": null
        },
        {
          "name": "Xinyu Yi",
          "affiliation": null
        },
        {
          "name": "Feng Xu",
          "affiliation": null
        },
        {
          "name": "Shihui Guo",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.GR",
        "cs.CV"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17101v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17101v1",
      "primary_category": "cs.GR",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17099v1",
      "title": "On the Universal Near Optimality of Hedge in Combinatorial Settings",
      "abstract": "In this paper, we study the classical Hedge algorithm in combinatorial\nsettings. In each round, the learner selects a vector $\\boldsymbol{x}_t$ from a\nset $X \\subseteq \\{0,1\\}^d$, observes a full loss vector $\\boldsymbol{y}_t \\in\n\\mathbb{R}^d$, and incurs a loss $\\langle \\boldsymbol{x}_t, \\boldsymbol{y}_t\n\\rangle \\in [-1,1]$. This setting captures several important problems,\nincluding extensive-form games, resource allocation, $m$-sets, online multitask\nlearning, and shortest-path problems on directed acyclic graphs (DAGs). It is\nwell known that Hedge achieves a regret of $O\\big(\\sqrt{T \\log |X|}\\big)$ after\n$T$ rounds of interaction. In this paper, we ask whether Hedge is optimal\nacross all combinatorial settings. To that end, we show that for any $X\n\\subseteq \\{0,1\\}^d$, Hedge is near-optimal--specifically, up to a $\\sqrt{\\log\nd}$ factor--by establishing a lower bound of $\\Omega\\big(\\sqrt{T \\log(|X|)/\\log\nd}\\big)$ that holds for any algorithm. We then identify a natural class of\ncombinatorial sets--namely, $m$-sets with $\\log d \\leq m \\leq \\sqrt{d}$--for\nwhich this lower bound is tight, and for which Hedge is provably suboptimal by\na factor of exactly $\\sqrt{\\log d}$. At the same time, we show that Hedge is\noptimal for online multitask learning, a generalization of the classical\n$K$-experts problem. Finally, we leverage the near-optimality of Hedge to\nestablish the existence of a near-optimal regularizer for online shortest-path\nproblems in DAGs--a setting that subsumes a broad range of combinatorial\ndomains. Specifically, we show that the classical Online Mirror Descent (OMD)\nalgorithm, when instantiated with the dilated entropy regularizer, is\niterate-equivalent to Hedge, and therefore inherits its near-optimal regret\nguarantees for DAGs.",
      "authors": [
        {
          "name": "Zhiyuan Fan",
          "affiliation": null
        },
        {
          "name": "Arnab Maiti",
          "affiliation": null
        },
        {
          "name": "Kevin Jamieson",
          "affiliation": null
        },
        {
          "name": "Lillian J. Ratliff",
          "affiliation": null
        },
        {
          "name": "Gabriele Farina",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cs.GT"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17099v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17099v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17098v1",
      "title": "Can Transformer Memory Be Corrupted? Investigating Cache-Side Vulnerabilities in Large Language Models",
      "abstract": "Even when prompts and parameters are secured, transformer language models\nremain vulnerable because their key-value (KV) cache during inference\nconstitutes an overlooked attack surface. This paper introduces Malicious Token\nInjection (MTI), a modular framework that systematically perturbs cached key\nvectors at selected layers and timesteps through controlled magnitude and\nfrequency, using additive Gaussian noise, zeroing, and orthogonal rotations. A\ntheoretical analysis quantifies how these perturbations propagate through\nattention, linking logit deviations to the Frobenius norm of corruption and\nsoftmax Lipschitz dynamics. Empirical results show that MTI significantly\nalters next-token distributions and downstream task performance across GPT-2\nand LLaMA-2/7B, as well as destabilizes retrieval-augmented and agentic\nreasoning pipelines. These findings identify cache integrity as a critical yet\nunderexplored vulnerability in current LLM deployments, positioning cache\ncorruption as a reproducible and theoretically grounded threat model for future\nrobustness and security research.",
      "authors": [
        {
          "name": "Elias Hossain",
          "affiliation": null
        },
        {
          "name": "Swayamjit Saha",
          "affiliation": null
        },
        {
          "name": "Somshubhra Roy",
          "affiliation": null
        },
        {
          "name": "Ravi Prasad",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17098v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17098v1",
      "primary_category": "cs.CR",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17095v1",
      "title": "GSPlane: Concise and Accurate Planar Reconstruction via Structured Representation",
      "abstract": "Planes are fundamental primitives of 3D sences, especially in man-made\nenvironments such as indoor spaces and urban streets. Representing these planes\nin a structured and parameterized format facilitates scene editing and physical\nsimulations in downstream applications. Recently, Gaussian Splatting (GS) has\ndemonstrated remarkable effectiveness in the Novel View Synthesis task, with\nextensions showing great potential in accurate surface reconstruction. However,\neven state-of-the-art GS representations often struggle to reconstruct planar\nregions with sufficient smoothness and precision. To address this issue, we\npropose GSPlane, which recovers accurate geometry and produces clean and\nwell-structured mesh connectivity for plane regions in the reconstructed scene.\nBy leveraging off-the-shelf segmentation and normal prediction models, GSPlane\nextracts robust planar priors to establish structured representations for\nplanar Gaussian coordinates, which help guide the training process by enforcing\ngeometric consistency. To further enhance training robustness, a Dynamic\nGaussian Re-classifier is introduced to adaptively reclassify planar Gaussians\nwith persistently high gradients as non-planar, ensuring more reliable\noptimization. Furthermore, we utilize the optimized planar priors to refine the\nmesh layouts, significantly improving topological structure while reducing the\nnumber of vertices and faces. We also explore applications of the structured\nplanar representation, which enable decoupling and flexible manipulation of\nobjects on supportive planes. Extensive experiments demonstrate that, with no\nsacrifice in rendering quality, the introduction of planar priors significantly\nimproves the geometric accuracy of the extracted meshes across various\nbaselines.",
      "authors": [
        {
          "name": "Ruitong Gan",
          "affiliation": null
        },
        {
          "name": "Junran Peng",
          "affiliation": null
        },
        {
          "name": "Yang Liu",
          "affiliation": null
        },
        {
          "name": "Chuanchen Luo",
          "affiliation": null
        },
        {
          "name": "Qing Li",
          "affiliation": null
        },
        {
          "name": "Zhaoxiang Zhang",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17095v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17095v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17088v1",
      "title": "Explainable Heterogeneous Anomaly Detection in Financial Networks via Adaptive Expert Routing",
      "abstract": "Financial anomalies exhibit heterogeneous mechanisms (price shocks, liquidity\nfreezes, contagion cascades, regime shifts), but existing detectors treat all\nanomalies uniformly, producing scalar scores without revealing which mechanism\nis failing, where risks concentrate, or how to intervene. This opacity prevents\ntargeted regulatory responses. Three unsolved challenges persist: (1) static\ngraph structures cannot adapt when market correlations shift during regime\nchanges; (2) uniform detection mechanisms miss type-specific signatures across\nmultiple temporal scales while failing to integrate individual behaviors with\nnetwork contagion; (3) black-box outputs provide no actionable guidance on\nanomaly mechanisms or their temporal evolution.\n  We address these via adaptive graph learning with specialized expert networks\nthat provide built-in interpretability. Our framework captures multi-scale\ntemporal dependencies through BiLSTM with self-attention, fuses temporal and\nspatial information via cross-modal attention, learns dynamic graphs through\nneural multi-source interpolation, adaptively balances learned dynamics with\nstructural priors via stress-modulated fusion, routes anomalies to four\nmechanism-specific experts, and produces dual-level interpretable attributions.\nCritically, interpretability is embedded architecturally rather than applied\npost-hoc.\n  On 100 US equities (2017-2024), we achieve 92.3% detection of 13 major events\nwith 3.8-day lead time, outperforming best baseline by 30.8pp. Silicon Valley\nBank case study demonstrates anomaly evolution tracking: Price-Shock expert\nweight rose to 0.39 (33% above baseline 0.29) during closure, peaking at 0.48\n(66% above baseline) one week later, revealing automatic temporal mechanism\nidentification without labeled supervision.",
      "authors": [
        {
          "name": "Zan Li",
          "affiliation": null
        },
        {
          "name": "Rui Fan",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CE"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17088v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17088v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17085v1",
      "title": "Data Reliability Scoring",
      "abstract": "How can we assess the reliability of a dataset without access to ground\ntruth? We introduce the problem of reliability scoring for datasets collected\nfrom potentially strategic sources. The true data are unobserved, but we see\noutcomes of an unknown statistical experiment that depends on them. To\nbenchmark reliability, we define ground-truth-based orderings that capture how\nmuch reported data deviate from the truth. We then propose the Gram determinant\nscore, which measures the volume spanned by vectors describing the empirical\ndistribution of the observed data and experiment outcomes. We show that this\nscore preserves several ground-truth based reliability orderings and, uniquely\nup to scaling, yields the same reliability ranking of datasets regardless of\nthe experiment -- a property we term experiment agnosticism. Experiments on\nsynthetic noise models, CIFAR-10 embeddings, and real employment data\ndemonstrate that the Gram determinant score effectively captures data quality\nacross diverse observation processes.",
      "authors": [
        {
          "name": "Yiling Chen",
          "affiliation": null
        },
        {
          "name": "Shi Feng",
          "affiliation": null
        },
        {
          "name": "Paul Kattuman",
          "affiliation": null
        },
        {
          "name": "Fang-Yi Yu",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cs.GT",
        "stat.ML"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17085v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17085v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17078v1",
      "title": "Towards a Generalizable Fusion Architecture for Multimodal Object Detection",
      "abstract": "Multimodal object detection improves robustness in chal- lenging conditions\nby leveraging complementary cues from multiple sensor modalities. We introduce\nFiltered Multi- Modal Cross Attention Fusion (FMCAF), a preprocess- ing\narchitecture designed to enhance the fusion of RGB and infrared (IR) inputs.\nFMCAF combines a frequency- domain filtering block (Freq-Filter) to suppress\nredun- dant spectral features with a cross-attention-based fusion module (MCAF)\nto improve intermodal feature sharing. Unlike approaches tailored to specific\ndatasets, FMCAF aims for generalizability, improving performance across\ndifferent multimodal challenges without requiring dataset- specific tuning. On\nLLVIP (low-light pedestrian detec- tion) and VEDAI (aerial vehicle detection),\nFMCAF outper- forms traditional fusion (concatenation), achieving +13.9% mAP@50\non VEDAI and +1.1% on LLVIP. These results support the potential of FMCAF as a\nflexible foundation for robust multimodal fusion in future detection pipelines.",
      "authors": [
        {
          "name": "Jad Berjawi",
          "affiliation": null
        },
        {
          "name": "Yoann Dupas",
          "affiliation": null
        },
        {
          "name": "Christophe C'erin",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "I.2.10; I.4.8"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17078v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17078v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17072v1",
      "title": "DFNN: A Deep Fr\u00e9chet Neural Network Framework for Learning Metric-Space-Valued Responses",
      "abstract": "Regression with non-Euclidean responses -- e.g., probability distributions,\nnetworks, symmetric positive-definite matrices, and compositions -- has become\nincreasingly important in modern applications. In this paper, we propose deep\nFr\\'echet neural networks (DFNNs), an end-to-end deep learning framework for\npredicting non-Euclidean responses -- which are considered as random objects in\na metric space -- from Euclidean predictors. Our method leverages the\nrepresentation-learning power of deep neural networks (DNNs) to the task of\napproximating conditional Fr\\'echet means of the response given the predictors,\nthe metric-space analogue of conditional expectations, by minimizing a\nFr\\'echet risk. The framework is highly flexible, accommodating diverse metrics\nand high-dimensional predictors. We establish a universal approximation theorem\nfor DFNNs, advancing the state-of-the-art of neural network approximation\ntheory to general metric-space-valued responses without making model\nassumptions or relying on local smoothing. Empirical studies on synthetic\ndistributional and network-valued responses, as well as a real-world\napplication to predicting employment occupational compositions, demonstrate\nthat DFNNs consistently outperform existing methods.",
      "authors": [
        {
          "name": "Kyum Kim",
          "affiliation": null
        },
        {
          "name": "Yaqing Chen",
          "affiliation": null
        },
        {
          "name": "Paromita Dubey",
          "affiliation": null
        }
      ],
      "categories": [
        "stat.ML",
        "cs.LG",
        "stat.ME"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17072v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17072v1",
      "primary_category": "stat.ML",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17068v1",
      "title": "ProDAT: Progressive Density-Aware Tail-Drop for Point Cloud Coding",
      "abstract": "Three-dimensional (3D) point clouds are becoming increasingly vital in\napplications such as autonomous driving, augmented reality, and immersive\ncommunication, demanding real-time processing and low latency. However, their\nlarge data volumes and bandwidth constraints hinder the deployment of\nhigh-quality services in resource-limited environments. Progres- sive coding,\nwhich allows for decoding at varying levels of detail, provides an alternative\nby allowing initial partial decoding with subsequent refinement. Although\nrecent learning-based point cloud geometry coding methods have achieved notable\nsuccess, their fixed latent representation does not support progressive\ndecoding. To bridge this gap, we propose ProDAT, a novel density-aware\ntail-drop mechanism for progressive point cloud coding. By leveraging density\ninformation as a guidance signal, latent features and coordinates are decoded\nadaptively based on their significance, therefore achieving progressive\ndecoding at multiple bitrates using one single model. Experimental results on\nbenchmark datasets show that the proposed ProDAT not only enables progressive\ncoding but also achieves superior coding efficiency compared to\nstate-of-the-art learning-based coding techniques, with over 28.6% BD-rate\nimprovement for PSNR- D2 on SemanticKITTI and over 18.15% for ShapeNet",
      "authors": [
        {
          "name": "Zhe Luo",
          "affiliation": null
        },
        {
          "name": "Wenjing Jia",
          "affiliation": null
        },
        {
          "name": "Stuart Perry",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17068v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17068v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17067v1",
      "title": "Convergence of Regret Matching in Potential Games and Constrained Optimization",
      "abstract": "Regret matching (RM} -- and its modern variants -- is a foundational online\nalgorithm that has been at the heart of many AI breakthrough results in solving\nbenchmark zero-sum games, such as poker. Yet, surprisingly little is known so\nfar in theory about its convergence beyond two-player zero-sum games. For\nexample, whether regret matching converges to Nash equilibria in potential\ngames has been an open problem for two decades. Even beyond games, one could\ntry to use RM variants for general constrained optimization problems. Recent\nempirical evidence suggests that they -- particularly regret matching$^+$\n(RM$^+$) -- attain strong performance on benchmark constrained optimization\nproblems, outperforming traditional gradient descent-type algorithms.\n  We show that alternating RM$^+$ converges to an $\\epsilon$-KKT point after\n$O_\\epsilon(1/\\epsilon^4)$ iterations, establishing for the first time that it\nis a sound and fast first-order optimizer. Our argument relates the KKT gap to\nthe accumulated regret, two quantities that are entirely disparate in general\nbut interact in an intriguing way in our setting, so much so that when regrets\nare bounded, our complexity bound improves all the way to\n$O_\\epsilon(1/\\epsilon^2)$. From a technical standpoint, while RM$^+$ does not\nhave the usual one-step improvement property in general, we show that it does\nin a certain region that the algorithm will quickly reach and remain in\nthereafter. In sharp contrast, our second main result establishes a lower\nbound: RM, with or without alternation, can take an exponential number of\niterations to reach a crude approximate solution even in two-player potential\ngames. This represents the first worst-case separation between RM and RM$^+$.\nOur lower bound shows that convergence to coarse correlated equilibria in\npotential games is exponentially faster than convergence to Nash equilibria.",
      "authors": [
        {
          "name": "Ioannis Anagnostides",
          "affiliation": null
        },
        {
          "name": "Emanuel Tewolde",
          "affiliation": null
        },
        {
          "name": "Brian Hu Zhang",
          "affiliation": null
        },
        {
          "name": "Ioannis Panageas",
          "affiliation": null
        },
        {
          "name": "Vincent Conitzer",
          "affiliation": null
        },
        {
          "name": "Tuomas Sandholm",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.GT",
        "cs.LG",
        "math.OC"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17067v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17067v1",
      "primary_category": "cs.GT",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17064v1",
      "title": "A Brain Cell Type Resource Created by Large Language Models and a Multi-Agent AI System for Collaborative Community Annotation",
      "abstract": "Single-cell RNA sequencing has transformed our ability to identify diverse\ncell types and their transcriptomic signatures. However, annotating these\nsignatures-especially those involving poorly characterized genes-remains a\nmajor challenge. Traditional methods, such as Gene Set Enrichment Analysis\n(GSEA), depend on well-curated annotations and often perform poorly in these\ncontexts. Large Language Models (LLMs) offer a promising alternative but\nstruggle to represent complex biological knowledge within structured\nontologies. To address this, we present BRAINCELL-AID (BRAINCELL-AID:\nhttps://biodataai.uth.edu/BRAINCELL-AID), a novel multi-agent AI system that\nintegrates free-text descriptions with ontology labels to enable more accurate\nand robust gene set annotation. By incorporating retrieval-augmented generation\n(RAG), we developed a robust agentic workflow that refines predictions using\nrelevant PubMed literature, reducing hallucinations and enhancing\ninterpretability. Using this workflow, we achieved correct annotations for 77%\nof mouse gene sets among their top predictions. Applying this approach, we\nannotated 5,322 brain cell clusters from the comprehensive mouse brain cell\natlas generated by the BRAIN Initiative Cell Census Network, enabling novel\ninsights into brain cell function by identifying region-specific gene\nco-expression patterns and inferring functional roles of gene ensembles.\nBRAINCELL-AID also identifies Basal Ganglia-related cell types with\nneurologically meaningful descriptions. Hence, we create a valuable resource to\nsupport community-driven cell type annotation.",
      "authors": [
        {
          "name": "Rongbin Li",
          "affiliation": null
        },
        {
          "name": "Wenbo Chen",
          "affiliation": null
        },
        {
          "name": "Zhao Li",
          "affiliation": null
        },
        {
          "name": "Rodrigo Munoz-Castaneda",
          "affiliation": null
        },
        {
          "name": "Jinbo Li",
          "affiliation": null
        },
        {
          "name": "Neha S. Maurya",
          "affiliation": null
        },
        {
          "name": "Arnav Solanki",
          "affiliation": null
        },
        {
          "name": "Huan He",
          "affiliation": null
        },
        {
          "name": "Hanwen Xing",
          "affiliation": null
        },
        {
          "name": "Meaghan Ramlakhan",
          "affiliation": null
        },
        {
          "name": "Zachary Wise",
          "affiliation": null
        },
        {
          "name": "Zhuhao Wu",
          "affiliation": null
        },
        {
          "name": "Hua Xu",
          "affiliation": null
        },
        {
          "name": "Michael Hawrylycz",
          "affiliation": null
        },
        {
          "name": "W. Jim Zheng",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.AI"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17064v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17064v1",
      "primary_category": "cs.AI",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17063v1",
      "title": "Mode Collapse of Mean-Field Variational Inference",
      "abstract": "Mean-field variational inference (MFVI) is a widely used method for\napproximating high-dimensional probability distributions by product measures.\nIt has been empirically observed that MFVI optimizers often suffer from mode\ncollapse. Specifically, when the target measure $\\pi$ is a mixture $\\pi = w P_0\n+ (1 - w) P_1$, the MFVI optimizer tends to place most of its mass near a\nsingle component of the mixture. This work provides the first theoretical\nexplanation of mode collapse in MFVI. We introduce the notion to capture the\nseparatedness of the two mixture components -- called\n$\\varepsilon$-separateness -- and derive explicit bounds on the fraction of\nmass that any MFVI optimizer assigns to each component when $P_0$ and $P_1$ are\n$\\varepsilon$-separated for sufficiently small $\\varepsilon$. Our results\nsuggest that the occurrence of mode collapse crucially depends on the relative\nposition of the components. To address this issue, we propose the rotational\nvariational inference (RoVI), which augments MFVI with a rotation matrix. The\nnumerical studies support our theoretical findings and demonstrate the benefits\nof RoVI.",
      "authors": [
        {
          "name": "Shunan Sheng",
          "affiliation": null
        },
        {
          "name": "Bohan Wu",
          "affiliation": null
        },
        {
          "name": "Alberto Gonz\u00e1lez-Sanz",
          "affiliation": null
        }
      ],
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17063v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17063v1",
      "primary_category": "stat.ML",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17062v1",
      "title": "Investigating Thinking Behaviours of Reasoning-Based Language Models for Social Bias Mitigation",
      "abstract": "While reasoning-based large language models excel at complex tasks through an\ninternal, structured thinking process, a concerning phenomenon has emerged that\nsuch a thinking process can aggregate social stereotypes, leading to biased\noutcomes. However, the underlying behaviours of these language models in social\nbias scenarios remain underexplored. In this work, we systematically\ninvestigate mechanisms within the thinking process behind this phenomenon and\nuncover two failure patterns that drive social bias aggregation: 1) stereotype\nrepetition, where the model relies on social stereotypes as its primary\njustification, and 2) irrelevant information injection, where it fabricates or\nintroduces new details to support a biased narrative. Building on these\ninsights, we introduce a lightweight prompt-based mitigation approach that\nqueries the model to review its own initial reasoning against these specific\nfailure patterns. Experiments on question answering (BBQ and StereoSet) and\nopen-ended (BOLD) benchmarks show that our approach effectively reduces bias\nwhile maintaining or improving accuracy.",
      "authors": [
        {
          "name": "Guoqing Luo",
          "affiliation": null
        },
        {
          "name": "Iffat Maab",
          "affiliation": null
        },
        {
          "name": "Lili Mou",
          "affiliation": null
        },
        {
          "name": "Junichi Yamagishi",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17062v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17062v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17059v1",
      "title": "Consistent Zero-Shot Imitation with Contrastive Goal Inference",
      "abstract": "In the same way that generative models today conduct most of their training\nin a self-supervised fashion, how can agentic models conduct their training in\na self-supervised fashion, interactively exploring, learning, and preparing to\nquickly adapt to new tasks? A prerequisite for embodied agents deployed in real\nworld interactions ought to be training with interaction, yet today's most\nsuccessful AI models (e.g., VLMs, LLMs) are trained without an explicit notion\nof action. The problem of pure exploration (which assumes no data as input) is\nwell studied in the reinforcement learning literature and provides agents with\na wide array of experiences, yet it fails to prepare them for rapid adaptation\nto new tasks. Today's language and vision models are trained on data provided\nby humans, which provides a strong inductive bias for the sorts of tasks that\nthe model will have to solve (e.g., modeling chords in a song, phrases in a\nsonnet, sentences in a medical record). However, when they are prompted to\nsolve a new task, there is a faulty tacit assumption that humans spend most of\ntheir time in the most rewarding states. The key contribution of our paper is a\nmethod for pre-training interactive agents in a self-supervised fashion, so\nthat they can instantly mimic human demonstrations. Our method treats goals\n(i.e., observations) as the atomic construct. During training, our method\nautomatically proposes goals and practices reaching them, building off prior\nwork in reinforcement learning exploration. During evaluation, our method\nsolves an (amortized) inverse reinforcement learning problem to explain\ndemonstrations as optimal goal-reaching behavior. Experiments on standard\nbenchmarks (not designed for goal-reaching) show that our approach outperforms\nprior methods for zero-shot imitation.",
      "authors": [
        {
          "name": "Kathryn Wantlin",
          "affiliation": null
        },
        {
          "name": "Chongyi Zheng",
          "affiliation": null
        },
        {
          "name": "Benjamin Eysenbach",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17059v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17059v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17058v1",
      "title": "Bitwidth-Specific Logarithmic Arithmetic for Future Hardware-Accelerated Training",
      "abstract": "While advancements in quantization have significantly reduced the\ncomputational costs of inference in deep learning, training still predominantly\nrelies on complex floating-point arithmetic. Low-precision fixed-point training\npresents a compelling alternative. This work introduces a novel enhancement in\nlow-precision logarithmic fixed-point training, geared towards future hardware\naccelerator designs. We propose incorporating bitwidth in the design of\napproximations to arithmetic operations. To this end, we introduce a new\nhardware-friendly, piece-wise linear approximation for logarithmic addition.\nUsing simulated annealing, we optimize this approximation at different\nprecision levels. A C++ bit-true simulation demonstrates training of VGG-11 and\nVGG-16 models on CIFAR-100 and TinyImageNet, respectively, using 12-bit integer\narithmetic with minimal accuracy degradation compared to 32-bit floating-point\ntraining. Our hardware study reveals up to 32.5% reduction in area and 53.5%\nreduction in energy consumption for the proposed LNS multiply-accumulate units\ncompared to that of linear fixed-point equivalents.",
      "authors": [
        {
          "name": "Hassan Hamad",
          "affiliation": null
        },
        {
          "name": "Yuou Qiu",
          "affiliation": null
        },
        {
          "name": "Peter A. Beerel",
          "affiliation": null
        },
        {
          "name": "Keith M. Chugg",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17058v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17058v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    },
    {
      "arxiv_id": "2510.17057v1",
      "title": "The Ends Justify the Thoughts: RL-Induced Motivated Reasoning in LLMs",
      "abstract": "The use of reinforcement learning (RL) with chain-of-thought (CoT) reasoning\nhas emerged as a promising approach for developing more capable language\nmodels. In turn, this has led to investigation of CoT monitoring as a\ncompelling method for detecting harmful behaviors such as reward hacking, under\nthe assumption that models' reasoning processes reflect their internal\ndecision-making. In practice, LLM training often produces unintended behaviors\ndue to imperfect reward signals, leading models to develop misaligned\ntendencies. A common corrective approach is to apply post-hoc instructions to\navoid problematic behaviors like sycophancy, but what happens to the model's\nreasoning process when these instructions conflict with learned behaviors? We\ninvestigate this question in simple settings and find that models engage in\nsystematic motivated reasoning -- generating plausible-sounding justifications\nfor violating their instructions while downplaying potential harms. Beyond\nbeing an interesting property of training, we find that while motivated\nreasoning can be detected by most frontier reasoning models, smaller LLM judges\ncan fail to identify a portion of it, and in rare cases can themselves be\npersuaded that the reasoning is correct, despite it contradicting clear\ninstructions. This capability gap raises concerns that as models become more\nsophisticated, their motivated reasoning may become increasingly difficult for\nmonitors to detect. Our results underscore the need to account for motivated\nreasoning when relying on chain-of-thought processes for model evaluation and\noversight. All code for this paper will be made available. WARNING: some\nexamples in this paper may be upsetting.",
      "authors": [
        {
          "name": "Nikolaus Howe",
          "affiliation": null
        },
        {
          "name": "Micah Carroll",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published_date": "2025-10-20",
      "arxiv_url": "http://arxiv.org/abs/2510.17057v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17057v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-20"
    }
  ]
}