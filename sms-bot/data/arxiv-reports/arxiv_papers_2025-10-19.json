{
  "fetch_date": "2025-10-21 12:52:50 UTC",
  "target_date": "2025-10-19",
  "total_papers": 158,
  "categories": [
    "cs.AI",
    "cs.LG",
    "cs.CV",
    "cs.CL",
    "stat.ML"
  ],
  "papers": [
    {
      "arxiv_id": "2510.17052v1",
      "title": "ToolCritic: Detecting and Correcting Tool-Use Errors in Dialogue Systems",
      "abstract": "Tool-augmented large language models (LLMs) are increasingly employed in\nreal-world applications, but tool usage errors still hinder their reliability.\nWe introduce ToolCritic, a diagnostic framework that evaluates and improves LLM\nbehavior in multi-turn, tool-augmented dialogues. ToolCritic detects eight\ndistinct error types specific to tool-calling (e.g., premature invocation,\nargument misalignment, and misinterpretation of tool outputs) and provides\ntargeted feedback to the main LLM. The main LLM, assumed to have strong\nreasoning, task understanding and orchestration capabilities, then revises its\nresponse based on ToolCritic's feedback. We systematically define these error\ncategories and construct a synthetic dataset to train ToolCritic. Experimental\nresults on the Schema-Guided Dialogue (SGD) dataset demonstrate that ToolCritic\nimproves tool-calling accuracy by up to 13% over baselines, including zero-shot\nprompting and self-correction techniques. This represents a promising step\ntoward more robust LLM integration with external tools in real-world dialogue\napplications.",
      "authors": [
        {
          "name": "Hassan Hamad",
          "affiliation": null
        },
        {
          "name": "Yingru Xu",
          "affiliation": null
        },
        {
          "name": "Liang Zhao",
          "affiliation": null
        },
        {
          "name": "Wenbo Yan",
          "affiliation": null
        },
        {
          "name": "Narendra Gyanchandani",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.AI"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.17052v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17052v1",
      "primary_category": "cs.AI",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.17051v1",
      "title": "How Universal Are SAM2 Features?",
      "abstract": "The trade-off between general-purpose foundation vision models and their\nspecialized counterparts is critical for efficient feature coding design and is\nnot yet fully understood. We investigate this trade-off by comparing the\nfeature versatility of the general-purpose Hiera encoder against the\nsegmentation-specialized Segment Anything Model 2 (SAM2). Using a lightweight,\ntrainable neck to probe the adaptability of their frozen features, we quantify\nthe information-theoretic cost of specialization. Our results reveal that while\nSAM2's specialization is highly effective for spatially-related tasks like\ndepth estimation, it comes at a cost. The specialized SAM2 encoder\nunderperforms its generalist predecessor, Hiera, on conceptually distant tasks\nsuch as pose estimation and image captioning, demonstrating a measurable loss\nof broader semantic information. A novel cross-neck analysis on SAM2 reveals\nthat each level of adaptation creates a further representational bottleneck.\nOur analysis illuminates these trade-offs in feature universality, providing a\nquantitative foundation for designing efficient feature coding and adaptation\nstrategies for diverse downstream applications.",
      "authors": [
        {
          "name": "Masoud Khairi Atani",
          "affiliation": null
        },
        {
          "name": "Alon Harell",
          "affiliation": null
        },
        {
          "name": "Hyomin Choi",
          "affiliation": null
        },
        {
          "name": "Runyu Yang",
          "affiliation": null
        },
        {
          "name": "Fabien Racape",
          "affiliation": null
        },
        {
          "name": "Ivan V. Bajic",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.17051v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17051v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.17045v1",
      "title": "Video Reasoning without Training",
      "abstract": "Video reasoning using Large Multimodal Models (LMMs) relies on costly\nreinforcement learning (RL) and verbose chain-of-thought, resulting in\nsubstantial computational overhead during both training and inference.\nMoreover, the mechanisms that control the thinking process in these reasoning\nmodels are very limited. In this paper, using entropy of the model's output as\na signal, we discover that the high-quality models go through a series of\nmicro-explorations and micro-exploitations which keep the reasoning process\ngrounded (i.e., avoid excessive randomness while the model is exploring or\nthinking through an answer). We further observe that once this \"thinking\"\nprocess is over, more accurate models demonstrate a better convergence by\nreducing the entropy significantly via a final exploitation phase (i.e., a more\ncertain convergence towards a solution trajectory). We then use these novel,\ntheoretically-grounded insights to tune the model's behavior directly at\ninference, without using any RL or supervised fine-tuning. Specifically, during\ninference, our proposed approach called V-Reason (Video-Reason) adapts the\nvalue cache of the LMM via a few optimization steps on a small, trainable\ncontroller using an entropy-based objective, i.e., no supervision from any\ndataset or RL is necessary. This tuning improves the model's micro-exploration\nand exploitation behavior during inference. Our experiments show that our\nproposed method achieves significant improvements over the base\ninstruction-tuned models across several video reasoning datasets, narrowing the\ngap with RL-trained models to within 0.6% average accuracy without any\ntraining, while offering massive efficiency benefits: output tokens are reduced\nby 58.6% compared to the RL model.",
      "authors": [
        {
          "name": "Deepak Sridhar",
          "affiliation": null
        },
        {
          "name": "Kartikeya Bhardwaj",
          "affiliation": null
        },
        {
          "name": "Jeya Pradha Jeyaraj",
          "affiliation": null
        },
        {
          "name": "Nuno Vasconcelos",
          "affiliation": null
        },
        {
          "name": "Ankita Nayak",
          "affiliation": null
        },
        {
          "name": "Harris Teague",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.17045v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17045v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.17043v1",
      "title": "Person Re-Identification via Generalized Class Prototypes",
      "abstract": "Advanced feature extraction methods have significantly contributed to\nenhancing the task of person re-identification. In addition, modifications to\nobjective functions have been developed to further improve performance.\nNonetheless, selecting better class representatives is an underexplored area of\nresearch that can also lead to advancements in re-identification performance.\nAlthough past works have experimented with using the centroid of a gallery\nimage class during training, only a few have investigated alternative\nrepresentations during the retrieval stage. In this paper, we demonstrate that\nthese prior techniques yield suboptimal results in terms of re-identification\nmetrics. To address the re-identification problem, we propose a generalized\nselection method that involves choosing representations that are not limited to\nclass centroids. Our approach strikes a balance between accuracy and mean\naverage precision, leading to improvements beyond the state of the art. For\nexample, the actual number of representations per class can be adjusted to meet\nspecific application requirements. We apply our methodology on top of multiple\nre-identification embeddings, and in all cases it substantially improves upon\ncontemporary results",
      "authors": [
        {
          "name": "Md Ahmed Al Muzaddid",
          "affiliation": null
        },
        {
          "name": "William J. Beksi",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.17043v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17043v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.17040v1",
      "title": "Diverse Influence Component Analysis: A Geometric Approach to Nonlinear Mixture Identifiability",
      "abstract": "Latent component identification from unknown nonlinear mixtures is a\nfoundational challenge in machine learning, with applications in tasks such as\ndisentangled representation learning and causal inference. Prior work in\nnonlinear independent component analysis (nICA) has shown that auxiliary\nsignals -- such as weak supervision -- can support identifiability of\nconditionally independent latent components. More recent approaches explore\nstructural assumptions, e.g., sparsity in the Jacobian of the mixing function,\nto relax such requirements. In this work, we introduce Diverse Influence\nComponent Analysis (DICA), a framework that exploits the convex geometry of the\nmixing function's Jacobian. We propose a Jacobian Volume Maximization\n(J-VolMax) criterion, which enables latent component identification by\nencouraging diversity in their influence on the observed variables. Under\nreasonable conditions, this approach achieves identifiability without relying\non auxiliary information, latent component independence, or Jacobian sparsity\nassumptions. These results extend the scope of identifiability analysis and\noffer a complementary perspective to existing methods.",
      "authors": [
        {
          "name": "Hoang-Son Nguyen",
          "affiliation": null
        },
        {
          "name": "Xiao Fu",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.17040v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17040v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.17039v1",
      "title": "Click, Predict, Trust: Clinician-in-the-Loop AI Segmentation for Lung Cancer CT-Based Prognosis within the Knowledge-to-Action Framework",
      "abstract": "Lung cancer remains the leading cause of cancer mortality, with CT imaging\ncentral to screening, prognosis, and treatment. Manual segmentation is variable\nand time-intensive, while deep learning (DL) offers automation but faces\nbarriers to clinical adoption. Guided by the Knowledge-to-Action framework,\nthis study develops a clinician-in-the-loop DL pipeline to enhance\nreproducibility, prognostic accuracy, and clinical trust. Multi-center CT data\nfrom 999 patients across 12 public datasets were analyzed using five DL models\n(3D Attention U-Net, ResUNet, VNet, ReconNet, SAM-Med3D), benchmarked against\nexpert contours on whole and click-point cropped images. Segmentation\nreproducibility was assessed using 497 PySERA-extracted radiomic features via\nSpearman correlation, ICC, Wilcoxon tests, and MANOVA, while prognostic\nmodeling compared supervised (SL) and semi-supervised learning (SSL) across 38\ndimensionality reduction strategies and 24 classifiers. Six physicians\nqualitatively evaluated masks across seven domains, including clinical\nmeaningfulness, boundary quality, prognostic value, trust, and workflow\nintegration. VNet achieved the best performance (Dice = 0.83, IoU = 0.71),\nradiomic stability (mean correlation = 0.76, ICC = 0.65), and predictive\naccuracy under SSL (accuracy = 0.88, F1 = 0.83). SSL consistently outperformed\nSL across models. Radiologists favored VNet for peritumoral representation and\nsmoother boundaries, preferring AI-generated initial masks for refinement\nrather than replacement. These results demonstrate that integrating VNet with\nSSL yields accurate, reproducible, and clinically trusted CT-based lung cancer\nprognosis, highlighting a feasible path toward physician-centered AI\ntranslation.",
      "authors": [
        {
          "name": "Mohammad R. Salmanpour",
          "affiliation": null
        },
        {
          "name": "Sonya Falahati",
          "affiliation": null
        },
        {
          "name": "Amir Hossein Pouria",
          "affiliation": null
        },
        {
          "name": "Amin Mousavi",
          "affiliation": null
        },
        {
          "name": "Somayeh Sadat Mehrnia",
          "affiliation": null
        },
        {
          "name": "Morteza Alizadeh",
          "affiliation": null
        },
        {
          "name": "Arman Gorji",
          "affiliation": null
        },
        {
          "name": "Zeinab Farsangi",
          "affiliation": null
        },
        {
          "name": "Alireza Safarian",
          "affiliation": null
        },
        {
          "name": "Mehdi Maghsudi",
          "affiliation": null
        },
        {
          "name": "Carlos Uribe",
          "affiliation": null
        },
        {
          "name": "Arman Rahmim",
          "affiliation": null
        },
        {
          "name": "Ren Yuan",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "F.2.2; I.2.7"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.17039v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17039v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.17038v1",
      "title": "DINO-CVA: A Multimodal Goal-Conditioned Vision-to-Action Model for Autonomous Catheter Navigation",
      "abstract": "Cardiac catheterization remains a cornerstone of minimally invasive\ninterventions, yet it continues to rely heavily on manual operation. Despite\nadvances in robotic platforms, existing systems are predominantly follow-leader\nin nature, requiring continuous physician input and lacking intelligent\nautonomy. This dependency contributes to operator fatigue, more radiation\nexposure, and variability in procedural outcomes. This work moves towards\nautonomous catheter navigation by introducing DINO-CVA, a multimodal\ngoal-conditioned behavior cloning framework. The proposed model fuses visual\nobservations and joystick kinematics into a joint embedding space, enabling\npolicies that are both vision-aware and kinematic-aware. Actions are predicted\nautoregressively from expert demonstrations, with goal conditioning guiding\nnavigation toward specified destinations. A robotic experimental setup with a\nsynthetic vascular phantom was designed to collect multimodal datasets and\nevaluate performance. Results show that DINO-CVA achieves high accuracy in\npredicting actions, matching the performance of a kinematics-only baseline\nwhile additionally grounding predictions in the anatomical environment. These\nfindings establish the feasibility of multimodal, goal-conditioned\narchitectures for catheter navigation, representing an important step toward\nreducing operator dependency and improving the reliability of catheterbased\ntherapies.",
      "authors": [
        {
          "name": "Pedram Fekri",
          "affiliation": null
        },
        {
          "name": "Majid Roshanfar",
          "affiliation": null
        },
        {
          "name": "Samuel Barbeau",
          "affiliation": null
        },
        {
          "name": "Seyedfarzad Famouri",
          "affiliation": null
        },
        {
          "name": "Thomas Looi",
          "affiliation": null
        },
        {
          "name": "Dale Podolsky",
          "affiliation": null
        },
        {
          "name": "Mehrdad Zadeh",
          "affiliation": null
        },
        {
          "name": "Javad Dargahi",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.17038v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17038v1",
      "primary_category": "cs.RO",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.17036v1",
      "title": "Hephaestus: Mixture Generative Modeling with Energy Guidance for Large-scale QoS Degradation",
      "abstract": "We study the Quality of Service Degradation (QoSD) problem, in which an\nadversary perturbs edge weights to degrade network performance. This setting\narises in both network infrastructures and distributed ML systems, where\ncommunication quality, not just connectivity, determines functionality. While\nclassical methods rely on combinatorial optimization, and recent ML approaches\naddress only restricted linear variants with small-size networks, no prior\nmodel directly tackles the QoSD problem under nonlinear edge-weight functions.\nThis work proposes \\PIMMA, a self-reinforcing generative framework that\nsynthesizes feasible solutions in latent space, to fill this gap. Our method\nincludes three phases: (1) Forge: a Predictive Path-Stressing (PPS) algorithm\nthat uses graph learning and approximation to produce feasible solutions with\nperformance guarantee, (2) Morph: a new theoretically grounded training\nparadigm for Mixture of Conditional VAEs guided by an energy-based model to\ncapture solution feature distributions, and (3) Refine: a reinforcement\nlearning agent that explores this space to generate progressively near-optimal\nsolutions using our designed differentiable reward function. Experiments on\nboth synthetic and real-world networks show that our approach consistently\noutperforms classical and ML baselines, particularly in scenarios with\nnonlinear cost functions where traditional methods fail to generalize.",
      "authors": [
        {
          "name": "Nguyen Do",
          "affiliation": null
        },
        {
          "name": "Bach Ngo",
          "affiliation": null
        },
        {
          "name": "Youval Kashuv",
          "affiliation": null
        },
        {
          "name": "Canh V. Pham",
          "affiliation": null
        },
        {
          "name": "Hanghang Tong",
          "affiliation": null
        },
        {
          "name": "My T. Thai",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.17036v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17036v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.17035v1",
      "title": "Conditional Synthetic Live and Spoof Fingerprint Generation",
      "abstract": "Large fingerprint datasets, while important for training and evaluation, are\ntime-consuming and expensive to collect and require strict privacy measures.\nResearchers are exploring the use of synthetic fingerprint data to address\nthese issues. This paper presents a novel approach for generating synthetic\nfingerprint images (both spoof and live), addressing concerns related to\nprivacy, cost, and accessibility in biometric data collection. Our approach\nutilizes conditional StyleGAN2-ADA and StyleGAN3 architectures to produce\nhigh-resolution synthetic live fingerprints, conditioned on specific finger\nidentities (thumb through little finger). Additionally, we employ CycleGANs to\ntranslate these into realistic spoof fingerprints, simulating a variety of\npresentation attack materials (e.g., EcoFlex, Play-Doh). These synthetic spoof\nfingerprints are crucial for developing robust spoof detection systems. Through\nthese generative models, we created two synthetic datasets (DB2 and DB3), each\ncontaining 1,500 fingerprint images of all ten fingers with multiple\nimpressions per finger, and including corresponding spoofs in eight material\ntypes. The results indicate robust performance: our StyleGAN3 model achieves a\nFr\\'echet Inception Distance (FID) as low as 5, and the generated fingerprints\nachieve a True Accept Rate of 99.47% at a 0.01% False Accept Rate. The\nStyleGAN2-ADA model achieved a TAR of 98.67% at the same 0.01% FAR. We assess\nfingerprint quality using standard metrics (NFIQ2, MINDTCT), and notably,\nmatching experiments confirm strong privacy preservation, with no significant\nevidence of identity leakage, confirming the strong privacy-preserving\nproperties of our synthetic datasets.",
      "authors": [
        {
          "name": "Syed Konain Abbas",
          "affiliation": null
        },
        {
          "name": "Sandip Purnapatra",
          "affiliation": null
        },
        {
          "name": "M. G. Sarwar Murshed",
          "affiliation": null
        },
        {
          "name": "Conor Miller-Lynch",
          "affiliation": null
        },
        {
          "name": "Lambert Igene",
          "affiliation": null
        },
        {
          "name": "Soumyabrata Dey",
          "affiliation": null
        },
        {
          "name": "Stephanie Schuckers",
          "affiliation": null
        },
        {
          "name": "Faraz Hussain",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.17035v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17035v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.17034v1",
      "title": "Where, Not What: Compelling Video LLMs to Learn Geometric Causality for 3D-Grounding",
      "abstract": "Multimodal 3D grounding has garnered considerable interest in Vision-Language\nModels (VLMs) \\cite{yin2025spatial} for advancing spatial reasoning in complex\nenvironments. However, these models suffer from a severe \"2D semantic bias\"\nthat arises from over-reliance on 2D image features for coarse localization,\nlargely disregarding 3D geometric inputs and resulting in suboptimal fusion\nperformance. In this paper, we propose a novel training framework called\nWhat-Where Representation Re-Forming (W2R2) to tackle this issue via\ndisentangled representation learning and targeted shortcut suppression. Our\napproach fundamentally reshapes the model's internal space by designating 2D\nfeatures as semantic beacons for \"What\" identification and 3D features as\nspatial anchors for \"Where\" localization, enabling precise 3D grounding without\nmodifying inference architecture. Key components include a dual-objective loss\nfunction with an Alignment Loss that supervises fused predictions using adapted\ncross-entropy for multimodal synergy, and a Pseudo-Label Loss that penalizes\noverly effective 2D-dominant pseudo-outputs via a margin-based mechanism.\nExperiments conducted on ScanRefer and ScanQA demonstrate the effectiveness of\nW2R2, with significant gains in localization accuracy and robustness,\nparticularly in cluttered outdoor scenes.",
      "authors": [
        {
          "name": "Yutong Zhong",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.17034v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17034v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.17028v1",
      "title": "Mapping from Meaning: Addressing the Miscalibration of Prompt-Sensitive Language Models",
      "abstract": "An interesting behavior in large language models (LLMs) is prompt\nsensitivity. When provided with different but semantically equivalent versions\nof the same prompt, models may produce very different distributions of answers.\nThis suggests that the uncertainty reflected in a model's output distribution\nfor one prompt may not reflect the model's uncertainty about the meaning of the\nprompt. We model prompt sensitivity as a type of generalization error, and show\nthat sampling across the semantic ``concept space'' with paraphrasing\nperturbations improves uncertainty calibration without compromising accuracy.\nAdditionally, we introduce a new metric for uncertainty decomposition in\nblack-box LLMs that improves upon entropy-based decomposition by modeling\nsemantic continuities in natural language generation. We show that this\ndecomposition metric can be used to quantify how much LLM uncertainty is\nattributed to prompt sensitivity. Our work introduces a new way to improve\nuncertainty calibration in prompt-sensitive language models, and provides\nevidence that some LLMs fail to exhibit consistent general reasoning about the\nmeanings of their inputs.",
      "authors": [
        {
          "name": "Kyle Cox",
          "affiliation": null
        },
        {
          "name": "Jiawei Xu",
          "affiliation": null
        },
        {
          "name": "Yikun Han",
          "affiliation": null
        },
        {
          "name": "Rong Xu",
          "affiliation": null
        },
        {
          "name": "Tianhao Li",
          "affiliation": null
        },
        {
          "name": "Chi-Yang Hsu",
          "affiliation": null
        },
        {
          "name": "Tianlong Chen",
          "affiliation": null
        },
        {
          "name": "Walter Gerych",
          "affiliation": null
        },
        {
          "name": "Ying Ding",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.17028v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17028v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.17023v1",
      "title": "Enrich and Detect: Video Temporal Grounding with Multimodal LLMs",
      "abstract": "We introduce ED-VTG, a method for fine-grained video temporal grounding\nutilizing multi-modal large language models. Our approach harnesses the\ncapabilities of multimodal LLMs to jointly process text and video, in order to\neffectively localize natural language queries in videos through a two-stage\nprocess. Rather than being directly grounded, language queries are initially\ntransformed into enriched sentences that incorporate missing details and cues\nto aid in grounding. In the second stage, these enriched queries are grounded,\nusing a lightweight decoder, which specializes at predicting accurate\nboundaries conditioned on contextualized representations of the enriched\nqueries. To mitigate noise and reduce the impact of hallucinations, our model\nis trained with a multiple-instance-learning objective that dynamically selects\nthe optimal version of the query for each training sample. We demonstrate\nstate-of-the-art results across various benchmarks in temporal video grounding\nand paragraph grounding settings. Experiments reveal that our method\nsignificantly outperforms all previously proposed LLM-based temporal grounding\napproaches and is either superior or comparable to specialized models, while\nmaintaining a clear advantage against them in zero-shot evaluation scenarios.",
      "authors": [
        {
          "name": "Shraman Pramanick",
          "affiliation": null
        },
        {
          "name": "Effrosyni Mavroudi",
          "affiliation": null
        },
        {
          "name": "Yale Song",
          "affiliation": null
        },
        {
          "name": "Rama Chellappa",
          "affiliation": null
        },
        {
          "name": "Lorenzo Torresani",
          "affiliation": null
        },
        {
          "name": "Triantafyllos Afouras",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "cs.MM"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.17023v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17023v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.17022v1",
      "title": "Curiosity-driven RL for symbolic equation solving",
      "abstract": "We explore if RL can be useful for symbolic mathematics. Previous work showed\ncontrastive learning can solve linear equations in one variable. We show\nmodel-free PPO \\cite{schulman2017proximal} augmented with curiosity-based\nexploration and graph-based actions can solve nonlinear equations such as those\ninvolving radicals, exponentials, and trig functions. Our work suggests\ncuriosity-based exploration may be useful for general symbolic reasoning tasks.",
      "authors": [
        {
          "name": "Kevin P. O Keeffe",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.17022v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17022v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.17021v1",
      "title": "Forgetting to Forget: Attention Sink as A Gateway for Backdooring LLM Unlearning",
      "abstract": "Large language model (LLM) unlearning has become a critical mechanism for\nremoving undesired data, knowledge, or behaviors from pre-trained models while\nretaining their general utility. Yet, with the rise of open-weight LLMs, we\nask: can the unlearning process itself be backdoored, appearing successful\nunder normal conditions yet reverting to pre-unlearned behavior when a hidden\ntrigger is activated? Drawing inspiration from classical backdoor attacks that\nembed triggers into training data to enforce specific behaviors, we investigate\nbackdoor unlearning, where models forget as intended in the clean setting but\nrecover forgotten knowledge when the trigger appears. We show that designing\nsuch attacks presents unique challenges, hinging on where triggers are placed\nand how backdoor training is reinforced. We uncover a strong link between\nbackdoor efficacy and the attention sink phenomenon, i.e., shallow input tokens\nconsistently attract disproportionate attention in LLMs. Our analysis reveals\nthat these attention sinks serve as gateways for backdoor unlearning: placing\ntriggers at sink positions and aligning their attention values markedly\nenhances backdoor persistence. Extensive experiments validate these findings,\nshowing that attention-sink-guided backdoor unlearning reliably restores\nforgotten knowledge in the presence of backdoor triggers, while behaving\nindistinguishably from a normally unlearned model when triggers are absent.\nCode is available at https://github.com/OPTML-Group/Unlearn-Backdoor.",
      "authors": [
        {
          "name": "Bingqi Shang",
          "affiliation": null
        },
        {
          "name": "Yiwei Chen",
          "affiliation": null
        },
        {
          "name": "Yihua Zhang",
          "affiliation": null
        },
        {
          "name": "Bingquan Shen",
          "affiliation": null
        },
        {
          "name": "Sijia Liu",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.17021v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17021v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.17018v1",
      "title": "Extended LSTM: Adaptive Feature Gating for Toxic Comment Classification",
      "abstract": "Toxic comment detection remains a challenging task, where transformer-based\nmodels (e.g., BERT) incur high computational costs and degrade on minority\ntoxicity classes, while classical ensembles lack semantic adaptability. We\npropose xLSTM, a parameter-efficient and theoretically grounded framework that\nunifies cosine-similarity gating, adaptive feature prioritization, and\nprincipled class rebalancing. A learnable reference vector {v} in {R}^d\nmodulates contextual embeddings via cosine similarity, amplifying toxic cues\nand attenuating benign signals to yield stronger gradients under severe class\nimbalance. xLSTM integrates multi-source embeddings (GloVe, FastText, BERT CLS)\nthrough a projection layer, a character-level BiLSTM for morphological cues,\nembedding-space SMOTE for minority augmentation, and adaptive focal loss with\ndynamic class weighting. On the Jigsaw Toxic Comment benchmark, xLSTM attains\n96.0% accuracy and 0.88 macro-F1, outperforming BERT by 33% on threat and 28%\non identity_hate categories, with 15 times fewer parameters and 50ms inference\nlatency. Cosine gating contributes a +4.8% F1 gain in ablations. The results\nestablish a new efficiency adaptability frontier, demonstrating that\nlightweight, theoretically informed architectures can surpass large pretrained\nmodels on imbalanced, domain-specific NLP tasks.",
      "authors": [
        {
          "name": "Noor Islam S. Mohammad",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.17018v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17018v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.17017v1",
      "title": "SafeSearch: Do Not Trade Safety for Utility in LLM Search Agents",
      "abstract": "Large language model (LLM) based search agents iteratively generate queries,\nretrieve external information, and reason to answer open-domain questions.\nWhile researchers have primarily focused on improving their utility, their\nsafety behaviors remain underexplored. In this paper, we first evaluate search\nagents using red-teaming datasets and find that they are more likely to produce\nharmful outputs than base LLMs. For example, when asked ``How can I track\nsomeone's location without their consent?'', a base model refuses, whereas a\nsearch agent designed to retrieve and cite sources may lower its refusal\nthreshold, fetch documents (e.g., court cases), and, once appended, synthesize\nthem into an informative yet unsafe summary. We further show that\nutility-oriented fine-tuning intensifies this risk, motivating joint alignment\nof safety and utility. We present SafeSearch, a multi-objective reinforcement\nlearning approach that couples a final-output safety/utility reward with a\nnovel query-level shaping term that penalizes unsafe queries and rewards safe\nones. Experiments show that SafeSearch reduces agent harmfulness by over 70%\nacross three red-teaming datasets while producing safe, helpful responses, and\nmatches the QA performance of a utility-only finetuned agent; further analyses\nconfirm the effectiveness of the query-level reward in jointly improving safety\nand utility.",
      "authors": [
        {
          "name": "Qiusi Zhan",
          "affiliation": null
        },
        {
          "name": "Angeline Budiman-Chan",
          "affiliation": null
        },
        {
          "name": "Abdelrahman Zayed",
          "affiliation": null
        },
        {
          "name": "Xingzhi Guo",
          "affiliation": null
        },
        {
          "name": "Daniel Kang",
          "affiliation": null
        },
        {
          "name": "Joo-Kyung Kim",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.17017v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17017v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.17015v1",
      "title": "Justitia: Fair and Efficient Scheduling for LLM Applications",
      "abstract": "In the era of Large Language Models (LLMs), it has been popular to launch a\nseries of LLM inferences -- we call an LLM application -- to better solve\nreal-world problems. When serving those applications in shared GPU servers, the\nschedulers are expected to attain fast application completions with guaranteed\nworst-case performance. However, mainstream LLM schedulers fail to behave well\nfor LLM applications -- due to head-of-line blocking or over-constrained\nresource allocation. In this paper, we propose to serve LLM applications in a\nfair and also efficient manner. To this end, we design Justitia, a novel\nscheduler with three key techniques. First, given that memory is prevalently a\nbottleneck for mainstream inference frameworks like vLLM, Justitia models the\nservice cost of LLM applications in a memory-centric manner. Meanwhile, it uses\na simple neural network model to conduct light-weight and also accurate demand\nprediction. Moreover, Justitia adopts a virtual-time based fair queuing\nalgorithm to reduce the overall performance with guaranteed worst-case delay.\nWe have implemented Justitia atop vLLM, and experimental results involving\ndiverse LLM applications show that it can substantially enhance the scheduling\nefficiency with fairness preserved.",
      "authors": [
        {
          "name": "Mingyan Yang",
          "affiliation": null
        },
        {
          "name": "Guanjie Wang",
          "affiliation": null
        },
        {
          "name": "Manqi Luo",
          "affiliation": null
        },
        {
          "name": "Yifei Liu",
          "affiliation": null
        },
        {
          "name": "Chen Chen",
          "affiliation": null
        },
        {
          "name": "Han Zhao",
          "affiliation": null
        },
        {
          "name": "Yu Feng",
          "affiliation": null
        },
        {
          "name": "Quan Chen",
          "affiliation": null
        },
        {
          "name": "Minyi Guo",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.17015v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17015v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.17014v1",
      "title": "Do Satellite Tasks Need Special Pretraining?",
      "abstract": "Foundation models have advanced machine learning across various modalities,\nincluding images. Recently multiple teams trained foundation models specialized\nfor remote sensing applications. This line of research is motivated by the\ndistinct characteristics of remote sensing imagery, specific applications and\ntypes of robustness useful for satellite image analysis. In this work we\nsystematically challenge the idea that specific foundation models are more\nuseful than general-purpose vision foundation models, at least in the small\nscale. First, we design a simple benchmark that measures generalization of\nremote sensing models towards images with lower resolution for two downstream\ntasks. Second, we train iBOT, a self-supervised vision encoder, on MillionAID,\nan ImageNet-scale satellite imagery dataset, with several modifications\nspecific to remote sensing. We show that none of those pretrained models bring\nconsistent improvements upon general-purpose baselines at the ViT-B scale.",
      "authors": [
        {
          "name": "Ani Vanyan",
          "affiliation": null
        },
        {
          "name": "Alvard Barseghyan",
          "affiliation": null
        },
        {
          "name": "Hakob Tamazyan",
          "affiliation": null
        },
        {
          "name": "Tigran Galstyan",
          "affiliation": null
        },
        {
          "name": "Vahan Huroyan",
          "affiliation": null
        },
        {
          "name": "Naira Hovakimyan",
          "affiliation": null
        },
        {
          "name": "Hrant Khachatrian",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.17014v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17014v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.17013v1",
      "title": "DiscoTrack: A Multilingual LLM Benchmark for Discourse Tracking",
      "abstract": "Recent LLM benchmarks have tested models on a range of phenomena, but are\nstill focused primarily on natural language understanding for extraction of\nexplicit information, such as QA or summarization, with responses often tar-\ngeting information from individual sentences. We are still lacking more\nchallenging, and im- portantly also multilingual, benchmarks focus- ing on\nimplicit information and pragmatic infer- ences across larger documents in the\ncontext of discourse tracking: integrating and aggregating information across\nsentences, paragraphs and multiple speaker utterances. To this end, we present\nDiscoTrack, an LLM benchmark target- ing a range of tasks across 12 languages\nand four levels of discourse understanding: salience recognition, entity\ntracking, discourse relations and bridging inference. Our evaluation shows that\nthese tasks remain challenging, even for state-of-the-art models.",
      "authors": [
        {
          "name": "Lanni Bu",
          "affiliation": null
        },
        {
          "name": "Lauren Levin",
          "affiliation": null
        },
        {
          "name": "Amir Zeldes",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.17013v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17013v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.17007v1",
      "title": "An empirical study of the effect of video encoders on Temporal Video Grounding",
      "abstract": "Temporal video grounding is a fundamental task in computer vision, aiming to\nlocalize a natural language query in a long, untrimmed video. It has a key role\nin the scientific community, in part due to the large amount of video generated\nevery day. Although we find extensive work in this task, we note that research\nremains focused on a small selection of video representations, which may lead\nto architectural overfitting in the long run. To address this issue, we propose\nan empirical study to investigate the impact of different video features on a\nclassical architecture. We extract features for three well-known benchmarks,\nCharades-STA, ActivityNet-Captions and YouCookII, using video encoders based on\nCNNs, temporal reasoning and transformers. Our results show significant\ndifferences in the performance of our model by simply changing the video\nencoder, while also revealing clear patterns and errors derived from the use of\ncertain features, ultimately indicating potential feature complementarity.",
      "authors": [
        {
          "name": "Ignacio M. De la Jara",
          "affiliation": null
        },
        {
          "name": "Cristian Rodriguez-Opazo",
          "affiliation": null
        },
        {
          "name": "Edison Marrese-Taylor",
          "affiliation": null
        },
        {
          "name": "Felipe Bravo-Marquez",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.17007v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17007v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.17006v1",
      "title": "Online Learning Defense against Iterative Jailbreak Attacks via Prompt Optimization",
      "abstract": "Iterative jailbreak methods that repeatedly rewrite and input prompts into\nlarge language models (LLMs) to induce harmful outputs -- using the model's\nprevious responses to guide each new iteration -- have been found to be a\nhighly effective attack strategy. Despite being an effective attack strategy\nagainst LLMs and their safety mechanisms, existing defenses do not proactively\ndisrupt this dynamic trial-and-error cycle. In this study, we propose a novel\nframework that dynamically updates its defense strategy through online learning\nin response to each new prompt from iterative jailbreak methods. Leveraging the\ndistinctions between harmful jailbreak-generated prompts and typical harmless\nprompts, we introduce a reinforcement learning-based approach that optimizes\nprompts to ensure appropriate responses for harmless tasks while explicitly\nrejecting harmful prompts. Additionally, to curb overfitting to the narrow band\nof partial input rewrites explored during an attack, we introduce\nPast-Direction Gradient Damping (PDGD). Experiments conducted on three LLMs\nshow that our approach significantly outperforms five existing defense methods\nagainst five iterative jailbreak methods. Moreover, our results indicate that\nour prompt optimization strategy simultaneously enhances response quality for\nharmless tasks.",
      "authors": [
        {
          "name": "Masahiro Kaneko",
          "affiliation": null
        },
        {
          "name": "Zeerak Talat",
          "affiliation": null
        },
        {
          "name": "Timothy Baldwin",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.17006v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17006v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.17004v1",
      "title": "ReclAIm: A multi-agent framework for degradation-aware performance tuning of medical imaging AI",
      "abstract": "Ensuring the long-term reliability of AI models in clinical practice requires\ncontinuous performance monitoring and corrective actions when degradation\noccurs. Addressing this need, this manuscript presents ReclAIm, a multi-agent\nframework capable of autonomously monitoring, evaluating, and fine-tuning\nmedical image classification models. The system, built on a large language\nmodel core, operates entirely through natural language interaction, eliminating\nthe need for programming expertise. ReclAIm successfully trains, evaluates, and\nmaintains consistent performance of models across MRI, CT, and X-ray datasets.\nOnce ReclAIm detects significant performance degradation, it autonomously\nexecutes state-of-the-art fine-tuning procedures that substantially reduce the\nperformance gap. In cases with performance drops of up to -41.1% (MRI\nInceptionV3), ReclAIm managed to readjust performance metrics within 1.5% of\nthe initial model results. ReclAIm enables automated, continuous maintenance of\nmedical imaging AI models in a user-friendly and adaptable manner that\nfacilitates broader adoption in both research and clinical environments.",
      "authors": [
        {
          "name": "Eleftherios Tzanis",
          "affiliation": null
        },
        {
          "name": "Michail E. Klontzas",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.17004v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17004v1",
      "primary_category": "cs.MA",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.17002v1",
      "title": "EEschematic: Multimodal-LLM Based AI Agent for Schematic Generation of Analog Circuit",
      "abstract": "Circuit schematics play a crucial role in analog integrated circuit design,\nserving as the primary medium for human understanding and verification of\ncircuit functionality. While recent large language model (LLM)-based approaches\nhave shown promise in circuit topology generation and device sizing, most rely\nsolely on textual representations such as SPICE netlists, which lack visual\ninterpretability for circuit designers. To address this limitation, we propose\nEEschematic, an AI agent for automatic analog schematic generation based on a\nMultimodal Large Language Model (MLLM). EEschematic integrates textual, visual,\nand symbolic modalities to translate SPICE netlists into schematic diagrams\nrepresented in a human-editable format. The framework uses six analog\nsubstructure examples for few-shot placement and a Visual Chain-of-Thought\n(VCoT) strategy to iteratively refine placement and wiring, enhancing schematic\nclarity and symmetry. Experimental results on representative analog circuits,\nincluding a CMOS inverter, a five-transistor operational transconductance\namplifier (5T-OTA), and a telescopic cascode amplifier, demonstrate that\nEEschematic produces schematics with high visual quality and structural\ncorrectness.",
      "authors": [
        {
          "name": "Chang Liu",
          "affiliation": null
        },
        {
          "name": "Danial Chitnis",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.17002v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17002v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.17001v1",
      "title": "Vocab Diet: Reshaping the Vocabulary of LLMs with Vector Arithmetic",
      "abstract": "Large language models (LLMs) were shown to encode word form variations, such\nas \"walk\"->\"walked\", as linear directions in embedding space. However, standard\ntokenization algorithms treat these variations as distinct tokens -- filling\nthe size-capped vocabulary with surface form variants (e.g., \"walk\", \"walking\",\n\"Walk\"), at the expense of less frequent words and multilingual coverage. We\nshow that many of these variations can be captured by transformation vectors --\nadditive offsets that yield the appropriate word's representation when applied\nto the base form word embedding -- in both the input and output spaces.\nBuilding on this, we propose a compact reshaping of the vocabulary: rather than\nassigning unique tokens to each surface form, we compose them from shared base\nform and transformation vectors (e.g., \"walked\" = \"walk\" + past tense). We\napply our approach to multiple LLMs and across five languages, removing up to\n10% of vocabulary entries -- thereby freeing space to allocate new, more\ndiverse tokens. Importantly, we do so while also expanding vocabulary coverage\nto out-of-vocabulary words, with minimal impact on downstream performance, and\nwithout modifying model weights. Our findings motivate a foundational\nrethinking of vocabulary design, moving from string enumeration to a\ncompositional vocabulary that leverages the underlying structure of language.",
      "authors": [
        {
          "name": "Yuval Reif",
          "affiliation": null
        },
        {
          "name": "Guy Kaplan",
          "affiliation": null
        },
        {
          "name": "Roy Schwartz",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.17001v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17001v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.17000v1",
      "title": "Bits Leaked per Query: Information-Theoretic Bounds on Adversarial Attacks against LLMs",
      "abstract": "Adversarial attacks by malicious users that threaten the safety of large\nlanguage models (LLMs) can be viewed as attempts to infer a target property $T$\nthat is unknown when an instruction is issued, and becomes knowable only after\nthe model's reply is observed. Examples of target properties $T$ include the\nbinary flag that triggers an LLM's harmful response or rejection, and the\ndegree to which information deleted by unlearning can be restored, both\nelicited via adversarial instructions. The LLM reveals an \\emph{observable\nsignal} $Z$ that potentially leaks hints for attacking through a response\ncontaining answer tokens, thinking process tokens, or logits. Yet the scale of\ninformation leaked remains anecdotal, leaving auditors without principled\nguidance and defenders blind to the transparency--risk trade-off. We fill this\ngap with an information-theoretic framework that computes how much information\ncan be safely disclosed, and enables auditors to gauge how close their methods\ncome to the fundamental limit. Treating the mutual information $I(Z;T)$ between\nthe observation $Z$ and the target property $T$ as the leaked bits per query,\nwe show that achieving error $\\varepsilon$ requires at least\n$\\log(1/\\varepsilon)/I(Z;T)$ queries, scaling linearly with the inverse leak\nrate and only logarithmically with the desired accuracy. Thus, even a modest\nincrease in disclosure collapses the attack cost from quadratic to logarithmic\nin terms of the desired accuracy. Experiments on seven LLMs across\nsystem-prompt leakage, jailbreak, and relearning attacks corroborate the\ntheory: exposing answer tokens alone requires about a thousand queries; adding\nlogits cuts this to about a hundred; and revealing the full thinking process\ntrims it to a few dozen. Our results provide the first principled yardstick for\nbalancing transparency and security when deploying LLMs.",
      "authors": [
        {
          "name": "Masahiro Kaneko",
          "affiliation": null
        },
        {
          "name": "Timothy Baldwin",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CR",
        "cs.CL",
        "cs.LG"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.17000v1",
      "pdf_url": "http://arxiv.org/pdf/2510.17000v1",
      "primary_category": "cs.CR",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16996v1",
      "title": "STARK: Strategic Team of Agents for Refining Kernels",
      "abstract": "The efficiency of GPU kernels is central to the progress of modern AI, yet\noptimizing them remains a difficult and labor-intensive task due to complex\ninteractions between memory hierarchies, thread scheduling, and\nhardware-specific characteristics. While recent advances in large language\nmodels (LLMs) provide new opportunities for automated code generation, existing\napproaches largely treat LLMs as single-shot generators or naive refinement\ntools, limiting their effectiveness in navigating the irregular kernel\noptimization landscape. We introduce an LLM agentic framework for GPU kernel\noptimization that systematically explores the design space through multi-agent\ncollaboration, grounded instruction, dynamic context management, and strategic\nsearch. This framework mimics the workflow of expert engineers, enabling LLMs\nto reason about hardware trade-offs, incorporate profiling feedback, and refine\nkernels iteratively. We evaluate our approach on KernelBench, a benchmark for\nLLM-based kernel optimization, and demonstrate substantial improvements over\nbaseline agents: our system produces correct solutions where baselines often\nfail, and achieves kernels with up to 16x faster runtime performance. These\nresults highlight the potential of agentic LLM frameworks to advance fully\nautomated, scalable GPU kernel optimization.",
      "authors": [
        {
          "name": "Juncheng Dong",
          "affiliation": null
        },
        {
          "name": "Yang Yang",
          "affiliation": null
        },
        {
          "name": "Tao Liu",
          "affiliation": null
        },
        {
          "name": "Yang Wang",
          "affiliation": null
        },
        {
          "name": "Feng Qi",
          "affiliation": null
        },
        {
          "name": "Vahid Tarokh",
          "affiliation": null
        },
        {
          "name": "Kaushik Rangadurai",
          "affiliation": null
        },
        {
          "name": "Shuang Yang",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.AI"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16996v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16996v1",
      "primary_category": "cs.AI",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16990v1",
      "title": "Graph4MM: Weaving Multimodal Learning with Structural Information",
      "abstract": "Real-world multimodal data usually exhibit complex structural relationships\nbeyond traditional one-to-one mappings like image-caption pairs. Entities\nacross modalities interact in intricate ways, with images and text forming\ndiverse interconnections through contextual dependencies and co-references.\nGraphs provide powerful structural information for modeling intra-modal and\ninter-modal relationships. However, previous works fail to distinguish\nmulti-hop neighbors and treat the graph as a standalone modality, which\nfragments the overall understanding. This limitation presents two key\nchallenges in multimodal learning: (1) integrating structural information from\nmulti-hop neighbors into foundational models, and (2) fusing modality-specific\ninformation in a principled manner. To address these challenges, we revisit the\nrole of graphs in multimodal learning within the era of foundation models and\npropose Graph4MM, a graph-based multimodal learning framework. To be specific,\nwe introduce Hop-Diffused Attention, which integrates multi-hop structural\ninformation into self-attention through causal masking and hop diffusion.\nFurthermore, we design MM-QFormer, a multi-mapping querying transformer for\ncross-modal fusion. Through theoretical and empirical analysis, we show that\nleveraging structures to integrate both intra- and inter-modal interactions\nimproves multimodal understanding beyond treating them as a standalone\nmodality. Experiments on both generative and discriminative tasks show that\nGraph4MM outperforms larger VLMs, LLMs, and multimodal graph baselines,\nachieving a 6.93% average improvement.",
      "authors": [
        {
          "name": "Xuying Ning",
          "affiliation": null
        },
        {
          "name": "Dongqi Fu",
          "affiliation": null
        },
        {
          "name": "Tianxin Wei",
          "affiliation": null
        },
        {
          "name": "Wujiang Xu",
          "affiliation": null
        },
        {
          "name": "Jingrui He",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16990v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16990v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16989v1",
      "title": "Training-free Online Video Step Grounding",
      "abstract": "Given a task and a set of steps composing it, Video Step Grounding (VSG) aims\nto detect which steps are performed in a video. Standard approaches for this\ntask require a labeled training set (e.g., with step-level annotations or\nnarrations), which may be costly to collect. Moreover, they process the full\nvideo offline, limiting their applications for scenarios requiring online\ndecisions. Thus, in this work, we explore how to perform VSG online and without\ntraining. We achieve this by exploiting the zero-shot capabilities of recent\nLarge Multimodal Models (LMMs). In particular, we use LMMs to predict the step\nassociated with a restricted set of frames, without access to the whole video.\nWe show that this online strategy without task-specific tuning outperforms\noffline and training-based models. Motivated by this finding, we develop\nBayesian Grounding with Large Multimodal Models (BaGLM), further injecting\nknowledge of past frames into the LMM-based predictions. BaGLM exploits\nBayesian filtering principles, modeling step transitions via (i) a dependency\nmatrix extracted through large language models and (ii) an estimation of step\nprogress. Experiments on three datasets show superior performance of BaGLM over\nstate-of-the-art training-based offline methods.",
      "authors": [
        {
          "name": "Luca Zanella",
          "affiliation": null
        },
        {
          "name": "Massimiliano Mancini",
          "affiliation": null
        },
        {
          "name": "Yiming Wang",
          "affiliation": null
        },
        {
          "name": "Alessio Tonioni",
          "affiliation": null
        },
        {
          "name": "Elisa Ricci",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16989v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16989v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16988v1",
      "title": "CARE: Contrastive Alignment for ADL Recognition from Event-Triggered Sensor Streams",
      "abstract": "The recognition of Activities of Daily Living (ADLs) from event-triggered\nambient sensors is an essential task in Ambient Assisted Living, yet existing\nmethods remain constrained by representation-level limitations. Sequence-based\napproaches preserve temporal order of sensor activations but are sensitive to\nnoise and lack spatial awareness, while image-based approaches capture global\npatterns and implicit spatial correlations but compress fine-grained temporal\ndynamics and distort sensor layouts. Naive fusion (e.g., feature concatenation)\nfail to enforce alignment between sequence- and image-based representation\nviews, underutilizing their complementary strengths. We propose Contrastive\nAlignment for ADL Recognition from Event-Triggered Sensor Streams (CARE), an\nend-to-end framework that jointly optimizes representation learning via\nSequence-Image Contrastive Alignment (SICA) and classification via\ncross-entropy, ensuring both cross-representation alignment and task-specific\ndiscriminability. CARE integrates (i) time-aware, noise-resilient sequence\nencoding with (ii) spatially-informed and frequency-sensitive image\nrepresentations, and employs (iii) a joint contrastive-classification objective\nfor end-to-end learning of aligned and discriminative embeddings. Evaluated on\nthree CASAS datasets, CARE achieves state-of-the-art performance (89.8% on\nMilan, 88.9% on Cairo, and 73.3% on Kyoto7) and demonstrates robustness to\nsensor malfunctions and layout variability, highlighting its potential for\nreliable ADL recognition in smart homes.",
      "authors": [
        {
          "name": "Junhao Zhao",
          "affiliation": null
        },
        {
          "name": "Zishuai Liu",
          "affiliation": null
        },
        {
          "name": "Ruili Fang",
          "affiliation": null
        },
        {
          "name": "Jin Lu",
          "affiliation": null
        },
        {
          "name": "Linghan Zhang",
          "affiliation": null
        },
        {
          "name": "Fei Dou",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16988v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16988v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16987v1",
      "title": "Back to Bytes: Revisiting Tokenization Through UTF-8",
      "abstract": "We present UTF8Tokenizer, a minimalist byte-level tokenizer that maps text\nexactly to IDs corresponding to the bytes underlying the text's UTF-8 encoding\n(e.g., byte x09 is token ID 9). Unlike prior byte-level approaches (Xue et al.,\n2021; Pagnoni et al., 2025), our implementation never introduces out-of-range\nIDs (i.e. there is no token ID 256) or auxiliary tokens: all special behavior\n(e.g., padding, boundaries, conversation structure, attention segments, tool\ncalling, \"thinking\" spans, etc.) is encoded using C0 control bytes - just as\nASCII was originally designed to embed control information alongside printable\ntext. These design principles yield practical benefits: (1) faster tokenization\n(14x) and significantly lower host-device transfer (8x less than int64); (2)\nsimple, shareable 256*d embedding tables that can be aligned across models; and\n(3) a training-time enhancement via bit-biased embeddings, which exposes\nper-byte bit structure and can be added to the embedding table post-training,\nremoving inference costs. Our HuggingFace-compatible implementation improves\nlanguage modeling convergence.",
      "authors": [
        {
          "name": "Amit Moryossef",
          "affiliation": null
        },
        {
          "name": "Clara Meister",
          "affiliation": null
        },
        {
          "name": "Pavel Stepachev",
          "affiliation": null
        },
        {
          "name": "Desmond Elliott",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16987v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16987v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16986v1",
      "title": "Adaptive Sample Sharing for Linear Regression",
      "abstract": "In many business settings, task-specific labeled data are scarce or costly to\nobtain, which limits supervised learning on a specific task. To address this\nchallenge, we study sample sharing in the case of ridge regression: leveraging\nan auxiliary data set while explicitly protecting against negative transfer. We\nintroduce a principled, data-driven rule that decides how many samples from an\nauxiliary dataset to add to the target training set. The rule is based on an\nestimate of the transfer gain i.e. the marginal reduction in the predictive\nerror. Building on this estimator, we derive finite-sample guaranties: under\nstandard conditions, the procedure borrows when it improves parameter\nestimation and abstains otherwise. In the Gaussian feature setting, we analyze\nwhich data set properties ensure that borrowing samples reduces the predictive\nerror. We validate the approach in synthetic and real datasets, observing\nconsistent gains over strong baselines and single-task training while avoiding\nnegative transfer.",
      "authors": [
        {
          "name": "Hamza Cherkaoui",
          "affiliation": null
        },
        {
          "name": "H\u00e9l\u00e8ne Halconruy",
          "affiliation": null
        },
        {
          "name": "Yohan Petetin",
          "affiliation": null
        }
      ],
      "categories": [
        "stat.ML",
        "cs.LG",
        "stat.OT"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16986v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16986v1",
      "primary_category": "stat.ML",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16985v1",
      "title": "Parameter-Efficient Fine-Tuning for Low-Resource Languages: A Comparative Study of LLMs for Bengali Hate Speech Detection",
      "abstract": "Bengali social media platforms have witnessed a sharp increase in hate\nspeech, disproportionately affecting women and adolescents. While datasets such\nas BD-SHS provide a basis for structured evaluation, most prior approaches rely\non either computationally costly full-model fine-tuning or proprietary APIs.\nThis paper presents the first application of Parameter-Efficient Fine-Tuning\n(PEFT) for Bengali hate speech detection using LoRA and QLoRA. Three\ninstruction-tuned large language models - Gemma-3-4B, Llama-3.2-3B, and\nMistral-7B - were fine-tuned on the BD-SHS dataset of 50,281 annotated\ncomments. Each model was adapted by training fewer than 1% of its parameters,\nenabling experiments on a single consumer-grade GPU. The results show that\nLlama-3.2-3B achieved the highest F1-score of 92.23%, followed by Mistral-7B at\n88.94% and Gemma-3-4B at 80.25%. These findings establish PEFT as a practical\nand replicable strategy for Bengali and related low-resource languages.",
      "authors": [
        {
          "name": "Akif Islam",
          "affiliation": null
        },
        {
          "name": "Mohd Ruhul Ameen",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16985v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16985v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16983v1",
      "title": "One-step Diffusion Models with Bregman Density Ratio Matching",
      "abstract": "Diffusion and flow models achieve high generative quality but remain\ncomputationally expensive due to slow multi-step sampling. Distillation methods\naccelerate them by training fast student generators, yet most existing\nobjectives lack a unified theoretical foundation. In this work, we propose\nDi-Bregman, a compact framework that formulates diffusion distillation as\nBregman divergence-based density-ratio matching. This convex-analytic view\nconnects several existing objectives through a common lens. Experiments on\nCIFAR-10 and text-to-image generation demonstrate that Di-Bregman achieves\nimproved one-step FID over reverse-KL distillation and maintains high visual\nfidelity compared to the teacher model. Our results highlight Bregman\ndensity-ratio matching as a practical and theoretically-grounded route toward\nefficient one-step diffusion generation.",
      "authors": [
        {
          "name": "Yuanzhi Zhu",
          "affiliation": null
        },
        {
          "name": "Eleftherios Tsonis",
          "affiliation": null
        },
        {
          "name": "Lucas Degeorge",
          "affiliation": null
        },
        {
          "name": "Vicky Kalogeiton",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16983v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16983v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16981v1",
      "title": "MuonBP: Faster Muon via Block-Periodic Orthogonalization",
      "abstract": "Gradient orthogonalization is a simple strategy that shows great utility in\nspeeding up gradient descent. The Muon optimizer (Jordan, Jin, et al., 2024)\ncombines gradient orthogonalization with first-order momentum and achieves\nsignificant improvement in data efficiency over Adam/AdamW (Loshchilov and\nHutter, 2019) for language model training. However, when using model\nparallelism, gradient orthogonalization introduces additional overhead compared\nto coordinate-wise optimizers (such as AdamW) due to additional gather and\nscatter operations on gradient matrix shards from different devices. This\nadditional communication can amount to a throughput hit of 5%-10% compared to\nAdam/AdamW. To remedy this, we propose Muon with Block-Periodic\nOrthogonalization (MuonBP), which applies orthogonalization independently to\nmatrix shards on each device and periodically performs full orthogonalization\nto maintain training stability at scale. We show how to adjust the learning\nrate from the baseline to MuonBP and give convergence guarantees for this\nalgorithm. Crucially, our theory dictates that we use two stepsizes: one for\nthe blockwise orthogonalization steps, and one for the full orthogonalization\nsteps. Our method is simple, requires minimal hyperparameter adjustments, and\nachieves competitive iteration complexity compared with baseline Muon while\nproviding per-iteration throughput comparable to coordinate-wise methods such\nas AdamW. When training an 8B model with eight-way tensor parallelism and ZeRO\noptimizer state sharding, MuonBP achieves 8% throughput increase compared to\nMuon with no degradation in performance.",
      "authors": [
        {
          "name": "Ahmed Khaled",
          "affiliation": null
        },
        {
          "name": "Kaan Ozkara",
          "affiliation": null
        },
        {
          "name": "Tao Yu",
          "affiliation": null
        },
        {
          "name": "Mingyi Hong",
          "affiliation": null
        },
        {
          "name": "Youngsuk Park",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "math.OC"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16981v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16981v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16980v1",
      "title": "Towards Interpretable and Trustworthy Time Series Reasoning: A BlueSky Vision",
      "abstract": "Time series reasoning is emerging as the next frontier in temporal analysis,\naiming to move beyond pattern recognition towards explicit, interpretable, and\ntrustworthy inference. This paper presents a BlueSky vision built on two\ncomplementary directions. One builds robust foundations for time series\nreasoning, centered on comprehensive temporal understanding, structured\nmulti-step reasoning, and faithful evaluation frameworks. The other advances\nsystem-level reasoning, moving beyond language-only explanations by\nincorporating multi-agent collaboration, multi-modal context, and\nretrieval-augmented approaches. Together, these directions outline a flexible\nand extensible framework for advancing time series reasoning, aiming to deliver\ninterpretable and trustworthy temporal intelligence across diverse domains.",
      "authors": [
        {
          "name": "Kanghui Ning",
          "affiliation": null
        },
        {
          "name": "Zijie Pan",
          "affiliation": null
        },
        {
          "name": "Yushan Jiang",
          "affiliation": null
        },
        {
          "name": "Anderson Schneider",
          "affiliation": null
        },
        {
          "name": "Yuriy Nevmyvaka",
          "affiliation": null
        },
        {
          "name": "Dongjin Song",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16980v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16980v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16974v1",
      "title": "Differentially Private Linear Regression and Synthetic Data Generation with Statistical Guarantees",
      "abstract": "In social sciences, small- to medium-scale datasets are common and linear\nregression (LR) is canonical. In privacy-aware settings, much work has focused\non differentially private (DP) LR, but mostly on point estimation with limited\nattention to uncertainty quantification. Meanwhile, synthetic data generation\n(SDG) is increasingly important for reproducibility studies, yet current DP LR\nmethods do not readily support it. Mainstream SDG approaches are either\ntailored to discretized data, making them less suitable for continuous\nregression, or rely on deep models that require large datasets, limiting their\nuse for the smaller, continuous data typical in social science. We propose a\nmethod for LR with valid inference under Gaussian DP: a DP bias-corrected\nestimator with asymptotic confidence intervals (CIs) and a general SDG\nprocedure in which regression on the synthetic data matches our DP regression.\nOur binning-aggregation strategy is effective in small- to moderate-dimensional\nsettings. Experiments show our method (1) improves accuracy over existing\nmethods, (2) provides valid CIs, and (3) produces more reliable synthetic data\nfor downstream ML tasks than current DP SDGs.",
      "authors": [
        {
          "name": "Shurong Lin",
          "affiliation": null
        },
        {
          "name": "Aleksandra Slavkovi\u0107",
          "affiliation": null
        },
        {
          "name": "Deekshith Reddy Bhoomireddy",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16974v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16974v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16973v1",
      "title": "Foundation Models in Medical Image Analysis: A Systematic Review and Meta-Analysis",
      "abstract": "Recent advancements in artificial intelligence (AI), particularly foundation\nmodels (FMs), have revolutionized medical image analysis, demonstrating strong\nzero- and few-shot performance across diverse medical imaging tasks, from\nsegmentation to report generation. Unlike traditional task-specific AI models,\nFMs leverage large corpora of labeled and unlabeled multimodal datasets to\nlearn generalized representations that can be adapted to various downstream\nclinical applications with minimal fine-tuning. However, despite the rapid\nproliferation of FM research in medical imaging, the field remains fragmented,\nlacking a unified synthesis that systematically maps the evolution of\narchitectures, training paradigms, and clinical applications across modalities.\nTo address this gap, this review article provides a comprehensive and\nstructured analysis of FMs in medical image analysis. We systematically\ncategorize studies into vision-only and vision-language FMs based on their\narchitectural foundations, training strategies, and downstream clinical tasks.\nAdditionally, a quantitative meta-analysis of the studies was conducted to\ncharacterize temporal trends in dataset utilization and application domains. We\nalso critically discuss persistent challenges, including domain adaptation,\nefficient fine-tuning, computational constraints, and interpretability along\nwith emerging solutions such as federated learning, knowledge distillation, and\nadvanced prompting. Finally, we identify key future research directions aimed\nat enhancing the robustness, explainability, and clinical integration of FMs,\nthereby accelerating their translation into real-world medical practice.",
      "authors": [
        {
          "name": "Praveenbalaji Rajendran",
          "affiliation": null
        },
        {
          "name": "Mojtaba Safari",
          "affiliation": null
        },
        {
          "name": "Wenfeng He",
          "affiliation": null
        },
        {
          "name": "Mingzhe Hu",
          "affiliation": null
        },
        {
          "name": "Shansong Wang",
          "affiliation": null
        },
        {
          "name": "Jun Zhou",
          "affiliation": null
        },
        {
          "name": "Xiaofeng Yang",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "cs.AI",
        "physics.med-ph"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16973v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16973v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16968v1",
      "title": "Leave It to the Experts: Detecting Knowledge Distillation via MoE Expert Signatures",
      "abstract": "Knowledge Distillation (KD) accelerates training of large language models\n(LLMs) but poses intellectual property protection and LLM diversity risks.\nExisting KD detection methods based on self-identity or output similarity can\nbe easily evaded through prompt engineering. We present a KD detection\nframework effective in both white-box and black-box settings by exploiting an\noverlooked signal: the transfer of MoE \"structural habits\", especially internal\nrouting patterns. Our approach analyzes how different experts specialize and\ncollaborate across various inputs, creating distinctive fingerprints that\npersist through the distillation process. To extend beyond the white-box setup\nand MoE architectures, we further propose Shadow-MoE, a black-box method that\nconstructs proxy MoE representations via auxiliary distillation to compare\nthese patterns between arbitrary model pairs. We establish a comprehensive,\nreproducible benchmark that offers diverse distilled checkpoints and an\nextensible framework to facilitate future research. Extensive experiments\ndemonstrate >94% detection accuracy across various scenarios and strong\nrobustness to prompt-based evasion, outperforming existing baselines while\nhighlighting the structural habits transfer in LLMs.",
      "authors": [
        {
          "name": "Pingzhi Li",
          "affiliation": null
        },
        {
          "name": "Morris Yu-Chao Huang",
          "affiliation": null
        },
        {
          "name": "Zhen Tan",
          "affiliation": null
        },
        {
          "name": "Qingquan Song",
          "affiliation": null
        },
        {
          "name": "Jie Peng",
          "affiliation": null
        },
        {
          "name": "Kai Zou",
          "affiliation": null
        },
        {
          "name": "Yu Cheng",
          "affiliation": null
        },
        {
          "name": "Kaidi Xu",
          "affiliation": null
        },
        {
          "name": "Tianlong Chen",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16968v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16968v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16958v1",
      "title": "Quantile Regression, Variational Autoencoders, and Diffusion Models for Uncertainty Quantification: A Spatial Analysis of Sub-seasonal Wind Speed Prediction",
      "abstract": "This study aims to improve the spatial representation of uncertainties when\nregressing surface wind speeds from large-scale atmospheric predictors for\nsub-seasonal forecasting. Sub-seasonal forecasting often relies on large-scale\natmospheric predictors such as 500 hPa geopotential height (Z500), which\nexhibit higher predictability than surface variables and can be downscaled to\nobtain more localised information. Previous work by Tian et al. (2024)\ndemonstrated that stochastic perturbations based on model residuals can improve\nensemble dispersion representation in statistical downscaling frameworks, but\nthis method fails to represent spatial correlations and physical consistency\nadequately. More sophisticated approaches are needed to capture the complex\nrelationships between large-scale predictors and local-scale predictands while\nmaintaining physical consistency. Probabilistic deep learning models offer\npromising solutions for capturing complex spatial dependencies. This study\nevaluates three probabilistic methods with distinct uncertainty quantification\nmechanisms: Quantile Regression Neural Network that directly models\ndistribution quantiles, Variational Autoencoders that leverage latent space\nsampling, and Diffusion Models that utilise iterative denoising. These models\nare trained on ERA5 reanalysis data and applied to ECMWF sub-seasonal hindcasts\nto regress probabilistic wind speed ensembles. Our results show that\nprobabilistic downscaling approaches provide more realistic spatial uncertainty\nrepresentations compared to simpler stochastic methods, with each probabilistic\nmodel offering different strengths in terms of ensemble dispersion,\ndeterministic skill, and physical consistency. These findings establish\nprobabilistic downscaling as an effective enhancement to operational\nsub-seasonal wind forecasts for renewable energy planning and risk assessment.",
      "authors": [
        {
          "name": "Ganglin Tian",
          "affiliation": null
        },
        {
          "name": "Anastase Alexandre Charantonis",
          "affiliation": null
        },
        {
          "name": "Camille Le Coz",
          "affiliation": null
        },
        {
          "name": "Alexis Tantet",
          "affiliation": null
        },
        {
          "name": "Riwal Plougonven",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16958v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16958v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16956v1",
      "title": "A Comparative User Evaluation of XRL Explanations using Goal Identification",
      "abstract": "Debugging is a core application of explainable reinforcement learning (XRL)\nalgorithms; however, limited comparative evaluations have been conducted to\nunderstand their relative performance. We propose a novel evaluation\nmethodology to test whether users can identify an agent's goal from an\nexplanation of its decision-making. Utilising the Atari's Ms. Pacman\nenvironment and four XRL algorithms, we find that only one achieved greater\nthan random accuracy for the tested goals and that users were generally\noverconfident in their selections. Further, we find that users' self-reported\nease of identification and understanding for every explanation did not\ncorrelate with their accuracy.",
      "authors": [
        {
          "name": "Mark Towers",
          "affiliation": null
        },
        {
          "name": "Yali Du",
          "affiliation": null
        },
        {
          "name": "Christopher Freeman",
          "affiliation": null
        },
        {
          "name": "Timothy J. Norman",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.AI"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16956v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16956v1",
      "primary_category": "cs.AI",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16952v1",
      "title": "Real-Time World Crafting: Generating Structured Game Behaviors from Natural Language with Large Language Models",
      "abstract": "We present a novel architecture for safely integrating Large Language Models\n(LLMs) into interactive game engines, allowing players to \"program\" new\nbehaviors using natural language. Our framework mitigates risks by using an LLM\nto translate commands into a constrained Domain-Specific Language (DSL), which\nconfigures a custom Entity-Component-System (ECS) at runtime. We evaluated this\nsystem in a 2D spell-crafting game prototype by experimentally assessing models\nfrom the Gemini, GPT, and Claude families with various prompting strategies. A\nvalidated LLM judge qualitatively rated the outputs, showing that while larger\nmodels better captured creative intent, the optimal prompting strategy is\ntask-dependent: Chain-of-Thought improved creative alignment, while few-shot\nexamples were necessary to generate more complex DSL scripts. This work offers\na validated LLM-ECS pattern for emergent gameplay and a quantitative\nperformance comparison for developers.",
      "authors": [
        {
          "name": "Austin Drake",
          "affiliation": null
        },
        {
          "name": "Hang Dong",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.HC",
        "cs.CL",
        "H.5.2; I.2.7"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16952v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16952v1",
      "primary_category": "cs.HC",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16948v1",
      "title": "Unlocking Off-the-Grid Sparse Recovery with Unlimited Sensing: Simultaneous Super-Resolution in Time and Amplitude",
      "abstract": "The recovery of Dirac impulses, or spikes, from filtered measurements is a\nclassical problem in signal processing. As the spikes lie in the continuous\ndomain while measurements are discrete, this task is known as super-resolution\nor off-the-grid sparse recovery. Despite significant theoretical and\nalgorithmic advances over the past decade, these developments often overlook\ncritical challenges at the analog-digital interface. In particular, when spikes\nexhibit strong-weak amplitude disparity, conventional digital acquisition may\nresult in clipping of strong components or loss of weak ones beneath the\nquantization noise floor. This motivates a broader perspective:\nsuper-resolution must simultaneously resolve both amplitude and temporal\nstructure. Under a fixed bit budget, such information loss is unavoidable. In\ncontrast, the emerging theory and practice of the Unlimited Sensing Framework\n(USF) demonstrate that these fundamental limitations can be overcome. Building\non this foundation, we demonstrate that modulo encoding within USF enables\ndigital super-resolution by enhancing measurement precision, thereby unlocking\ntemporal super-resolution beyond conventional limits. We develop new\ntheoretical results that extend to non-bandlimited kernels commonly encountered\nin practice and introduce a robust algorithm for off-the-grid sparse recovery.\nTo demonstrate practical impact, we instantiate our framework in the context of\ntime-of-flight imaging. Both numerical simulations and hardware experiments\nvalidate the effectiveness of our approach under low-bit quantization, enabling\nsuper-resolution in amplitude and time.",
      "authors": [
        {
          "name": "Ruiming Guo",
          "affiliation": null
        },
        {
          "name": "Ayush Bhandari",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.IT",
        "cs.CV",
        "eess.SP",
        "math.IT"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16948v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16948v1",
      "primary_category": "cs.IT",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16943v1",
      "title": "Peering Inside the Black Box: Uncovering LLM Errors in Optimization Modelling through Component-Level Evaluation",
      "abstract": "Large language models (LLMs) are increasingly used to convert natural\nlanguage descriptions into mathematical optimization formulations. Current\nevaluations often treat formulations as a whole, relying on coarse metrics like\nsolution accuracy or runtime, which obscure structural or numerical errors. In\nthis study, we present a comprehensive, component-level evaluation framework\nfor LLM-generated formulations. Beyond the conventional optimality gap, our\nframework introduces metrics such as precision and recall for decision\nvariables and constraints, constraint and objective root mean squared error\n(RMSE), and efficiency indicators based on token usage and latency. We evaluate\nGPT-5, LLaMA 3.1 Instruct, and DeepSeek Math across optimization problems of\nvarying complexity under six prompting strategies. Results show that GPT-5\nconsistently outperforms other models, with chain-of-thought, self-consistency,\nand modular prompting proving most effective. Analysis indicates that solver\nperformance depends primarily on high constraint recall and low constraint\nRMSE, which together ensure structural correctness and solution reliability.\nConstraint precision and decision variable metrics play secondary roles, while\nconcise outputs enhance computational efficiency. These findings highlight\nthree principles for NLP-to-optimization modeling: (i) Complete constraint\ncoverage prevents violations, (ii) minimizing constraint RMSE ensures\nsolver-level accuracy, and (iii) concise outputs improve computational\nefficiency. The proposed framework establishes a foundation for fine-grained,\ndiagnostic evaluation of LLMs in optimization modeling.",
      "authors": [
        {
          "name": "Dania Refai",
          "affiliation": null
        },
        {
          "name": "Moataz Ahmed",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16943v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16943v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16940v1",
      "title": "A Primer on Kolmogorov-Arnold Networks (KANs) for Probabilistic Time Series Forecasting",
      "abstract": "This work introduces Probabilistic Kolmogorov-Arnold Network (P-KAN), a novel\nprobabilistic extension of Kolmogorov-Arnold Networks (KANs) for time series\nforecasting. By replacing scalar weights with spline-based functional\nconnections and directly parameterizing predictive distributions, P-KANs offer\nexpressive yet parameter-efficient models capable of capturing nonlinear and\nheavy-tailed dynamics. We evaluate P-KANs on satellite traffic forecasting,\nwhere uncertainty-aware predictions enable dynamic thresholding for resource\nallocation. Results show that P-KANs consistently outperform Multi Layer\nPerceptron (MLP) baselines in both accuracy and calibration, achieving superior\nefficiency-risk trade-offs while using significantly fewer parameters. We build\nup P-KANs on two distributions, namely Gaussian and Student-t distributions.\nThe Gaussian variant provides robust, conservative forecasts suitable for\nsafety-critical scenarios, whereas the Student-t variant yields sharper\ndistributions that improve efficiency under stable demand. These findings\nestablish P-KANs as a powerful framework for probabilistic forecasting with\ndirect applicability to satellite communications and other resource-constrained\ndomains.",
      "authors": [
        {
          "name": "Cristian J. Vaca-Rubio",
          "affiliation": null
        },
        {
          "name": "Roberto Pereira",
          "affiliation": null
        },
        {
          "name": "Luis Blanco",
          "affiliation": null
        },
        {
          "name": "Engin Zeydan",
          "affiliation": null
        },
        {
          "name": "M\u00e0rius Caus",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SP"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16940v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16940v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16938v1",
      "title": "A Topological Approach to Parameterizing Deep Hedging Networks",
      "abstract": "Deep hedging uses recurrent neural networks to hedge financial products that\ncannot be fully hedged in incomplete markets. Previous work in this area\nfocuses on minimizing some measure of quadratic hedging error by calculating\npathwise gradients, but doing so requires large batch sizes and can make\ntraining effective models in a reasonable amount of time challenging. We show\nthat by adding certain topological features, we can reduce batch sizes\nsubstantially and make training these models more practically feasible without\ngreatly compromising hedging performance.",
      "authors": [
        {
          "name": "Alok Das",
          "affiliation": null
        },
        {
          "name": "Kiseop Lee",
          "affiliation": null
        }
      ],
      "categories": [
        "q-fin.MF",
        "cs.LG"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16938v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16938v1",
      "primary_category": "q-fin.MF",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16937v1",
      "title": "Prediction-Augmented Trees for Reliable Statistical Inference",
      "abstract": "The remarkable success of machine learning (ML) in predictive tasks has led\nscientists to incorporate ML predictions as a core component of the scientific\ndiscovery pipeline. This was exemplified by the landmark achievement of\nAlphaFold (Jumper et al. (2021)). In this paper, we study how ML predictions\ncan be safely used in statistical analysis of data towards scientific\ndiscovery. In particular, we follow the framework introduced by Angelopoulos et\nal. (2023). In this framework, we assume access to a small set of $n$\ngold-standard labeled samples, a much larger set of $N$ unlabeled samples, and\na ML model that can be used to impute the labels of the unlabeled data points.\nWe introduce two new learning-augmented estimators: (1) Prediction-Augmented\nResidual Tree (PART), and (2) Prediction-Augmented Quadrature (PAQ). Both\nestimators have significant advantages over existing estimators like PPI and\nPPI++ introduced by Angelopoulos et al. (2023) and Angelopoulos et al. (2024),\nrespectively. PART is a decision-tree based estimator built using a greedy\ncriterion. We first characterize PART's asymptotic distribution and demonstrate\nhow to construct valid confidence intervals. Then we show that PART outperforms\nexisting methods in real-world datasets from ecology, astronomy, and census\nreports, among other domains. This leads to estimators with higher confidence,\nwhich is the result of using both the gold-standard samples and the machine\nlearning predictions. Finally, we provide a formal proof of the advantage of\nPART by exploring PAQ, an estimation that arises when considering the limit of\nPART when the depth its tree grows to infinity. Under appropriate assumptions\nin the input data we show that the variance of PAQ shrinks at rate of $O(N^{-1}\n+ n^{-4})$, improving significantly on the $O(N^{-1}+n^{-1})$ rate of existing\nmethods.",
      "authors": [
        {
          "name": "Vikram Kher",
          "affiliation": null
        },
        {
          "name": "Argyris Oikonomou",
          "affiliation": null
        },
        {
          "name": "Manolis Zampetakis",
          "affiliation": null
        }
      ],
      "categories": [
        "stat.ML",
        "cs.LG",
        "stat.ME",
        "G.3"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16937v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16937v1",
      "primary_category": "stat.ML",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16933v1",
      "title": "Tutoring LLM into a Better CUDA Optimizer",
      "abstract": "Recent leaps in large language models (LLMs) caused a revolution in\nprogramming tools (like GitHub Copilot) that can help with code generation,\ndebugging, and even performance optimization. In this paper, we focus on the\ncapabilities of the most recent reasoning models to generate optimized CUDA\ncode for predefined, well-known tasks. Our objective is to determine which\ntypes of code optimizations and parallel patterns the LLMs can perform by\nthemselves and whether they can be improved by tutoring (providing more\ndetailed hints and guidelines in the prompt). The generated solutions were\nevaluated both automatically (for correctness and speedup) and manually (code\nreviews) to provide a more detailed perspective. We also tried an interactive\napproach where the LLM can fix its previous mistakes within a session. The\nresults indicate that LLMs are quite skilled coders; however, they require\ntutoring to reach optimized solutions provided by parallel computing experts.",
      "authors": [
        {
          "name": "Maty\u00e1\u0161 Brabec",
          "affiliation": null
        },
        {
          "name": "Ji\u0159\u00ed Klepl",
          "affiliation": null
        },
        {
          "name": "Michal T\u00f6pfer",
          "affiliation": null
        },
        {
          "name": "Martin Kruli\u0161",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16933v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16933v1",
      "primary_category": "cs.DC",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16932v1",
      "title": "Prompt-MII: Meta-Learning Instruction Induction for LLMs",
      "abstract": "A popular method to adapt large language models (LLMs) to new tasks is\nin-context learning (ICL), which is effective but incurs high inference costs\nas context length grows. In this paper we propose a method to perform\ninstruction induction, where we take training examples and reduce them to a\ncompact but descriptive prompt that can achieve performance comparable to ICL\nover the full training set. Specifically, we propose PROMPT-MII, a\nreinforcement learning (RL) based framework to meta-learn an instruction\ninduction model that can generate compact instructions on the fly for an\narbitrary new dataset. We train on over 3,000 diverse classification datasets\nfrom the HuggingFace hub, and evaluate on 90 unseen tasks. PROMPT-MII improves\ndownstream model quality by 4-9 F1 points (10-20% relative), matching ICL\nperformance while requiring 3-13x fewer tokens.",
      "authors": [
        {
          "name": "Emily Xiao",
          "affiliation": null
        },
        {
          "name": "Yixiao Zeng",
          "affiliation": null
        },
        {
          "name": "Ada Chen",
          "affiliation": null
        },
        {
          "name": "Chin-Jou Li",
          "affiliation": null
        },
        {
          "name": "Amanda Bertsch",
          "affiliation": null
        },
        {
          "name": "Graham Neubig",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16932v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16932v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16928v1",
      "title": "ChiKhaPo: A Large-Scale Multilingual Benchmark for Evaluating Lexical Comprehension and Generation in Large Language Models",
      "abstract": "Existing benchmarks for large language models (LLMs) are largely restricted\nto high- or mid-resource languages, and often evaluate performance on\nhigher-order tasks in reasoning and generation. However, plenty of evidence\npoints to the fact that LLMs lack basic linguistic competence in the vast\nmajority of the world's 3800+ written languages. We introduce ChiKhaPo,\nconsisting of 8 subtasks of varying difficulty designed to evaluate the lexical\ncomprehension and generation abilities of generative models. ChiKhaPo draws on\nexisting lexicons, monolingual data, and bitext, and provides coverage for\n2700+ languages for 2 subtasks, surpassing any existing benchmark in terms of\nlanguage coverage. We further show that 6 SOTA models struggle on our\nbenchmark, and discuss the factors contributing to performance scores,\nincluding language family, language resourcedness, task, and comprehension\nversus generation directions. With ChiKhaPo, we hope to enable and encourage\nthe massively multilingual benchmarking of LLMs.",
      "authors": [
        {
          "name": "Emily Chang",
          "affiliation": null
        },
        {
          "name": "Niyati Bafna",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16928v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16928v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16927v1",
      "title": "Closing the Curvature Gap: Full Transformer Hessians and Their Implications for Scaling Laws",
      "abstract": "The lack of theoretical results for Layer Normalization and feedforward\nHessians has left a gap in the study of Transformer optimization landscapes. We\naddress this by deriving explicit second-order expressions for these\ncomponents, thereby completing the Hessian characterization of full Transformer\nblocks. Our results generalize prior self-attention analyses and yield\nestimations for the role of each sublayer in curvature propagation. We\ndemonstrate how these Hessian structures inform both convergence dynamics and\nthe empirical scaling laws governing large-model performance. Further, we\npropose a Taylor-expansion-based framework for analyzing loss differences to\nquantify convergence trajectories. By extending Hessian theory to the full\nTransformer architecture, this work establishes a new foundation for\ntheoretical and empirical investigations of optimization in large-scale deep\nlearning.",
      "authors": [
        {
          "name": "Egor Petrov",
          "affiliation": null
        },
        {
          "name": "Nikita Kiselev",
          "affiliation": null
        },
        {
          "name": "Vladislav Meshkov",
          "affiliation": null
        },
        {
          "name": "Andrey Grabovoy",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "I.2.6; I.2.7; G.1.3"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16927v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16927v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16926v1",
      "title": "Res-Bench: Benchmarking the Robustness of Multimodal Large Language Models to Dynamic Resolution Input",
      "abstract": "Multimodal Large Language Models (MLLMs) increasingly support dynamic image\nresolutions. However, current evaluation paradigms primarily assess semantic\nperformance, overlooking the critical question of resolution robustness -\nwhether performance remains stable across varying input resolutions. To address\nthis gap, we introduce \\textbf{Res-Bench}, a comprehensive benchmark comprising\n14,400 samples across 12 resolution levels and six core capability dimensions.\nWe designed a novel evaluation framework that goes beyond traditional accuracy\nmetrics to capture performance stability. This framework introduces multiple\nrobustness metrics: Spearman's correlation for assessing resolution-performance\ntrends, and Absolute/Relative Continuous Error (ACE/RCE) for measuring\nperformance volatility. Using these metrics, we conducted a large-scale\nevaluation of leading MLLMs. Our analysis encompasses: (1) model-centric and\ntask-centric robustness examination, (2) investigation of preprocessing\nstrategies including padding and super-resolution, and (3) exploration of\nfine-tuning for stability enhancement.",
      "authors": [
        {
          "name": "Chenxu Li",
          "affiliation": null
        },
        {
          "name": "Zhicai Wang",
          "affiliation": null
        },
        {
          "name": "Yuan Sheng",
          "affiliation": null
        },
        {
          "name": "Xingyu Zhu",
          "affiliation": null
        },
        {
          "name": "Yanbin Hao",
          "affiliation": null
        },
        {
          "name": "Xiang Wang",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "cs.CL"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16926v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16926v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16924v1",
      "title": "Does Visual Grounding Enhance the Understanding of Embodied Knowledge in Large Language Models?",
      "abstract": "Despite significant progress in multimodal language models (LMs), it remains\nunclear whether visual grounding enhances their understanding of embodied\nknowledge compared to text-only models. To address this question, we propose a\nnovel embodied knowledge understanding benchmark based on the perceptual theory\nfrom psychology, encompassing visual, auditory, tactile, gustatory, olfactory\nexternal senses, and interoception. The benchmark assesses the models'\nperceptual abilities across different sensory modalities through vector\ncomparison and question-answering tasks with over 1,700 questions. By comparing\n30 state-of-the-art LMs, we surprisingly find that vision-language models\n(VLMs) do not outperform text-only models in either task. Moreover, the models\nperform significantly worse in the visual dimension compared to other sensory\ndimensions. Further analysis reveals that the vector representations are easily\ninfluenced by word form and frequency, and the models struggle to answer\nquestions involving spatial perception and reasoning. Our findings underscore\nthe need for more effective integration of embodied knowledge in LMs to enhance\ntheir understanding of the physical world.",
      "authors": [
        {
          "name": "Zhihui Yang",
          "affiliation": null
        },
        {
          "name": "Yupei Wang",
          "affiliation": null
        },
        {
          "name": "Kaijie Mo",
          "affiliation": null
        },
        {
          "name": "Zhe Zhao",
          "affiliation": null
        },
        {
          "name": "Renfen Hu",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16924v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16924v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16923v1",
      "title": "UNDREAM: Bridging Differentiable Rendering and Photorealistic Simulation for End-to-end Adversarial Attacks",
      "abstract": "Deep learning models deployed in safety critical applications like autonomous\ndriving use simulations to test their robustness against adversarial attacks in\nrealistic conditions. However, these simulations are non-differentiable,\nforcing researchers to create attacks that do not integrate simulation\nenvironmental factors, reducing attack success. To address this limitation, we\nintroduce UNDREAM, the first software framework that bridges the gap between\nphotorealistic simulators and differentiable renderers to enable end-to-end\noptimization of adversarial perturbations on any 3D objects. UNDREAM enables\nmanipulation of the environment by offering complete control over weather,\nlighting, backgrounds, camera angles, trajectories, and realistic human and\nobject movements, thereby allowing the creation of diverse scenes. We showcase\na wide array of distinct physically plausible adversarial objects that UNDREAM\nenables researchers to swiftly explore in different configurable environments.\nThis combination of photorealistic simulation and differentiable optimization\nopens new avenues for advancing research of physical adversarial attacks.",
      "authors": [
        {
          "name": "Mansi Phute",
          "affiliation": null
        },
        {
          "name": "Matthew Hull",
          "affiliation": null
        },
        {
          "name": "Haoran Wang",
          "affiliation": null
        },
        {
          "name": "Alec Helbling",
          "affiliation": null
        },
        {
          "name": "ShengYun Peng",
          "affiliation": null
        },
        {
          "name": "Willian Lunardi",
          "affiliation": null
        },
        {
          "name": "Martin Andreoni",
          "affiliation": null
        },
        {
          "name": "Wenke Lee",
          "affiliation": null
        },
        {
          "name": "Polo Chau",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16923v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16923v1",
      "primary_category": "cs.CR",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16917v1",
      "title": "SAKE: Towards Editing Auditory Attribute Knowledge of Large Audio-Language Models",
      "abstract": "Knowledge editing offers an efficient way to update model knowledge without\nfull retraining, but prior work has concentrated almost exclusively on textual\nor visual modalities. We introduce SAKE, the first benchmark specifically\ndesigned for editing auditory attribute knowledge in Large Audio-Language\nModels (LALMs). Unlike factual updates, SAKE targets several abstract auditory\nattributes, capturing knowledge types that go beyond conventional textual and\nvisual domains. We benchmark seven editing methods on two LALMs along four\ndimensions: reliability, generality, audio/text locality, and portability.\nResults highlight challenges such as preserving intra-attribute knowledge\nunrelated to the edit, generalizing edits to multimodal reasoning, and\nmaintaining edits under sequential updates. SAKE provides a principled\nframework to study how knowledge editing extends to the auditory modalities,\nopening new directions for maintaining and adapting LALMs in more diverse\nreal-world scenarios.",
      "authors": [
        {
          "name": "Chih-Kai Yang",
          "affiliation": null
        },
        {
          "name": "Yen-Ting Piao",
          "affiliation": null
        },
        {
          "name": "Tzu-Wen Hsu",
          "affiliation": null
        },
        {
          "name": "Szu-Wei Fu",
          "affiliation": null
        },
        {
          "name": "Zhehuai Chen",
          "affiliation": null
        },
        {
          "name": "Ke-Han Lu",
          "affiliation": null
        },
        {
          "name": "Sung-Feng Huang",
          "affiliation": null
        },
        {
          "name": "Chao-Han Huck Yang",
          "affiliation": null
        },
        {
          "name": "Yu-Chiang Frank Wang",
          "affiliation": null
        },
        {
          "name": "Yun-Nung Chen",
          "affiliation": null
        },
        {
          "name": "Hung-yi Lee",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "eess.AS"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16917v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16917v1",
      "primary_category": "cs.SD",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16916v1",
      "title": "SolverLLM: Leveraging Test-Time Scaling for Optimization Problem via LLM-Guided Search",
      "abstract": "Large Language Models (LLMs) offer promising capabilities for tackling\ncomplex reasoning tasks, including optimization problems. However, existing\nmethods either rely on prompt engineering, which leads to poor generalization\nacross problem types, or require costly supervised training. We introduce\nSolverLLM, a training-free framework that leverages test-time scaling to solve\ndiverse optimization problems. Rather than solving directly, SolverLLM\ngenerates mathematical formulations and translates them into solver-ready code,\nguided by a novel Monte Carlo Tree Search (MCTS) strategy. To enhance the\nsearch process, we modify classical MCTS with (1) dynamic expansion for\nadaptive formulation generation, (2) prompt backpropagation to guide\nexploration via outcome-driven feedback, and (3) uncertainty backpropagation to\nincorporate reward reliability into decision-making. Experiments on six\nstandard benchmark datasets demonstrate that SolverLLM outperforms both\nprompt-based and learning-based baselines, achieving strong generalization\nwithout additional training.",
      "authors": [
        {
          "name": "Dong Li",
          "affiliation": null
        },
        {
          "name": "Xujiang Zhao",
          "affiliation": null
        },
        {
          "name": "Linlin Yu",
          "affiliation": null
        },
        {
          "name": "Yanchi Liu",
          "affiliation": null
        },
        {
          "name": "Wei Cheng",
          "affiliation": null
        },
        {
          "name": "Zhengzhang Chen",
          "affiliation": null
        },
        {
          "name": "Zhong Chen",
          "affiliation": null
        },
        {
          "name": "Feng Chen",
          "affiliation": null
        },
        {
          "name": "Chen Zhao",
          "affiliation": null
        },
        {
          "name": "Haifeng Chen",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16916v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16916v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16914v1",
      "title": "Domain Generalizable Continual Learning",
      "abstract": "To adapt effectively to dynamic real-world environments, intelligent systems\nmust continually acquire new skills while generalizing them to diverse, unseen\nscenarios. Here, we introduce a novel and realistic setting named domain\ngeneralizable continual learning (DGCL): a model learns sequential tasks with\neach involving a single domain, aiming to perform well across all encountered\ntasks and domains. This setting poses unique challenges in acquiring,\nretaining, and leveraging both semantic- and domain-relevant information for\nrobust generalization. Although state-of-the-art continual learning (CL)\nmethods have employed pre-trained models (PTMs) to enhance task-specific\ngeneralization, they typically assume identical training and testing domains\nfor each task and therefore perform poorly in DGCL. To this end, we propose\nadaptive Domain Transformation (DoT), an innovative PTMs-based approach\ntailored to DGCL. Inspired by the distributed-plus-hub theory of the human\nbrain, DoT disentangles semantic- and domain-relevant information in\nrepresentation learning, and adaptively transforms task representations across\nvarious domains for output alignment, ensuring balanced and generalized\npredictions. DoT serves as a plug-in strategy that greatly facilitates\nstate-of-the-art CL baselines under both full parameter tuning and\nparameter-efficient tuning paradigms in DGCL, validated by extensive\nexperiments. Also, DoT is shown to accumulate domain-generalizable knowledge\nfrom DGCL, and ensure resource efficiency with a lightweight implementation.",
      "authors": [
        {
          "name": "Hongwei Yan",
          "affiliation": null
        },
        {
          "name": "Guanglong Sun",
          "affiliation": null
        },
        {
          "name": "Zhiqi Kang",
          "affiliation": null
        },
        {
          "name": "Yi Zhong",
          "affiliation": null
        },
        {
          "name": "Liyuan Wang",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16914v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16914v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16913v1",
      "title": "Beyond RGB: Leveraging Vision Transformers for Thermal Weapon Segmentation",
      "abstract": "Thermal weapon segmentation is crucial for surveillance and security\napplications, enabling robust detection under lowlight and visually obscured\nconditions where RGB-based systems fail. While convolutional neural networks\n(CNNs) dominate thermal segmentation literature, their ability to capture\nlong-range dependencies and fine structural details is limited. Vision\nTransformers (ViTs), with their global context modeling capabilities, have\nachieved state-of-the-art results in RGB segmentation tasks, yet their\npotential in thermal weapon segmentation remains underexplored. This work\nadapts and evaluates four transformer-based architectures SegFormer,\nDeepLabV3\\+, SegNeXt, and Swin Transformer for binary weapon segmentation on a\ncustom thermal dataset comprising 9,711 images collected from real world\nsurveillance videos and automatically annotated using SAM2. We employ standard\naugmentation strategies within the MMSegmentation framework to ensure robust\nmodel training and fair architectural comparison. Experimental results\ndemonstrate significant improvements in segmentation performance: SegFormer-b5\nachieves the highest mIoU (94.15\\%) and Pixel Accuracy (97.04\\%), while\nSegFormer-b0 provides the fastest inference speed (98.32 FPS) with competitive\nmIoU (90.84\\%). SegNeXt-mscans offers balanced performance with 85.12 FPS and\n92.24\\% mIoU, and DeepLabV3\\+ R101-D8 reaches 92.76\\% mIoU at 29.86 FPS. The\ntransformer architectures demonstrate robust generalization capabilities for\nweapon detection in low-light and occluded thermal environments, with flexible\naccuracy-speed trade-offs suitable for diverse real-time security applications.",
      "authors": [
        {
          "name": "Akhila Kambhatla",
          "affiliation": null
        },
        {
          "name": "Ahmed R Khaled",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "68T07, 68U10, 68U35",
        "I.2.10; I.4.8; I.4.9"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16913v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16913v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16911v1",
      "title": "A Lightweight DL Model for Smart Grid Power Forecasting with Feature and Resolution Mismatch",
      "abstract": "How can short-term energy consumption be accurately forecasted when sensor\ndata is noisy, incomplete, and lacks contextual richness? This question guided\nour participation in the \\textit{2025 Competition on Electric Energy\nConsumption Forecast Adopting Multi-criteria Performance Metrics}, which\nchallenged teams to predict next-day power demand using real-world\nhigh-frequency data. We proposed a robust yet lightweight Deep Learning (DL)\npipeline combining hourly downsizing, dual-mode imputation (mean and polynomial\nregression), and comprehensive normalization, ultimately selecting Standard\nScaling for optimal balance. The lightweight GRU-LSTM sequence-to-one model\nachieves an average RMSE of 601.9~W, MAE of 468.9~W, and 84.36\\% accuracy.\nDespite asymmetric inputs and imputed gaps, it generalized well, captured\nnonlinear demand patterns, and maintained low inference latency. Notably,\nspatiotemporal heatmap analysis reveals a strong alignment between temperature\ntrends and predicted consumption, further reinforcing the model's reliability.\nThese results demonstrate that targeted preprocessing paired with compact\nrecurrent architectures can still enable fast, accurate, and deployment-ready\nenergy forecasting in real-world conditions.",
      "authors": [
        {
          "name": "Sarah Al-Shareeda",
          "affiliation": null
        },
        {
          "name": "Gulcihan Ozdemir",
          "affiliation": null
        },
        {
          "name": "Heung Seok Jeon",
          "affiliation": null
        },
        {
          "name": "Khaleel Ahmad",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16911v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16911v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16907v1",
      "title": "VAGEN: Reinforcing World Model Reasoning for Multi-Turn VLM Agents",
      "abstract": "A key challenge in training Vision-Language Model (VLM) agents, compared to\nLanguage Model (LLM) agents, lies in the shift from textual states to complex\nvisual observations. This transition introduces partial observability and\ndemands robust world modeling. We ask: Can VLM agents construct internal world\nmodels through explicit visual state reasoning? To address this question, we\narchitecturally enforce and reward the agent's reasoning process via\nreinforcement learning (RL), formulating it as a Partially Observable Markov\nDecision Process (POMDP). We find that decomposing the agent's reasoning into\nState Estimation (\"what is the current state?\") and Transition Modeling (\"what\ncomes next?\") is critical for success, as demonstrated through five reasoning\nstrategies. Our investigation into how agents represent internal beliefs\nreveals that the optimal representation is task-dependent: Natural Language\nexcels at capturing semantic relationships in general tasks, while Structured\nformats are indispensable for precise manipulation and control. Building on\nthese insights, we design a World Modeling Reward that provides dense,\nturn-level supervision for accurate state prediction, and introduce Bi-Level\nGeneral Advantage Estimation (Bi-Level GAE) for turn-aware credit assignment.\nThrough this form of visual state reasoning, a 3B-parameter model achieves a\nscore of 0.82 across five diverse agent benchmarks, representing a 3$\\times$\nimprovement over its untrained counterpart (0.21) and outperforming proprietary\nreasoning models such as GPT-5 (0.75), Gemini 2.5 Pro (0.67) and Claude 4.5\n(0.62). All experiments are conducted within our VAGEN framework, a scalable\nsystem for training and analyzing multi-turn VLM agents in diverse visual\nenvironments. Code and data are publicly available at\nhttps://vagen-ai.github.io.",
      "authors": [
        {
          "name": "Kangrui Wang",
          "affiliation": null
        },
        {
          "name": "Pingyue Zhang",
          "affiliation": null
        },
        {
          "name": "Zihan Wang",
          "affiliation": null
        },
        {
          "name": "Yaning Gao",
          "affiliation": null
        },
        {
          "name": "Linjie Li",
          "affiliation": null
        },
        {
          "name": "Qineng Wang",
          "affiliation": null
        },
        {
          "name": "Hanyang Chen",
          "affiliation": null
        },
        {
          "name": "Chi Wan",
          "affiliation": null
        },
        {
          "name": "Yiping Lu",
          "affiliation": null
        },
        {
          "name": "Zhengyuan Yang",
          "affiliation": null
        },
        {
          "name": "Lijuan Wang",
          "affiliation": null
        },
        {
          "name": "Ranjay Krishna",
          "affiliation": null
        },
        {
          "name": "Jiajun Wu",
          "affiliation": null
        },
        {
          "name": "Li Fei-Fei",
          "affiliation": null
        },
        {
          "name": "Yejin Choi",
          "affiliation": null
        },
        {
          "name": "Manling Li",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16907v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16907v1",
      "primary_category": "cs.AI",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16899v1",
      "title": "SNOMED CT-powered Knowledge Graphs for Structured Clinical Data and Diagnostic Reasoning",
      "abstract": "The effectiveness of artificial intelligence (AI) in healthcare is\nsignificantly hindered by unstructured clinical documentation, which results in\nnoisy, inconsistent, and logically fragmented training data. To address this\nchallenge, we present a knowledge-driven framework that integrates the\nstandardized clinical terminology SNOMED CT with the Neo4j graph database to\nconstruct a structured medical knowledge graph. In this graph, clinical\nentities such as diseases, symptoms, and medications are represented as nodes,\nand semantic relationships such as ``caused by,'' ``treats,'' and ``belongs\nto'' are modeled as edges in Neo4j, with types mapped from formal SNOMED CT\nrelationship concepts (e.g., \\texttt{Causative agent}, \\texttt{Indicated for}).\nThis design enables multi-hop reasoning and ensures terminological consistency.\nBy extracting and standardizing entity-relationship pairs from clinical texts,\nwe generate structured, JSON-formatted datasets that embed explicit diagnostic\npathways. These datasets are used to fine-tune large language models (LLMs),\nsignificantly improving the clinical logic consistency of their outputs.\nExperimental results demonstrate that our knowledge-guided approach enhances\nthe validity and interpretability of AI-generated diagnostic reasoning,\nproviding a scalable solution for building reliable AI-assisted clinical\nsystems.",
      "authors": [
        {
          "name": "Dun Liu",
          "affiliation": null
        },
        {
          "name": "Qin Pang",
          "affiliation": null
        },
        {
          "name": "Guangai Liu",
          "affiliation": null
        },
        {
          "name": "Hongyu Mou",
          "affiliation": null
        },
        {
          "name": "Jipeng Fan",
          "affiliation": null
        },
        {
          "name": "Yiming Miao",
          "affiliation": null
        },
        {
          "name": "Pin-Han Ho",
          "affiliation": null
        },
        {
          "name": "Limei Peng",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16899v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16899v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16898v1",
      "title": "Adaptive Online Learning with LSTM Networks for Energy Price Prediction",
      "abstract": "Accurate prediction of electricity prices is crucial for stakeholders in the\nenergy market, particularly for grid operators, energy producers, and\nconsumers. This study focuses on developing a predictive model leveraging Long\nShort-Term Memory (LSTM) networks to forecast day-ahead electricity prices in\nthe California energy market. The model incorporates a variety of features,\nincluding historical price data, weather conditions, and the energy generation\nmix. A novel custom loss function that integrates Mean Absolute Error (MAE),\nJensen-Shannon Divergence (JSD), and a smoothness penalty is introduced to\nenhance the prediction accuracy and interpretability. Additionally, an online\nlearning approach is implemented to allow the model to adapt to new data\nincrementally, ensuring continuous relevance and accuracy. The results\ndemonstrate that the custom loss function can improve the model's performance,\naligning predicted prices more closely with actual values, particularly during\npeak intervals. Also, the online learning model outperforms other models by\neffectively incorporating real-time data, resulting in lower prediction error\nand variability. The inclusion of the energy generation mix further enhances\nthe model's predictive capabilities, highlighting the importance of\ncomprehensive feature integration. This research provides a robust framework\nfor electricity price forecasting, offering valuable insights and tools for\nbetter decision-making in dynamic electricity markets.",
      "authors": [
        {
          "name": "Salih Salihoglu",
          "affiliation": null
        },
        {
          "name": "Ibrahim Ahmed",
          "affiliation": null
        },
        {
          "name": "Afshin Asadi",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16898v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16898v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16897v1",
      "title": "DeepChem Equivariant: SE(3)-Equivariant Support in an Open-Source Molecular Machine Learning Library",
      "abstract": "Neural networks that incorporate geometric relationships respecting SE(3)\ngroup transformations (e.g. rotations and translations) are increasingly\nimportant in molecular applications, such as molecular property prediction,\nprotein structure modeling, and materials design. These models, known as\nSE(3)-equivariant neural networks, ensure outputs transform predictably with\ninput coordinate changes by explicitly encoding spatial atomic positions.\nAlthough libraries such as E3NN [4] and SE(3)-TRANSFORMER [3 ] offer powerful\nimplementations, they often require substantial deep learning or mathematical\nprior knowledge and lack complete training pipelines. We extend DEEPCHEM [ 13]\nwith support for ready-to-use equivariant models, enabling scientists with\nminimal deep learning background to build, train, and evaluate models, such as\nSE(3)-Transformer and Tensor Field Networks. Our implementation includes\nequivariant models, complete training pipelines, and a toolkit of equivariant\nutilities, supported with comprehensive tests and documentation, to facilitate\nboth application and further development of SE(3)-equivariant models.",
      "authors": [
        {
          "name": "Jose Siguenza",
          "affiliation": null
        },
        {
          "name": "Bharath Ramsundar",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16897v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16897v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16893v1",
      "title": "Investigating Safety Vulnerabilities of Large Audio-Language Models Under Speaker Emotional Variations",
      "abstract": "Large audio-language models (LALMs) extend text-based LLMs with auditory\nunderstanding, offering new opportunities for multimodal applications. While\ntheir perception, reasoning, and task performance have been widely studied,\ntheir safety alignment under paralinguistic variation remains underexplored.\nThis work systematically investigates the role of speaker emotion. We construct\na dataset of malicious speech instructions expressed across multiple emotions\nand intensities, and evaluate several state-of-the-art LALMs. Our results\nreveal substantial safety inconsistencies: different emotions elicit varying\nlevels of unsafe responses, and the effect of intensity is non-monotonic, with\nmedium expressions often posing the greatest risk. These findings highlight an\noverlooked vulnerability in LALMs and call for alignment strategies explicitly\ndesigned to ensure robustness under emotional variation, a prerequisite for\ntrustworthy deployment in real-world settings.",
      "authors": [
        {
          "name": "Bo-Han Feng",
          "affiliation": null
        },
        {
          "name": "Chien-Feng Liu",
          "affiliation": null
        },
        {
          "name": "Yu-Hsuan Li Liang",
          "affiliation": null
        },
        {
          "name": "Chih-Kai Yang",
          "affiliation": null
        },
        {
          "name": "Szu-Wei Fu",
          "affiliation": null
        },
        {
          "name": "Zhehuai Chen",
          "affiliation": null
        },
        {
          "name": "Ke-Han Lu",
          "affiliation": null
        },
        {
          "name": "Sung-Feng Huang",
          "affiliation": null
        },
        {
          "name": "Chao-Han Huck Yang",
          "affiliation": null
        },
        {
          "name": "Yu-Chiang Frank Wang",
          "affiliation": null
        },
        {
          "name": "Yun-Nung Chen",
          "affiliation": null
        },
        {
          "name": "Hung-yi Lee",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "eess.AS"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16893v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16893v1",
      "primary_category": "cs.SD",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16891v1",
      "title": "Contrail-to-Flight Attribution Using Ground Visible Cameras and Flight Surveillance Data",
      "abstract": "Aviation's non-CO2 effects, particularly contrails, are a significant\ncontributor to its climate impact. Persistent contrails can evolve into\ncirrus-like clouds that trap outgoing infrared radiation, with radiative\nforcing potentially comparable to or exceeding that of aviation's CO2\nemissions. While physical models simulate contrail formation, evolution and\ndissipation, validating and calibrating these models requires linking observed\ncontrails to the flights that generated them, a process known as\ncontrail-to-flight attribution. Satellite-based attribution is challenging due\nto limited spatial and temporal resolution, as contrails often drift and deform\nbefore detection. In this paper, we evaluate an alternative approach using\nground-based cameras, which capture contrails shortly after formation at high\nspatial and temporal resolution, when they remain thin, linear, and visually\ndistinct. Leveraging the ground visible camera contrail sequences (GVCCS)\ndataset, we introduce a modular framework for attributing contrails observed\nusing ground-based cameras to theoretical contrails derived from aircraft\nsurveillance and meteorological data. The framework accommodates multiple\ngeometric representations and distance metrics, incorporates temporal\nsmoothing, and enables flexible probability-based assignment strategies. This\nwork establishes a strong baseline and provides a modular framework for future\nresearch in linking contrails to their source flight.",
      "authors": [
        {
          "name": "Ramon Dalmau",
          "affiliation": null
        },
        {
          "name": "Gabriel Jarry",
          "affiliation": null
        },
        {
          "name": "Philippe Very",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16891v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16891v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16888v1",
      "title": "Uniworld-V2: Reinforce Image Editing with Diffusion Negative-aware Finetuning and MLLM Implicit Feedback",
      "abstract": "Instruction-based image editing has achieved remarkable progress; however,\nmodels solely trained via supervised fine-tuning often overfit to annotated\npatterns, hindering their ability to explore and generalize beyond training\ndistributions. To this end, we introduce Edit-R1, a novel post-training\nframework for instruction-based image editing based on policy optimization.\nSpecifically, we utilize Diffusion Negative-aware Finetuning (DiffusionNFT), a\nlikelihood-free policy optimization method consistent with the flow matching\nforward process, thereby enabling the use of higher-order samplers and more\nefficient training. Another key challenge here is the absence of a universal\nreward model, resulting from the diverse nature of editing instructions and\ntasks. To bridge this gap, we employ a Multimodal Large Language Model (MLLM)\nas a unified, training-free reward model, leveraging its output logits to\nprovide fine-grained feedback. Furthermore, we carefully design a low-variance\ngroup filtering mechanism to reduce MLLM scoring noise and stabilize\noptimization. UniWorld-V2, trained with this framework, achieves\n\\textbf{state-of-the-art} results on the ImgEdit and GEdit-Bench benchmarks,\nscoring 4.49 and 7.83, respectively. Crucially, our framework is\nmodel-agnostic, delivering substantial performance gains when applied to\ndiverse base models like Qwen-Image-Edit and FLUX-Kontext, demonstrating its\nwide applicability. Code and models are publicly available at\nhttps://github.com/PKU-YuanGroup/UniWorld-V2.",
      "authors": [
        {
          "name": "Zongjian Li",
          "affiliation": null
        },
        {
          "name": "Zheyuan Liu",
          "affiliation": null
        },
        {
          "name": "Qihui Zhang",
          "affiliation": null
        },
        {
          "name": "Bin Lin",
          "affiliation": null
        },
        {
          "name": "Shenghai Yuan",
          "affiliation": null
        },
        {
          "name": "Zhiyuan Yan",
          "affiliation": null
        },
        {
          "name": "Yang Ye",
          "affiliation": null
        },
        {
          "name": "Wangbo Yu",
          "affiliation": null
        },
        {
          "name": "Yuwei Niu",
          "affiliation": null
        },
        {
          "name": "Li Yuan",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16888v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16888v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16887v1",
      "title": "Class-N-Diff: Classification-Induced Diffusion Model Can Make Fair Skin Cancer Diagnosis",
      "abstract": "Generative models, especially Diffusion Models, have demonstrated remarkable\ncapability in generating high-quality synthetic data, including medical images.\nHowever, traditional class-conditioned generative models often struggle to\ngenerate images that accurately represent specific medical categories, limiting\ntheir usefulness for applications such as skin cancer diagnosis. To address\nthis problem, we propose a classification-induced diffusion model, namely,\nClass-N-Diff, to simultaneously generate and classify dermoscopic images. Our\nClass-N-Diff model integrates a classifier within a diffusion model to guide\nimage generation based on its class conditions. Thus, the model has better\ncontrol over class-conditioned image synthesis, resulting in more realistic and\ndiverse images. Additionally, the classifier demonstrates improved performance,\nhighlighting its effectiveness for downstream diagnostic tasks. This unique\nintegration in our Class-N-Diff makes it a robust tool for enhancing the\nquality and utility of diffusion model-based synthetic dermoscopic image\ngeneration. Our code is available at https://github.com/Munia03/Class-N-Diff.",
      "authors": [
        {
          "name": "Nusrat Munia",
          "affiliation": null
        },
        {
          "name": "Abdullah Imran",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16887v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16887v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16885v1",
      "title": "UniGTE: Unified Graph-Text Encoding for Zero-Shot Generalization across Graph Tasks and Domains",
      "abstract": "Generalizing to unseen graph tasks without task-specific supervision is\nchallenging: conventional graph neural networks are typically tied to a fixed\nlabel space, while large language models (LLMs) struggle to capture graph\nstructure. We introduce UniGTE, an instruction-tuned encoder-decoder framework\nthat unifies structural and semantic reasoning. The encoder augments a\npretrained autoregressive LLM with learnable alignment tokens and a\nstructure-aware graph-text attention mechanism, enabling it to attend jointly\nto a tokenized graph and a natural-language task prompt while remaining\npermutation-invariant to node order. This yields compact, task-aware graph\nrepresentations. Conditioned solely on these representations, a frozen LLM\ndecoder predicts and reconstructs: it outputs the task answer and\nsimultaneously paraphrases the input graph in natural language. The\nreconstruction objective regularizes the encoder to preserve structural cues.\nUniGTE is instruction-tuned on five datasets spanning node-level, edge-level,\nand graph-level tasks across diverse domains, yet requires no fine-tuning at\ninference. It achieves new state-of-the-art zero-shot results on node\nclassification, link prediction, graph classification, and graph regression\nunder cross-task and cross-domain settings, demonstrating that tight\nintegration of graph structure with LLM semantics enables robust, transferable\ngraph reasoning.",
      "authors": [
        {
          "name": "Duo Wang",
          "affiliation": null
        },
        {
          "name": "Yuan Zuo",
          "affiliation": null
        },
        {
          "name": "Guangyue Lu",
          "affiliation": null
        },
        {
          "name": "Junjie Wu",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16885v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16885v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16882v1",
      "title": "Utility-Diversity Aware Online Batch Selection for LLM Supervised Fine-tuning",
      "abstract": "Supervised fine-tuning (SFT) is a commonly used technique to adapt large\nlanguage models (LLMs) to downstream tasks. In practice, SFT on a full dataset\nis computationally expensive and sometimes suffers from overfitting or bias\namplification. This facilitates the rise of data curation in SFT, which\nprioritizes the most valuable data to optimze. This work studies the online\nbatch selection family that dynamically scores and filters samples during the\ntraining process. However, existing popular methods often (i) rely merely on\nthe utility of data to select a subset while neglecting other crucial factors\nlike diversity, (ii) rely on external resources such as reference models or\nvalidation sets, and (iii) incur extra training time over full-dataset\ntraining. To address these limitations, this work develops \\textbf{UDS\n(Utility-Diversity Sampling)}, a framework for efficient online batch selection\nin SFT. UDS leverages the nuclear norm of the logits matrix to capture both\ndata utility and intra-sample diversity, while estimating inter-sample\ndiversity through efficient low-dimensional embedding comparisons with a\nlightweight memory buffer of historical samples. Such a design eliminates the\nneed for external resources and unnecessary backpropagation, securing\ncomputational efficiency. Experiments on multiple benchmarks demonstrate that\nUDS consistently outperforms state-of-the-art online batch selection methods\nunder varying data budgets, and significantly reduces training time compared to\nfull-dataset fine-tuning. Code is available at https://github.com/gfyddha/UDS.",
      "authors": [
        {
          "name": "Heming Zou",
          "affiliation": null
        },
        {
          "name": "Yixiu Mao",
          "affiliation": null
        },
        {
          "name": "Yun Qu",
          "affiliation": null
        },
        {
          "name": "Qi Wang",
          "affiliation": null
        },
        {
          "name": "Xiangyang Ji",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16882v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16882v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16877v1",
      "title": "Fly-CL: A Fly-Inspired Framework for Enhancing Efficient Decorrelation and Reduced Training Time in Pre-trained Model-based Continual Representation Learning",
      "abstract": "Using a nearly-frozen pretrained model, the continual representation learning\nparadigm reframes parameter updates as a similarity-matching problem to\nmitigate catastrophic forgetting. However, directly leveraging pretrained\nfeatures for downstream tasks often suffers from multicollinearity in the\nsimilarity-matching stage, and more advanced methods can be computationally\nprohibitive for real-time, low-latency applications. Inspired by the fly\nolfactory circuit, we propose Fly-CL, a bio-inspired framework compatible with\na wide range of pretrained backbones. Fly-CL substantially reduces training\ntime while achieving performance comparable to or exceeding that of current\nstate-of-the-art methods. We theoretically show how Fly-CL progressively\nresolves multicollinearity, enabling more effective similarity matching with\nlow time complexity. Extensive simulation experiments across diverse network\narchitectures and data regimes validate Fly-CL's effectiveness in addressing\nthis challenge through a biologically inspired design. Code is available at\nhttps://github.com/gfyddha/Fly-CL.",
      "authors": [
        {
          "name": "Heming Zou",
          "affiliation": null
        },
        {
          "name": "Yunliang Zang",
          "affiliation": null
        },
        {
          "name": "Wutong Xu",
          "affiliation": null
        },
        {
          "name": "Xiangyang Ji",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16877v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16877v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16872v1",
      "title": "DeepAnalyze: Agentic Large Language Models for Autonomous Data Science",
      "abstract": "Autonomous data science, from raw data sources to analyst-grade deep research\nreports, has been a long-standing challenge, and is now becoming feasible with\nthe emergence of powerful large language models (LLMs). Recent workflow-based\ndata agents have shown promising results on specific data tasks but remain\nfundamentally limited in achieving fully autonomous data science due to their\nreliance on predefined workflows. In this paper, we introduce DeepAnalyze-8B,\nthe first agentic LLM designed for autonomous data science, capable of\nautomatically completing the end-toend pipeline from data sources to\nanalyst-grade deep research reports. To tackle high-complexity data science\ntasks, we propose a curriculum-based agentic training paradigm that emulates\nthe learning trajectory of human data scientists, enabling LLMs to\nprogressively acquire and integrate multiple capabilities in real-world\nenvironments. We also introduce a data-grounded trajectory synthesis framework\nthat constructs high-quality training data. Through agentic training,\nDeepAnalyze learns to perform a broad spectrum of data tasks, ranging from data\nquestion answering and specialized analytical tasks to open-ended data\nresearch. Experiments demonstrate that, with only 8B parameters, DeepAnalyze\noutperforms previous workflow-based agents built on most advanced proprietary\nLLMs. The model, code, and training data of DeepAnalyze are open-sourced,\npaving the way toward autonomous data science.",
      "authors": [
        {
          "name": "Shaolei Zhang",
          "affiliation": null
        },
        {
          "name": "Ju Fan",
          "affiliation": null
        },
        {
          "name": "Meihao Fan",
          "affiliation": null
        },
        {
          "name": "Guoliang Li",
          "affiliation": null
        },
        {
          "name": "Xiaoyong Du",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.DB"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16872v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16872v1",
      "primary_category": "cs.AI",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16870v1",
      "title": "Uncovering Brain-Like Hierarchical Patterns in Vision-Language Models through fMRI-Based Neural Encoding",
      "abstract": "While brain-inspired artificial intelligence(AI) has demonstrated promising\nresults, current understanding of the parallels between artificial neural\nnetworks (ANNs) and human brain processing remains limited: (1) unimodal ANN\nstudies fail to capture the brain's inherent multimodal processing\ncapabilities, and (2) multimodal ANN research primarily focuses on high-level\nmodel outputs, neglecting the crucial role of individual neurons. To address\nthese limitations, we propose a novel neuron-level analysis framework that\ninvestigates the multimodal information processing mechanisms in\nvision-language models (VLMs) through the lens of human brain activity. Our\napproach uniquely combines fine-grained artificial neuron (AN) analysis with\nfMRI-based voxel encoding to examine two architecturally distinct VLMs: CLIP\nand METER. Our analysis reveals four key findings: (1) ANs successfully predict\nbiological neurons (BNs) activities across multiple functional networks\n(including language, vision, attention, and default mode), demonstrating shared\nrepresentational mechanisms; (2) Both ANs and BNs demonstrate functional\nredundancy through overlapping neural representations, mirroring the brain's\nfault-tolerant and collaborative information processing mechanisms; (3) ANs\nexhibit polarity patterns that parallel the BNs, with oppositely activated BNs\nshowing mirrored activation trends across VLM layers, reflecting the complexity\nand bidirectional nature of neural information processing; (4) The\narchitectures of CLIP and METER drive distinct BNs: CLIP's independent branches\nshow modality-specific specialization, whereas METER's cross-modal design\nyields unified cross-modal activation, highlighting the architecture's\ninfluence on ANN brain-like properties. These results provide compelling\nevidence for brain-like hierarchical processing in VLMs at the neuronal level.",
      "authors": [
        {
          "name": "Yudan Ren",
          "affiliation": null
        },
        {
          "name": "Xinlong Wang",
          "affiliation": null
        },
        {
          "name": "Kexin Wang",
          "affiliation": null
        },
        {
          "name": "Tian Xia",
          "affiliation": null
        },
        {
          "name": "Zihan Ma",
          "affiliation": null
        },
        {
          "name": "Zhaowei Li",
          "affiliation": null
        },
        {
          "name": "Xiangrong Bi",
          "affiliation": null
        },
        {
          "name": "Xiao Li",
          "affiliation": null
        },
        {
          "name": "Xiaowei He",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16870v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16870v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16865v1",
      "title": "Registration is a Powerful Rotation-Invariance Learner for 3D Anomaly Detection",
      "abstract": "3D anomaly detection in point-cloud data is critical for industrial quality\ncontrol, aiming to identify structural defects with high reliability. However,\ncurrent memory bank-based methods often suffer from inconsistent feature\ntransformations and limited discriminative capacity, particularly in capturing\nlocal geometric details and achieving rotation invariance. These limitations\nbecome more pronounced when registration fails, leading to unreliable detection\nresults. We argue that point-cloud registration plays an essential role not\nonly in aligning geometric structures but also in guiding feature extraction\ntoward rotation-invariant and locally discriminative representations. To this\nend, we propose a registration-induced, rotation-invariant feature extraction\nframework that integrates the objectives of point-cloud registration and\nmemory-based anomaly detection. Our key insight is that both tasks rely on\nmodeling local geometric structures and leveraging feature similarity across\nsamples. By embedding feature extraction into the registration learning\nprocess, our framework jointly optimizes alignment and representation learning.\nThis integration enables the network to acquire features that are both robust\nto rotations and highly effective for anomaly detection. Extensive experiments\non the Anomaly-ShapeNet and Real3D-AD datasets demonstrate that our method\nconsistently outperforms existing approaches in effectiveness and\ngeneralizability.",
      "authors": [
        {
          "name": "Yuyang Yu",
          "affiliation": null
        },
        {
          "name": "Zhengwei Chen",
          "affiliation": null
        },
        {
          "name": "Xuemiao Xu",
          "affiliation": null
        },
        {
          "name": "Lei Zhang",
          "affiliation": null
        },
        {
          "name": "Haoxin Yang",
          "affiliation": null
        },
        {
          "name": "Yongwei Nie",
          "affiliation": null
        },
        {
          "name": "Shengfeng He",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16865v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16865v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16863v1",
      "title": "BARL: Bilateral Alignment in Representation and Label Spaces for Semi-Supervised Volumetric Medical Image Segmentation",
      "abstract": "Semi-supervised medical image segmentation (SSMIS) seeks to match fully\nsupervised performance while sharply reducing annotation cost. Mainstream SSMIS\nmethods rely on \\emph{label-space consistency}, yet they overlook the equally\ncritical \\emph{representation-space alignment}. Without harmonizing latent\nfeatures, models struggle to learn representations that are both discriminative\nand spatially coherent. To this end, we introduce \\textbf{Bilateral Alignment\nin Representation and Label spaces (BARL)}, a unified framework that couples\ntwo collaborative branches and enforces alignment in both spaces. For\nlabel-space alignment, inspired by co-training and multi-scale decoding, we\ndevise \\textbf{Dual-Path Regularization (DPR)} and \\textbf{Progressively\nCognitive Bias Correction (PCBC)} to impose fine-grained cross-branch\nconsistency while mitigating error accumulation from coarse to fine scales. For\nrepresentation-space alignment, we conduct region-level and lesion-instance\nmatching between branches, explicitly capturing the fragmented, complex\npathological patterns common in medical imagery. Extensive experiments on four\npublic benchmarks and a proprietary CBCT dataset demonstrate that BARL\nconsistently surpasses state-of-the-art SSMIS methods. Ablative studies further\nvalidate the contribution of each component. Code will be released soon.",
      "authors": [
        {
          "name": "Shujian Gao",
          "affiliation": null
        },
        {
          "name": "Yuan Wang",
          "affiliation": null
        },
        {
          "name": "Zekuan Yu",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16863v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16863v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16857v1",
      "title": "DrivAerStar: An Industrial-Grade CFD Dataset for Vehicle Aerodynamic Optimization",
      "abstract": "Vehicle aerodynamics optimization has become critical for automotive\nelectrification, where drag reduction directly determines electric vehicle\nrange and energy efficiency. Traditional approaches face an intractable\ntrade-off: computationally expensive Computational Fluid Dynamics (CFD)\nsimulations requiring weeks per design iteration, or simplified models that\nsacrifice production-grade accuracy. While machine learning offers\ntransformative potential, existing datasets exhibit fundamental limitations --\ninadequate mesh resolution, missing vehicle components, and validation errors\nexceeding 5% -- preventing deployment in industrial workflows. We present\nDrivAerStar, comprising 12,000 industrial-grade automotive CFD simulations\ngenerated using $\\text{STAR-CCM+}^\\unicode{xAE}$ software. The dataset\nsystematically explores three vehicle configurations through 20 Computer Aided\nDesign (CAD) parameters via Free Form Deformation (FFD) algorithms, including\ncomplete engine compartments and cooling systems with realistic internal\nairflow. DrivAerStar achieves wind tunnel validation accuracy below 1.04% -- a\nfive-fold improvement over existing datasets -- through refined mesh strategies\nwith strict wall $y^+$ control. Benchmarks demonstrate that models trained on\nthis data achieve production-ready accuracy while reducing computational costs\nfrom weeks to minutes. This represents the first dataset bridging academic\nmachine learning research and industrial CFD practice, establishing a new\nstandard for data-driven aerodynamic optimization in automotive development.\nBeyond automotive applications, DrivAerStar demonstrates a paradigm for\nintegrating high-fidelity physics simulations with Artificial Intelligence (AI)\nacross engineering disciplines where computational constraints currently limit\ninnovation.",
      "authors": [
        {
          "name": "Jiyan Qiu",
          "affiliation": null
        },
        {
          "name": "Lyulin Kuang",
          "affiliation": null
        },
        {
          "name": "Guan Wang",
          "affiliation": null
        },
        {
          "name": "Yichen Xu",
          "affiliation": null
        },
        {
          "name": "Leiyao Cui",
          "affiliation": null
        },
        {
          "name": "Shaotong Fu",
          "affiliation": null
        },
        {
          "name": "Yixin Zhu",
          "affiliation": null
        },
        {
          "name": "Ruihua Zhang",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16857v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16857v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16854v1",
      "title": "ArmFormer: Lightweight Transformer Architecture for Real-Time Multi-Class Weapon Segmentation and Classification",
      "abstract": "The escalating threat of weapon-related violence necessitates automated\ndetection systems capable of pixel-level precision for accurate threat\nassessment in real-time security applications. Traditional weapon detection\napproaches rely on object detection frameworks that provide only coarse\nbounding box localizations, lacking the fine-grained segmentation required for\ncomprehensive threat analysis. Furthermore, existing semantic segmentation\nmodels either sacrifice accuracy for computational efficiency or require\nexcessive computational resources incompatible with edge deployment scenarios.\nThis paper presents ArmFormer, a lightweight transformer-based semantic\nsegmentation framework that strategically integrates Convolutional Block\nAttention Module (CBAM) with MixVisionTransformer architecture to achieve\nsuperior accuracy while maintaining computational efficiency suitable for\nresource-constrained edge devices. Our approach combines CBAM-enhanced encoder\nbackbone with attention-integrated hamburger decoder to enable multi-class\nweapon segmentation across five categories: handgun, rifle, knife, revolver,\nand human. Comprehensive experiments demonstrate that ArmFormer achieves\nstate-of-the-art performance with 80.64% mIoU and 89.13% mFscore while\nmaintaining real-time inference at 82.26 FPS. With only 4.886G FLOPs and 3.66M\nparameters, ArmFormer outperforms heavyweight models requiring up to 48x more\ncomputation, establishing it as the optimal solution for deployment on portable\nsecurity cameras, surveillance drones, and embedded AI accelerators in\ndistributed security infrastructure.",
      "authors": [
        {
          "name": "Akhila Kambhatla",
          "affiliation": null
        },
        {
          "name": "Taminul Islam",
          "affiliation": null
        },
        {
          "name": "Khaled R Ahmed",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "cs.AI",
        "68T07",
        "I.2.10; I.5.4; I.4.6"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16854v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16854v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16853v1",
      "title": "Agentic Inequality",
      "abstract": "Autonomous AI agents, capable of complex planning and action, represent a\nsignificant technological evolution beyond current generative tools. As these\nsystems become integrated into political and economic life, their distribution\nand capabilities will be highly consequential. This paper introduces and\nexplores \"agentic inequality\" - the potential disparities in power,\nopportunity, and outcomes stemming from differential access to, and\ncapabilities of, AI agents. We analyse the dual potential of this technology,\nexploring how agents could both exacerbate existing divides and, under the\nright conditions, serve as a powerful equalising force. To this end, the paper\nmakes three primary contributions. First, it establishes an analytical\nframework by delineating the three core dimensions through which this\ninequality can manifest: disparities in the availability, quality, and quantity\nof agents. Second, it argues that agentic inequality is distinct from prior\ntechnological divides. Unlike tools that primarily augment human abilities,\nagents act as autonomous delegates, creating novel power asymmetries through\nscalable goal delegation and direct agent-to-agent competition that are poised\nto reshape outcomes across economic and socio-political spheres. Finally, it\nprovides a systematic analysis of the technical and socioeconomic drivers -\nfrom model release strategies to market incentives - that will shape the\ndistribution of agentic power, concluding with a research agenda for navigating\nthe complex governance challenges ahead.",
      "authors": [
        {
          "name": "Matthew Sharp",
          "affiliation": null
        },
        {
          "name": "Omer Bilgin",
          "affiliation": null
        },
        {
          "name": "Iason Gabriel",
          "affiliation": null
        },
        {
          "name": "Lewis Hammond",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16853v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16853v1",
      "primary_category": "cs.CY",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16851v1",
      "title": "Neuronal Group Communication for Efficient Neural representation",
      "abstract": "The ever-increasing scale of modern neural networks has brought unprecedented\nperformance alongside daunting challenges in efficiency and interpretability.\nThis paper addresses the core question of how to build large neural systems\nthat learn efficient, modular, and interpretable representations. We propose\nNeuronal Group Communication (NGC), a theory-driven framework that reimagines a\nneural network as a dynamical system of interacting neuronal groups rather than\na monolithic collection of neural weights. Instead of treating each weight as\nan independent trainable parameter, NGC treats weights as transient\ninteractions between embedding-like neuronal states, with neural computation\nunfolding through iterative communication among groups of neurons. This\nlow-rank, modular representation yields compact models: groups of neurons\nexchange low-dimensional signals, enabling intra-group specialization and\ninter-group information sharing while dramatically reducing redundant\nparameters. By drawing on dynamical systems theory, we introduce a neuronal\nstability metric (analogous to Lyapunov stability) that quantifies the\ncontraction of neuron activations toward stable patterns during sequence\nprocessing. Using this metric, we reveal that emergent reasoning capabilities\ncorrespond to an external driving force or ``potential'', which nudges the\nneural dynamics away from trivial trajectories while preserving stability.\nEmpirically, we instantiate NGC in large language models (LLMs) and demonstrate\nimproved performance on complex reasoning benchmarks under moderate\ncompression. NGC consistently outperforms standard low-rank approximations and\ncross-layer basis-sharing methods at comparable compression rates. We conclude\nby discussing the broader implications of NGC, including how structured\nneuronal group dynamics might relate to generalization in high-dimensional\nlearning systems.",
      "authors": [
        {
          "name": "Zhengqi Pei",
          "affiliation": null
        },
        {
          "name": "Qingming Huang",
          "affiliation": null
        },
        {
          "name": "Shuhui Wang",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.NE"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16851v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16851v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16844v1",
      "title": "FinSight: Towards Real-World Financial Deep Research",
      "abstract": "Generating professional financial reports is a labor-intensive and\nintellectually demanding process that current AI systems struggle to fully\nautomate. To address this challenge, we introduce FinSight (Financial InSight),\na novel multi agent framework for producing high-quality, multimodal financial\nreports. The foundation of FinSight is the Code Agent with Variable Memory\n(CAVM) architecture, which unifies external data, designed tools, and agents\ninto a programmable variable space, enabling flexible data collection, analysis\nand report generation through executable code. To ensure professional-grade\nvisualization, we propose an Iterative Vision-Enhanced Mechanism that\nprogressively refines raw visual outputs into polished financial charts.\nFurthermore, a two stage Writing Framework expands concise Chain-of-Analysis\nsegments into coherent, citation-aware, and multimodal reports, ensuring both\nanalytical depth and structural consistency. Experiments on various company and\nindustry-level tasks demonstrate that FinSight significantly outperforms all\nbaselines, including leading deep research systems in terms of factual\naccuracy, analytical depth, and presentation quality, demonstrating a clear\npath toward generating reports that approach human-expert quality.",
      "authors": [
        {
          "name": "Jiajie Jin",
          "affiliation": null
        },
        {
          "name": "Yuyao Zhang",
          "affiliation": null
        },
        {
          "name": "Yimeng Xu",
          "affiliation": null
        },
        {
          "name": "Hongjin Qian",
          "affiliation": null
        },
        {
          "name": "Yutao Zhu",
          "affiliation": null
        },
        {
          "name": "Zhicheng Dou",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CE"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16844v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16844v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16837v1",
      "title": "2DGS-R: Revisiting the Normal Consistency Regularization in 2D Gaussian Splatting",
      "abstract": "Recent advancements in 3D Gaussian Splatting (3DGS) have greatly influenced\nneural fields, as it enables high-fidelity rendering with impressive visual\nquality. However, 3DGS has difficulty accurately representing surfaces. In\ncontrast, 2DGS transforms the 3D volume into a collection of 2D planar Gaussian\ndisks. Despite advancements in geometric fidelity, rendering quality remains\ncompromised, highlighting the challenge of achieving both high-quality\nrendering and precise geometric structures. This indicates that optimizing both\ngeometric and rendering quality in a single training stage is currently\nunfeasible. To overcome this limitation, we present 2DGS-R, a new method that\nuses a hierarchical training approach to improve rendering quality while\nmaintaining geometric accuracy. 2DGS-R first trains the original 2D Gaussians\nwith the normal consistency regularization. Then 2DGS-R selects the 2D\nGaussians with inadequate rendering quality and applies a novel in-place\ncloning operation to enhance the 2D Gaussians. Finally, we fine-tune the 2DGS-R\nmodel with opacity frozen. Experimental results show that compared to the\noriginal 2DGS, our method requires only 1\\% more storage and minimal additional\ntraining time. Despite this negligible overhead, it achieves high-quality\nrendering results while preserving fine geometric structures. These findings\nindicate that our approach effectively balances efficiency with performance,\nleading to improvements in both visual fidelity and geometric reconstruction\naccuracy.",
      "authors": [
        {
          "name": "Haofan Ren",
          "affiliation": null
        },
        {
          "name": "Qingsong Yan",
          "affiliation": null
        },
        {
          "name": "Ming Lu",
          "affiliation": null
        },
        {
          "name": "Rongfeng Lu",
          "affiliation": null
        },
        {
          "name": "Zunjie Zhu",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16837v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16837v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16834v1",
      "title": "Schr\u00f6dinger Bridge Mamba for One-Step Speech Enhancement",
      "abstract": "We propose Schr\\\"odinger Bridge Mamba (SBM), a new concept of\ntraining-inference framework motivated by the inherent compatibility between\nSchr\\\"odinger Bridge (SB) training paradigm and selective state-space model\nMamba. We exemplify the concept of SBM with an implementation for generative\nspeech enhancement. Experiments on a joint denoising and dereverberation task\nusing four benchmark datasets demonstrate that SBM, with only 1-step inference,\noutperforms strong baselines with 1-step or iterative inference and achieves\nthe best real-time factor (RTF). Beyond speech enhancement, we discuss the\nintegration of SB paradigm and selective state-space model architecture based\non their underlying alignment, which indicates a promising direction for\nexploring new deep generative models potentially applicable to a broad range of\ngenerative tasks. Demo page: https://sbmse.github.io",
      "authors": [
        {
          "name": "Jing Yang",
          "affiliation": null
        },
        {
          "name": "Sirui Wang",
          "affiliation": null
        },
        {
          "name": "Chao Wu",
          "affiliation": null
        },
        {
          "name": "Fan Fan",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16834v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16834v1",
      "primary_category": "cs.SD",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16833v1",
      "title": "From Mannequin to Human: A Pose-Aware and Identity-Preserving Video Generation Framework for Lifelike Clothing Display",
      "abstract": "Mannequin-based clothing displays offer a cost-effective alternative to\nreal-model showcases for online fashion presentation, but lack realism and\nexpressive detail. To overcome this limitation, we introduce a new task called\nmannequin-to-human (M2H) video generation, which aims to synthesize\nidentity-controllable, photorealistic human videos from footage of mannequins.\nWe propose M2HVideo, a pose-aware and identity-preserving video generation\nframework that addresses two key challenges: the misalignment between head and\nbody motion, and identity drift caused by temporal modeling. In particular,\nM2HVideo incorporates a dynamic pose-aware head encoder that fuses facial\nsemantics with body pose to produce consistent identity embeddings across\nframes. To address the loss of fine facial details due to latent space\ncompression, we introduce a mirror loss applied in pixel space through a\ndenoising diffusion implicit model (DDIM)-based one-step denoising.\nAdditionally, we design a distribution-aware adapter that aligns statistical\ndistributions of identity and clothing features to enhance temporal coherence.\nExtensive experiments on the UBC fashion dataset, our self-constructed ASOS\ndataset, and the newly collected MannequinVideos dataset captured on-site\ndemonstrate that M2HVideo achieves superior performance in terms of clothing\nconsistency, identity preservation, and video fidelity in comparison to\nstate-of-the-art methods.",
      "authors": [
        {
          "name": "Xiangyu Mu",
          "affiliation": null
        },
        {
          "name": "Dongliang Zhou",
          "affiliation": null
        },
        {
          "name": "Jie Hou",
          "affiliation": null
        },
        {
          "name": "Haijun Zhang",
          "affiliation": null
        },
        {
          "name": "Weili Guan",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "cs.GR"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16833v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16833v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16832v1",
      "title": "Robust Cross-Domain Adaptation in Texture Features Transferring for Wood Chip Moisture Content Prediction",
      "abstract": "Accurate and quick prediction of wood chip moisture content is critical for\noptimizing biofuel production and ensuring energy efficiency. The current\nwidely used direct method (oven drying) is limited by its longer processing\ntime and sample destructiveness. On the other hand, existing indirect methods,\nincluding near-infrared spectroscopy-based, electrical capacitance-based, and\nimage-based approaches, are quick but not accurate when wood chips come from\nvarious sources. Variability in the source material can alter data\ndistributions, undermining the performance of data-driven models. Therefore,\nthere is a need for a robust approach that effectively mitigates the impact of\nsource variability. Previous studies show that manually extracted texture\nfeatures have the potential to predict wood chip moisture class. Building on\nthis, in this study, we conduct a comprehensive analysis of five distinct\ntexture feature types extracted from wood chip images to predict moisture\ncontent. Our findings reveal that a combined feature set incorporating all five\ntexture features achieves an accuracy of 95% and consistently outperforms\nindividual texture features in predicting moisture content. To ensure robust\nmoisture prediction, we propose a domain adaptation method named AdaptMoist\nthat utilizes the texture features to transfer knowledge from one source of\nwood chip data to another, addressing variability across different domains. We\nalso proposed a criterion for model saving based on adjusted mutual\ninformation. The AdaptMoist method improves prediction accuracy across domains\nby 23%, achieving an average accuracy of 80%, compared to 57% for non-adapted\nmodels. These results highlight the effectiveness of AdaptMoist as a robust\nsolution for wood chip moisture content estimation across domains, making it a\npotential solution for wood chip-reliant industries.",
      "authors": [
        {
          "name": "Abdur Rahman",
          "affiliation": null
        },
        {
          "name": "Mohammad Marufuzzaman",
          "affiliation": null
        },
        {
          "name": "Jason Street",
          "affiliation": null
        },
        {
          "name": "Haifeng Wang",
          "affiliation": null
        },
        {
          "name": "Veera G. Gude",
          "affiliation": null
        },
        {
          "name": "Randy Buchanan",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16832v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16832v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16830v1",
      "title": "Verifiable Fine-Tuning for LLMs: Zero-Knowledge Training Proofs Bound to Data Provenance and Policy",
      "abstract": "Large language models are often adapted through parameter efficient fine\ntuning, but current release practices provide weak assurances about what data\nwere used and how updates were computed. We present Verifiable Fine Tuning, a\nprotocol and system that produces succinct zero knowledge proofs that a\nreleased model was obtained from a public initialization under a declared\ntraining program and an auditable dataset commitment. The approach combines\nfive elements. First, commitments that bind data sources, preprocessing,\nlicenses, and per epoch quota counters to a manifest. Second, a verifiable\nsampler that supports public replayable and private index hiding batch\nselection. Third, update circuits restricted to parameter efficient fine tuning\nthat enforce AdamW style optimizer semantics and proof friendly approximations\nwith explicit error budgets. Fourth, recursive aggregation that folds per step\nproofs into per epoch and end to end certificates with millisecond\nverification. Fifth, provenance binding and optional trusted execution property\ncards that attest code identity and constants. On English and bilingual\ninstruction mixtures, the method maintains utility within tight budgets while\nachieving practical proof performance. Policy quotas are enforced with zero\nviolations, and private sampling windows show no measurable index leakage.\nFederated experiments demonstrate that the system composes with probabilistic\naudits and bandwidth constraints. These results indicate that end to end\nverifiable fine tuning is feasible today for real parameter efficient\npipelines, closing a critical trust gap for regulated and decentralized\ndeployments.",
      "authors": [
        {
          "name": "Hasan Akgul",
          "affiliation": null
        },
        {
          "name": "Daniel Borg",
          "affiliation": null
        },
        {
          "name": "Arta Berisha",
          "affiliation": null
        },
        {
          "name": "Amina Rahimova",
          "affiliation": null
        },
        {
          "name": "Andrej Novak",
          "affiliation": null
        },
        {
          "name": "Mila Petrov",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CR",
        "cs.CL",
        "68T07, 94A60, 68Q25",
        "I.2.6; G.1.6; E.3; C.2.4"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16830v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16830v1",
      "primary_category": "cs.CR",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16829v1",
      "title": "Who's Asking? Simulating Role-Based Questions for Conversational AI Evaluation",
      "abstract": "Language model users often embed personal and social context in their\nquestions. The asker's role -- implicit in how the question is framed --\ncreates specific needs for an appropriate response. However, most evaluations,\nwhile capturing the model's capability to respond, often ignore who is asking.\nThis gap is especially critical in stigmatized domains such as opioid use\ndisorder (OUD), where accounting for users' contexts is essential to provide\naccessible, stigma-free responses. We propose CoRUS (COmmunity-driven Roles for\nUser-centric Question Simulation), a framework for simulating role-based\nquestions. Drawing on role theory and posts from an online OUD recovery\ncommunity (r/OpiatesRecovery), we first build a taxonomy of asker roles --\npatients, caregivers, practitioners. Next, we use it to simulate 15,321\nquestions that embed each role's goals, behaviors, and experiences. Our\nevaluations show that these questions are both highly believable and comparable\nto real-world data. When used to evaluate five LLMs, for the same question but\ndiffering roles, we find systematic differences: vulnerable roles, such as\npatients and caregivers, elicit more supportive responses (+17%) and reduced\nknowledge content (-19%) in comparison to practitioners. Our work demonstrates\nhow implicitly signaling a user's role shapes model responses, and provides a\nmethodology for role-informed evaluation of conversational AI.",
      "authors": [
        {
          "name": "Navreet Kaur",
          "affiliation": null
        },
        {
          "name": "Hoda Ayad",
          "affiliation": null
        },
        {
          "name": "Hayoung Jung",
          "affiliation": null
        },
        {
          "name": "Shravika Mittal",
          "affiliation": null
        },
        {
          "name": "Munmun De Choudhury",
          "affiliation": null
        },
        {
          "name": "Tanushree Mitra",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.HC"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16829v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16829v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16824v1",
      "title": "ProtoMol: Enhancing Molecular Property Prediction via Prototype-Guided Multimodal Learning",
      "abstract": "Multimodal molecular representation learning, which jointly models molecular\ngraphs and their textual descriptions, enhances predictive accuracy and\ninterpretability by enabling more robust and reliable predictions of drug\ntoxicity, bioactivity, and physicochemical properties through the integration\nof structural and semantic information. However, existing multimodal methods\nsuffer from two key limitations: (1) they typically perform cross-modal\ninteraction only at the final encoder layer, thus overlooking hierarchical\nsemantic dependencies; (2) they lack a unified prototype space for robust\nalignment between modalities. To address these limitations, we propose\nProtoMol, a prototype-guided multimodal framework that enables fine-grained\nintegration and consistent semantic alignment between molecular graphs and\ntextual descriptions. ProtoMol incorporates dual-branch hierarchical encoders,\nutilizing Graph Neural Networks to process structured molecular graphs and\nTransformers to encode unstructured texts, resulting in comprehensive\nlayer-wise representations. Then, ProtoMol introduces a layer-wise\nbidirectional cross-modal attention mechanism that progressively aligns\nsemantic features across layers. Furthermore, a shared prototype space with\nlearnable, class-specific anchors is constructed to guide both modalities\ntoward coherent and discriminative representations. Extensive experiments on\nmultiple benchmark datasets demonstrate that ProtoMol consistently outperforms\nstate-of-the-art baselines across a variety of molecular property prediction\ntasks.",
      "authors": [
        {
          "name": "Yingxu Wang",
          "affiliation": null
        },
        {
          "name": "Kunyu Zhang",
          "affiliation": null
        },
        {
          "name": "Jiaxin Huang",
          "affiliation": null
        },
        {
          "name": "Nan Yin",
          "affiliation": null
        },
        {
          "name": "Siwei Liu",
          "affiliation": null
        },
        {
          "name": "Eran Segal",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "q-bio.MN"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16824v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16824v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16822v1",
      "title": "ReefNet: A Large scale, Taxonomically Enriched Dataset and Benchmark for Hard Coral Classification",
      "abstract": "Coral reefs are rapidly declining due to anthropogenic pressures such as\nclimate change, underscoring the urgent need for scalable, automated\nmonitoring. We introduce ReefNet, a large public coral reef image dataset with\npoint-label annotations mapped to the World Register of Marine Species (WoRMS).\nReefNet aggregates imagery from 76 curated CoralNet sources and an additional\nsite from Al Wajh in the Red Sea, totaling approximately 925000 genus-level\nhard coral annotations with expert-verified labels. Unlike prior datasets,\nwhich are often limited by size, geography, or coarse labels and are not\nML-ready, ReefNet offers fine-grained, taxonomically mapped labels at a global\nscale to WoRMS. We propose two evaluation settings: (i) a within-source\nbenchmark that partitions each source's images for localized evaluation, and\n(ii) a cross-source benchmark that withholds entire sources to test domain\ngeneralization. We analyze both supervised and zero-shot classification\nperformance on ReefNet and find that while supervised within-source performance\nis promising, supervised performance drops sharply across domains, and\nperformance is low across the board for zero-shot models, especially for rare\nand visually similar genera. This provides a challenging benchmark intended to\ncatalyze advances in domain generalization and fine-grained coral\nclassification. We will release our dataset, benchmarking code, and pretrained\nmodels to advance robust, domain-adaptive, global coral reef monitoring and\nconservation.",
      "authors": [
        {
          "name": "Yahia Battach",
          "affiliation": null
        },
        {
          "name": "Abdulwahab Felemban",
          "affiliation": null
        },
        {
          "name": "Faizan Farooq Khan",
          "affiliation": null
        },
        {
          "name": "Yousef A. Radwan",
          "affiliation": null
        },
        {
          "name": "Xiang Li",
          "affiliation": null
        },
        {
          "name": "Fabio Marchese",
          "affiliation": null
        },
        {
          "name": "Sara Beery",
          "affiliation": null
        },
        {
          "name": "Burton H. Jones",
          "affiliation": null
        },
        {
          "name": "Francesca Benzoni",
          "affiliation": null
        },
        {
          "name": "Mohamed Elhoseiny",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16822v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16822v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16820v1",
      "title": "Finding Manifolds With Bilinear Autoencoders",
      "abstract": "Sparse autoencoders are a standard tool for uncovering interpretable latent\nrepresentations in neural networks. Yet, their interpretation depends on the\ninputs, making their isolated study incomplete. Polynomials offer a solution;\nthey serve as algebraic primitives that can be analysed without reference to\ninput and can describe structures ranging from linear concepts to complicated\nmanifolds. This work uses bilinear autoencoders to efficiently decompose\nrepresentations into quadratic polynomials. We discuss improvements that induce\nimportance ordering, clustering, and activation sparsity. This is an initial\nstep toward nonlinear yet analysable latents through their algebraic\nproperties.",
      "authors": [
        {
          "name": "Thomas Dooms",
          "affiliation": null
        },
        {
          "name": "Ward Gauderis",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16820v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16820v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16819v1",
      "title": "Cross-Genre Authorship Attribution via LLM-Based Retrieve-and-Rerank",
      "abstract": "Authorship attribution (AA) is the task of identifying the most likely author\nof a query document from a predefined set of candidate authors. We introduce a\ntwo-stage retrieve-and-rerank framework that finetunes LLMs for cross-genre AA.\nUnlike the field of information retrieval (IR), where retrieve-and-rerank is a\nde facto strategy, cross-genre AA systems must avoid relying on topical cues\nand instead learn to identify author-specific linguistic patterns that are\nindependent of the text's subject matter (genre/domain/topic). Consequently,\nfor the reranker, we demonstrate that training strategies commonly used in IR\nare fundamentally misaligned with cross-genre AA, leading to suboptimal\nbehavior. To address this, we introduce a targeted data curation strategy that\nenables the reranker to effectively learn author-discriminative signals. Using\nour LLM-based retrieve-and-rerank pipeline, we achieve substantial gains of\n22.3 and 34.4 absolute Success@8 points over the previous state-of-the-art on\nHIATUS's challenging HRS1 and HRS2 cross-genre AA benchmarks.",
      "authors": [
        {
          "name": "Shantanu Agarwal",
          "affiliation": null
        },
        {
          "name": "Joel Barry",
          "affiliation": null
        },
        {
          "name": "Steven Fincke",
          "affiliation": null
        },
        {
          "name": "Scott Miller",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16819v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16819v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16817v1",
      "title": "Trace Regularity PINNs: Enforcing $\\mathrm{H}^{\\frac{1}{2}}(\\partial \u03a9)$ for Boundary Data",
      "abstract": "We propose an enhanced physics-informed neural network (PINN), the Trace\nRegularity Physics-Informed Neural Network (TRPINN), which enforces the\nboundary loss in the Sobolev-Slobodeckij norm $H^{1/2}(\\partial \\Omega)$, the\ncorrect trace space associated with $H^1(\\Omega)$. We reduce computational cost\nby computing only the theoretically essential portion of the semi-norm and\nenhance convergence stability by avoiding denominator evaluations in the\ndiscretization. By incorporating the exact $H^{1/2}(\\partial \\Omega)$ norm, we\nshow that the approximation converges to the true solution in the\n$H^{1}(\\Omega)$ sense, and, through Neural Tangent Kernel (NTK) analysis, we\ndemonstrate that TRPINN can converge faster than standard PINNs. Numerical\nexperiments on the Laplace equation with highly oscillatory Dirichlet boundary\nconditions exhibit cases where TRPINN succeeds even when standard PINNs fail,\nand show performance improvements of one to three decimal digits.",
      "authors": [
        {
          "name": "Doyoon Kim",
          "affiliation": null
        },
        {
          "name": "Junbin Song",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "math.AP"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16817v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16817v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16816v1",
      "title": "Efficient High-Accuracy PDEs Solver with the Linear Attention Neural Operator",
      "abstract": "Neural operators offer a powerful data-driven framework for learning mappings\nbetween function spaces, in which the transformer-based neural operator\narchitecture faces a fundamental scalability-accuracy trade-off: softmax\nattention provides excellent fidelity but incurs quadratic complexity\n$\\mathcal{O}(N^2 d)$ in the number of mesh points $N$ and hidden dimension $d$,\nwhile linear attention variants reduce cost to $\\mathcal{O}(N d^2)$ but often\nsuffer significant accuracy degradation. To address the aforementioned\nchallenge, in this paper, we present a novel type of neural operators, Linear\nAttention Neural Operator (LANO), which achieves both scalability and high\naccuracy by reformulating attention through an agent-based mechanism. LANO\nresolves this dilemma by introducing a compact set of $M$ agent tokens $(M \\ll\nN)$ that mediate global interactions among $N$ tokens. This agent attention\nmechanism yields an operator layer with linear complexity $\\mathcal{O}(MN d)$\nwhile preserving the expressive power of softmax attention. Theoretically, we\ndemonstrate the universal approximation property, thereby demonstrating\nimproved conditioning and stability properties. Empirically, LANO surpasses\ncurrent state-of-the-art neural PDE solvers, including Transolver with\nslice-based softmax attention, achieving average $19.5\\%$ accuracy improvement\nacross standard benchmarks. By bridging the gap between linear complexity and\nsoftmax-level performance, LANO establishes a scalable, high-accuracy\nfoundation for scientific machine learning applications.",
      "authors": [
        {
          "name": "Ming Zhong",
          "affiliation": null
        },
        {
          "name": "Zhenya Yan",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "math-ph",
        "math.MP",
        "physics.comp-ph"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16816v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16816v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16815v1",
      "title": "Knowing the Facts but Choosing the Shortcut: Understanding How Large Language Models Compare Entities",
      "abstract": "Large Language Models (LLMs) are increasingly used for knowledge-based\nreasoning tasks, yet understanding when they rely on genuine knowledge versus\nsuperficial heuristics remains challenging. We investigate this question\nthrough entity comparison tasks by asking models to compare entities along\nnumerical attributes (e.g., ``Which river is longer, the Danube or the\nNile?''), which offer clear ground truth for systematic analysis. Despite\nhaving sufficient numerical knowledge to answer correctly, LLMs frequently make\npredictions that contradict this knowledge. We identify three heuristic biases\nthat strongly influence model predictions: entity popularity, mention order,\nand semantic co-occurrence. For smaller models, a simple logistic regression\nusing only these surface cues predicts model choices more accurately than the\nmodel's own numerical predictions, suggesting heuristics largely override\nprincipled reasoning. Crucially, we find that larger models (32B parameters)\nselectively rely on numerical knowledge when it is more reliable, while smaller\nmodels (7--8B parameters) show no such discrimination, which explains why\nlarger models outperform smaller ones even when the smaller models possess more\naccurate knowledge. Chain-of-thought prompting steers all models towards using\nthe numerical features across all model sizes.",
      "authors": [
        {
          "name": "Hans Hergen Lehmann",
          "affiliation": null
        },
        {
          "name": "Jae Hee Lee",
          "affiliation": null
        },
        {
          "name": "Steven Schockaert",
          "affiliation": null
        },
        {
          "name": "Stefan Wermter",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16815v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16815v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16814v1",
      "title": "Needles in the Landscape: Semi-Supervised Pseudolabeling for Archaeological Site Discovery under Label Scarcity",
      "abstract": "Archaeological predictive modelling estimates where undiscovered sites are\nlikely to occur by combining known locations with environmental, cultural, and\ngeospatial variables. We address this challenge using a deep learning approach\nbut must contend with structural label scarcity inherent to archaeology:\npositives are rare, and most locations are unlabeled. To address this, we adopt\na semi-supervised, positive-unlabeled (PU) learning strategy, implemented as a\nsemantic segmentation model and evaluated on two datasets covering a\nrepresentative range of archaeological periods. Our approach employs dynamic\npseudolabeling, refined with a Conditional Random Field (CRF) implemented via\nan RNN, increasing label confidence under severe class imbalance. On a\ngeospatial dataset derived from a digital elevation model (DEM), our model\nperforms on par with the state-of-the-art, LAMAP, while achieving higher Dice\nscores. On raw satellite imagery, assessed end-to-end with stratified k-fold\ncross-validation, it maintains performance and yields predictive surfaces with\nimproved interpretability. Overall, our results indicate that semi-supervised\nlearning offers a promising approach to identifying undiscovered sites across\nlarge, sparsely annotated landscapes.",
      "authors": [
        {
          "name": "Simon Jaxy",
          "affiliation": null
        },
        {
          "name": "Anton Theys",
          "affiliation": null
        },
        {
          "name": "Patrick Willett",
          "affiliation": null
        },
        {
          "name": "W. Chris Carleton",
          "affiliation": null
        },
        {
          "name": "Ralf Vandam",
          "affiliation": null
        },
        {
          "name": "Pieter Libin",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16814v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16814v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16811v1",
      "title": "Graph Learning is Suboptimal in Causal Bandits",
      "abstract": "We study regret minimization in causal bandits under causal sufficiency where\nthe underlying causal structure is not known to the agent. Previous work has\nfocused on identifying the reward's parents and then applying classic bandit\nmethods to them, or jointly learning the parents while minimizing regret. We\ninvestigate whether such strategies are optimal. Somewhat counterintuitively,\nour results show that learning the parent set is suboptimal. We do so by\nproving that there exist instances where regret minimization and parent\nidentification are fundamentally conflicting objectives. We further analyze\nboth the known and unknown parent set size regimes, establish novel regret\nlower bounds that capture the combinatorial structure of the action space.\nBuilding on these insights, we propose nearly optimal algorithms that bypass\ngraph and parent recovery, demonstrating that parent identification is indeed\nunnecessary for regret minimization. Experiments confirm that there exists a\nlarge performance gap between our method and existing baselines in various\nenvironments.",
      "authors": [
        {
          "name": "Mohammad Shahverdikondori",
          "affiliation": null
        },
        {
          "name": "Jalal Etesami",
          "affiliation": null
        },
        {
          "name": "Negar Kiyavash",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16811v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16811v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16809v1",
      "title": "When Many-Shot Prompting Fails: An Empirical Study of LLM Code Translation",
      "abstract": "Large Language Models (LLMs) with vast context windows offer new avenues for\nin-context learning (ICL), where providing many examples (\"many-shot\"\nprompting) is often assumed to enhance performance. We investigate this\nassumption for the complex task of code translation. Through a large-scale\nempirical study of over 90,000 translations, we systematically evaluate the\nimpact of scaling in-context examples from zero-shot to many-shot\nconfigurations of up to 625 examples, with prompts spanning from approximately\n100,000 to 800,000 tokens. Our findings reveal a \"many-shot paradox\": while\nstatic similarity metrics may modestly improve with more examples, functional\ncorrectness consistently peaks with few-shot prompting (5-25 examples).\nProviding substantially more examples often degrades this crucial functional\nperformance. This study highlights that for code translation, the quality of a\nfew well-chosen examples outweighs sheer quantity, challenging the universal\nefficacy of \"more is better\" for ICL and underscoring the task-dependent nature\nof optimal prompting strategies. Our results have significant implications for\neffectively leveraging LLMs in software engineering.",
      "authors": [
        {
          "name": "Amirkia Rafiei Oskooei",
          "affiliation": null
        },
        {
          "name": "Kaan Baturalp Cosdan",
          "affiliation": null
        },
        {
          "name": "Husamettin Isiktas",
          "affiliation": null
        },
        {
          "name": "Mehmet S. Aktas",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL",
        "cs.PL",
        "68T50, 68N30, 68W40",
        "I.2.7; D.2.7; I.2.6"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16809v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16809v1",
      "primary_category": "cs.SE",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16807v1",
      "title": "Improving Model Representation and Reducing KV Cache via Skip Connections with First Value Heads",
      "abstract": "Transformer models have driven breakthroughs across various language tasks by\ntheir strong capability to learn rich contextual representations. Scaling them\nto improve representation, however, often demands substantial memory and\ncompute costs, such as the Key-Value (KV) cache used during auto-regressive\ndecoding. Skip connections offer a promising way to improve representation\nwithout bloating resource usage, yet most prior works either improve\nexpressivity while leaving KV costs unchanged, or reduce memory at the cost of\nweaker representation. In this work, we propose SkipV1Former, a Transformer\nvariant that uses skip connections from the first layer's Value heads to\nstrengthen model representation and reduce KV cache. Specifically, from the\nsecond block onward, each layer reuses half of its Value heads from the very\nfirst layer, while computing the other half as usual-cutting Value projections\nand V cache by nearly 50 \\%. Theoretically, we show that routing uncompressed\nfirst-layer Values into deeper layers restores information lost to compression\nand accelerates the model's implicit mesa-optimization-a key pattern of\nTransformer in auto-regressive tasks. Empirically, across different model\nscales, SkipV1Former delivers consistent reductions of approximately 25 \\% in\nKV cache while improving perplexity relative to standard Multi-Head Attention\n(MHA) Transformers and some advanced variants. Moreover, we propose a recipe\nfor uptraining existing MHA Transformer checkpoints to SkipV1Former with only\n10-15\\% additional compute. Finally, SkipV1Former can seamlessly combine\nadvanced methods like Group-Query Attention and Multi-Latent Attention to\nachieve further KV cache savings and performance improvement. When combined\nwith YOCO, it cuts KV cache size by nearly 50 \\% while still improving\nperformance.",
      "authors": [
        {
          "name": "Zhoutong Wu",
          "affiliation": null
        },
        {
          "name": "Yuan Zhang",
          "affiliation": null
        },
        {
          "name": "Yiming Dong",
          "affiliation": null
        },
        {
          "name": "Chenheng Zhang",
          "affiliation": null
        },
        {
          "name": "Cong Fang",
          "affiliation": null
        },
        {
          "name": "Kun Yuan",
          "affiliation": null
        },
        {
          "name": "Zhouchen Lin",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16807v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16807v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16806v1",
      "title": "Computational Budget Should Be Considered in Data Selection",
      "abstract": "Data selection improves computational efficiency by choosing informative\nsubsets of training samples. However, existing methods ignore the compute\nbudget, treating data selection and importance evaluation independently of\ncompute budget constraints. Yet empirical studies show no algorithm can\nconsistently outperform others (or even random selection) across varying\nbudgets. We therefore argue that compute budget must be integral to\ndata-selection strategies, since different budgets impose distinct requirements\non data quantity, quality, and distribution for effective training. To this\nend, we propose a novel Computational budget-Aware Data Selection (CADS) method\nand naturally formulate it into a bilevel optimization framework, where the\ninner loop trains the model within the constraints of the computational budget\non some selected subset of training data, while the outer loop optimizes data\nselection based on model evaluation. Our technical contributions lie in\naddressing two main challenges in solving this bilevel optimization problem:\nthe expensive Hessian matrix estimation for outer-loop gradients and the\ncomputational burden of achieving inner-loop optimality during iterations. To\nsolve the first issue, we propose a probabilistic reparameterization strategy\nand compute the gradient using a Hessian-free policy gradient estimator. To\naddress the second challenge, we transform the inner optimization problem into\na penalty term in the outer objective, further discovering that we only need to\nestimate the minimum of a one-dimensional loss to calculate the gradient,\nsignificantly improving efficiency. Extensive experiments show that our method\nachieves performance gains of up to 14.42% over baselines in vision and\nlanguage benchmarks.",
      "authors": [
        {
          "name": "Weilin Wan",
          "affiliation": null
        },
        {
          "name": "Weizhong Zhang",
          "affiliation": null
        },
        {
          "name": "Cheng Jin",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16806v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16806v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16805v1",
      "title": "Mixed-Precision Quantization for Language Models: Techniques and Prospects",
      "abstract": "The rapid scaling of language models (LMs) has resulted in unprecedented\ncomputational, memory, and energy requirements, making their training and\ndeployment increasingly unsustainable. Quantization has emerged as an essential\ncompression technique to reduce model size, alleviate memory bottlenecks, and\naccelerate inference. However, while uniform low-bit quantization (e.g., INT8,\nINT4) provides significant efficiency gains, it can degrade accuracy in\nsensitive components of transformer-based LMs. Mixed-precision quantization\noffers a promising alternative by selectively allocating precision across\nlayers or within tensors to balance efficiency and accuracy. This survey\nprovides a comprehensive overview of Mixed-Precision quantization frameworks\nfor LMs (MXPLMs). We first review quantization fundamentals, including uniform\nand non-uniform quantizers, quantization granularity, and methods widely used\nin post-training quantization. We then categorize and compare recent MXPLM\nframeworks according to their bit allocation strategies and precision\nconfigurations across weights, activations, and key-value caches. A comparative\nanalysis highlights differences in perplexity, zero-shot task performance, and\ndeployment trade-offs. Furthermore, we contrast MXPLMs with earlier\nmixed-precision quantization methods for deep neural networks, identifying\nstrategies that transfer and those that face challenges in the LM setting.\nFinally, we summarize open issues and future directions, including\nhardware-aware design, activation quantization, and scalable optimization\nmethods for billion-parameter models. By consolidating recent advances, this\nwork serves as a reference for understanding the current landscape and research\nprospects of mixed-precision quantization for large-scale language models.",
      "authors": [
        {
          "name": "Mariam Rakka",
          "affiliation": null
        },
        {
          "name": "Marios Fournarakis",
          "affiliation": null
        },
        {
          "name": "Olga Krestinskaya",
          "affiliation": null
        },
        {
          "name": "Jinane Bazzi",
          "affiliation": null
        },
        {
          "name": "Khaled N. Salama",
          "affiliation": null
        },
        {
          "name": "Fadi Kurdahi",
          "affiliation": null
        },
        {
          "name": "Ahmed M. Eltawil",
          "affiliation": null
        },
        {
          "name": "Mohammed E. Fouda",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16805v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16805v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16802v1",
      "title": "Domain-Contextualized Concept Graphs: A Computable Framework for Knowledge Representation",
      "abstract": "Traditional knowledge graphs are constrained by fixed ontologies that\norganize concepts within rigid hierarchical structures. The root cause lies in\ntreating domains as implicit context rather than as explicit, reasoning-level\ncomponents. To overcome these limitations, we propose the Domain-Contextualized\nConcept Graph (CDC), a novel knowledge modeling framework that elevates domains\nto first-class elements of conceptual representation. CDC adopts a C-D-C triple\nstructure - <Concept, Relation@Domain, Concept'> - where domain specifications\nserve as dynamic classification dimensions defined on demand. Grounded in a\ncognitive-linguistic isomorphic mapping principle, CDC operationalizes how\nhumans understand concepts through contextual frames. We formalize more than\ntwenty standardized relation predicates (structural, logical, cross-domain, and\ntemporal) and implement CDC in Prolog for full inference capability. Case\nstudies in education, enterprise knowledge systems, and technical documentation\ndemonstrate that CDC enables context-aware reasoning, cross-domain analogy, and\npersonalized knowledge modeling - capabilities unattainable under traditional\nontology-based frameworks.",
      "authors": [
        {
          "name": "Chao Li",
          "affiliation": null
        },
        {
          "name": "Yuru Wang",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.AI"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16802v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16802v1",
      "primary_category": "cs.AI",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16800v1",
      "title": "An RGB-D Image Dataset for Lychee Detection and Maturity Classification for Robotic Harvesting",
      "abstract": "Lychee is a high-value subtropical fruit. The adoption of vision-based\nharvesting robots can significantly improve productivity while reduce reliance\non labor. High-quality data are essential for developing such harvesting\nrobots. However, there are currently no consistently and comprehensively\nannotated open-source lychee datasets featuring fruits in natural growing\nenvironments. To address this, we constructed a dataset to facilitate lychee\ndetection and maturity classification. Color (RGB) images were acquired under\ndiverse weather conditions, and at different times of the day, across multiple\nlychee varieties, such as Nuomici, Feizixiao, Heiye, and Huaizhi. The dataset\nencompasses three different ripeness stages and contains 11,414 images,\nconsisting of 878 raw RGB images, 8,780 augmented RGB images, and 1,756 depth\nimages. The images are annotated with 9,658 pairs of lables for lychee\ndetection and maturity classification. To improve annotation consistency, three\nindividuals independently labeled the data, and their results were then\naggregated and verified by a fourth reviewer. Detailed statistical analyses\nwere done to examine the dataset. Finally, we performed experiments using three\nrepresentative deep learning models to evaluate the dataset. It is publicly\navailable for academic",
      "authors": [
        {
          "name": "Zhenpeng Zhang",
          "affiliation": null
        },
        {
          "name": "Yi Wang",
          "affiliation": null
        },
        {
          "name": "Shanglei Chai",
          "affiliation": null
        },
        {
          "name": "Yingying Liu",
          "affiliation": null
        },
        {
          "name": "Zekai Xie",
          "affiliation": null
        },
        {
          "name": "Wenhao Huang",
          "affiliation": null
        },
        {
          "name": "Pengyu Li",
          "affiliation": null
        },
        {
          "name": "Zipei Luo",
          "affiliation": null
        },
        {
          "name": "Dajiang Lu",
          "affiliation": null
        },
        {
          "name": "Yibin Tian",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16800v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16800v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16797v1",
      "title": "MOSAIC: Masked Objective with Selective Adaptation for In-domain Contrastive Learning",
      "abstract": "We introduce MOSAIC (Masked Objective with Selective Adaptation for In-domain\nContrastive learning), a multi-stage framework for domain adaptation of\nsentence embedding models that incorporates joint domain-specific masked\nsupervision. Our approach addresses the challenges of adapting large-scale\ngeneral-domain sentence embedding models to specialized domains. By jointly\noptimizing masked language modeling (MLM) and contrastive objectives within a\nunified training pipeline, our method enables effective learning of\ndomain-relevant representations while preserving the robust semantic\ndiscrimination properties of the original model. We empirically validate our\napproach on both high-resource and low-resource domains, achieving improvements\nup to 13.4% in NDCG@10 (Normalized Discounted Cumulative Gain) over strong\ngeneral-domain baselines. Comprehensive ablation studies further demonstrate\nthe effectiveness of each component, highlighting the importance of balanced\njoint supervision and staged adaptation.",
      "authors": [
        {
          "name": "Vera Pavlova",
          "affiliation": null
        },
        {
          "name": "Mohammed Makhlouf",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16797v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16797v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16794v1",
      "title": "Black-box Optimization of LLM Outputs by Asking for Directions",
      "abstract": "We present a novel approach for attacking black-box large language models\n(LLMs) by exploiting their ability to express confidence in natural language.\nExisting black-box attacks require either access to continuous model outputs\nlike logits or confidence scores (which are rarely available in practice), or\nrely on proxy signals from other models. Instead, we demonstrate how to prompt\nLLMs to express their internal confidence in a way that is sufficiently\ncalibrated to enable effective adversarial optimization. We apply our general\nmethod to three attack scenarios: adversarial examples for vision-LLMs,\njailbreaks and prompt injections. Our attacks successfully generate malicious\ninputs against systems that only expose textual outputs, thereby dramatically\nexpanding the attack surface for deployed LLMs. We further find that better and\nlarger models exhibit superior calibration when expressing confidence, creating\na concerning security paradox where model capability improvements directly\nenhance vulnerability. Our code is available at this\n[link](https://github.com/zj-jayzhang/black_box_llm_optimization).",
      "authors": [
        {
          "name": "Jie Zhang",
          "affiliation": null
        },
        {
          "name": "Meng Ding",
          "affiliation": null
        },
        {
          "name": "Yang Liu",
          "affiliation": null
        },
        {
          "name": "Jue Hong",
          "affiliation": null
        },
        {
          "name": "Florian Tram\u00e8r",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CR",
        "cs.LG"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16794v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16794v1",
      "primary_category": "cs.CR",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16791v1",
      "title": "Personalized Image Filter: Mastering Your Photographic Style",
      "abstract": "Photographic style, as a composition of certain photographic concepts, is the\ncharm behind renowned photographers. But learning and transferring photographic\nstyle need a profound understanding of how the photo is edited from the unknown\noriginal appearance. Previous works either fail to learn meaningful\nphotographic concepts from reference images, or cannot preserve the content of\nthe content image. To tackle these issues, we proposed a Personalized Image\nFilter (PIF). Based on a pretrained text-to-image diffusion model, the\ngenerative prior enables PIF to learn the average appearance of photographic\nconcepts, as well as how to adjust them according to text prompts. PIF then\nlearns the photographic style of reference images with the textual inversion\ntechnique, by optimizing the prompts for the photographic concepts. PIF shows\noutstanding performance in extracting and transferring various kinds of\nphotographic style. Project page: https://pif.pages.dev/",
      "authors": [
        {
          "name": "Chengxuan Zhu",
          "affiliation": null
        },
        {
          "name": "Shuchen Weng",
          "affiliation": null
        },
        {
          "name": "Jiacong Fang",
          "affiliation": null
        },
        {
          "name": "Peixuan Zhang",
          "affiliation": null
        },
        {
          "name": "Si Li",
          "affiliation": null
        },
        {
          "name": "Chao Xu",
          "affiliation": null
        },
        {
          "name": "Boxin Shi",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16791v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16791v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16790v1",
      "title": "Unsupervised Monocular Road Segmentation for Autonomous Driving via Scene Geometry",
      "abstract": "This paper presents a fully unsupervised approach for binary road\nsegmentation (road vs. non-road), eliminating the reliance on costly manually\nlabeled datasets. The method leverages scene geometry and temporal cues to\ndistinguish road from non-road regions. Weak labels are first generated from\ngeometric priors, marking pixels above the horizon as non-road and a predefined\nquadrilateral in front of the vehicle as road. In a refinement stage, temporal\nconsistency is enforced by tracking local feature points across frames and\npenalizing inconsistent label assignments using mutual information\nmaximization. This enhances both precision and temporal stability. On the\nCityscapes dataset, the model achieves an Intersection-over-Union (IoU) of\n0.82, demonstrating high accuracy with a simple design. These findings\ndemonstrate the potential of combining geometric constraints and temporal\nconsistency for scalable unsupervised road segmentation in autonomous driving.",
      "authors": [
        {
          "name": "Sara Hatami Rostami",
          "affiliation": null
        },
        {
          "name": "Behrooz Nasihatkon",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16790v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16790v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16786v1",
      "title": "More with Less: An Empirical Study of Turn-Control Strategies for Efficient Coding Agents",
      "abstract": "LLM-powered coding agents, which operate in iterative loops (turns) to solve\nsoftware engineering tasks, are becoming increasingly powerful. However, their\npractical deployment is hindered by significant and unpredictable costs. This\nchallenge arises from a combination of factors: quadratically growing token\ncounts with each turn, the high price of models, the large number of turns\nrequired for real-world tasks, and the tendency of agents to take inefficient\nor unnecessary actions. While existing research focuses on optimizing\nindividual turns, the strategic control of the total number of turns remains an\nunderexplored area for managing agent performance and cost. To address this\ngap, we conduct a comprehensive empirical study on SWE-bench using three\nstate-of-the-art models and evaluate the impact of three distinct turn-control\nstrategies: an unrestricted baseline, a fixed-turn limit with reminders, and a\nnovel dynamic-turn strategy that grants extensions on-demand. Our findings\nfirst reveal a fundamental trade-off in the unrestricted setting, where no\nsingle model excels across performance, cost, and turn efficiency. We then show\nthat a fixed-turn limit, specifically at the 75th percentile of the baseline,\nserves as a \"sweet spot\", substantially reducing costs (by 24%-68%) with\nminimal impact on solve rates. Most significantly, the dynamic-turn strategy\nconsistently outperforms fixed-limit approaches, achieving comparable or better\nsolve rates while further reducing costs by an additional 12%-24% by\nintelligently allocating resources only to tasks that need them. This work\nprovides the first systematic analysis of turn-control strategies, offering\nsimple yet effective guidelines for developers to balance cost and efficacy. We\ndemonstrate that dynamic resource allocation is a superior, easy-to-implement\napproach for deploying powerful yet economically viable coding agents.",
      "authors": [
        {
          "name": "Pengfei Gao",
          "affiliation": null
        },
        {
          "name": "Chao Peng",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16786v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16786v1",
      "primary_category": "cs.SE",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16785v1",
      "title": "Segmentation as A Plug-and-Play Capability for Frozen Multimodal LLMs",
      "abstract": "Integrating diverse visual capabilities into a unified model is a significant\ntrend in Multimodal Large Language Models (MLLMs). Among these, the inclusion\nof segmentation poses a distinct set of challenges. To equip MLLMs with\npixel-level segmentation abilities, prevailing methods require finetuning the\nmodel to produce specific outputs compatible with a mask decoder. This process\ntypically alters the model's output space and compromises its intrinsic\ngeneralization, which undermines the goal of building a unified model. We\nintroduce LENS (Leveraging kEypoiNts for MLLMs' Segmentation), a novel\nplug-and-play solution. LENS attaches a lightweight, trainable head to a\ncompletely frozen MLLM. By refining the spatial cues embedded in attention\nmaps, LENS extracts keypoints and describes them into point-wise features\ndirectly compatible with the mask decoder. Extensive experiments validate our\napproach: LENS achieves segmentation performance competitive with or superior\nto that of retraining-based methods. Crucially, it does so while fully\npreserving the MLLM's generalization capabilities, which are significantly\ndegraded by finetuning approaches. As such, the attachable design of LENS\nestablishes an efficient and powerful paradigm for extending MLLMs, paving the\nway for truly multi-talented, unified models.",
      "authors": [
        {
          "name": "Jiazhen Liu",
          "affiliation": null
        },
        {
          "name": "Long Chen",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16785v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16785v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16783v1",
      "title": "LC-Eval: A Bilingual Multi-Task Evaluation Benchmark for Long-Context Understanding",
      "abstract": "Recent advancements in Large Language Models (LLMs) have demonstrated\nsophisticated capabilities, including the ability to process and comprehend\nextended contexts. These emergent capabilities necessitate rigorous evaluation\nmethods to effectively assess their performance in long-context understanding.\nIn this paper, we present \\textbf{LC-Eval}, a bilingual, multi-task evaluation\nbenchmark designed to evaluate long-context understanding in English and\nArabic, targeting context lengths ranging from 4k to over 128k tokens. LC-Eval\nintroduces four novel and challenging tasks: multi-document question answering,\nbilingual question answering, claim verification within a paragraph, and\nmultiple-choice questions based on long contexts. These tasks are designed to\nassess LLMs' abilities in deep reasoning, document comprehension, information\ntracing, and bilingual information extraction and understanding. The benchmark\nincludes datasets in both Arabic and English for each task, allowing for a\ncomparative analysis of their performance across different text genres.\nEvaluations were conducted on both open-weight and closed LLMs, with results\nindicating that LC-Eval presents significant challenges. Even high-performing\nmodels, such as GPT-4o, struggled with certain tasks, highlighting the\ncomplexity and rigor of the benchmark.",
      "authors": [
        {
          "name": "Sheikh Jubair",
          "affiliation": null
        },
        {
          "name": "Arwa Omayrah",
          "affiliation": null
        },
        {
          "name": "Amal Alshammari",
          "affiliation": null
        },
        {
          "name": "Alhanoof Althnian",
          "affiliation": null
        },
        {
          "name": "Abdulhamed Alothaimen",
          "affiliation": null
        },
        {
          "name": "Norah A. Alzahrani",
          "affiliation": null
        },
        {
          "name": "Shahad D. Alzaidi",
          "affiliation": null
        },
        {
          "name": "Nora Al-Twairesh",
          "affiliation": null
        },
        {
          "name": "Abdulmohsen Al-Thubaity",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16783v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16783v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16782v1",
      "title": "Near-Optimal Quantum Algorithms for Computing (Coarse) Correlated Equilibria of General-Sum Games",
      "abstract": "Computing Nash equilibria of zero-sum games in classical and quantum settings\nis extensively studied. For general-sum games, computing Nash equilibria is\nPPAD-hard and the computing of a more general concept called correlated\nequilibria has been widely explored in game theory. In this paper, we initiate\nthe study of quantum algorithms for computing $\\varepsilon$-approximate\ncorrelated equilibria (CE) and coarse correlated equilibria (CCE) in\nmulti-player normal-form games. Our approach utilizes quantum improvements to\nthe multi-scale Multiplicative Weight Update (MWU) method for CE calculations,\nachieving a query complexity of $\\tilde{O}(m\\sqrt{n})$ for fixed $\\varepsilon$.\nFor CCE, we extend techniques from quantum algorithms for zero-sum games to\nmulti-player settings, achieving query complexity\n$\\tilde{O}(m\\sqrt{n}/\\varepsilon^{2.5})$. Both algorithms demonstrate a\nnear-optimal scaling in the number of players $m$ and actions $n$, as confirmed\nby our quantum query lower bounds.",
      "authors": [
        {
          "name": "Tongyang Li",
          "affiliation": null
        },
        {
          "name": "Xinzhao Wang",
          "affiliation": null
        },
        {
          "name": "Yexin Zhang",
          "affiliation": null
        }
      ],
      "categories": [
        "quant-ph",
        "cs.CC",
        "cs.GT",
        "cs.LG"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16782v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16782v1",
      "primary_category": "quant-ph",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16781v1",
      "title": "Xiaoice: Training-Free Video Understanding via Self-Supervised Spatio-Temporal Clustering of Semantic Features",
      "abstract": "The remarkable zero-shot reasoning capabilities of large-scale Visual\nLanguage Models (VLMs) on static images have yet to be fully translated to the\nvideo domain. Conventional video understanding models often rely on extensive,\ntask-specific training on annotated datasets, a process that is both costly and\nlimited in scalability. This paper introduces a novel, training-free framework\nfor video understanding that circumvents end-to-end training by synergistically\ncombining the rich semantic priors of pre-trained VLMs with classic machine\nlearning algorithms for pattern discovery. Our core idea is to reframe video\nunderstanding as a self-supervised spatio-temporal clustering problem within a\nhigh-dimensional semantic feature space. The proposed pipeline first transforms\na video stream into a semantic feature trajectory using the frozen visual\nencoder of a pre-trained VLM. Subsequently, we employ Kernel Temporal\nSegmentation (KTS), a robust machine learning technique, to partition the\ncontinuous feature stream into discrete, semantically coherent event segments.\nThese segments are then subjected to unsupervised density-based clustering to\nidentify recurring macroscopic scenes and themes throughout the video. By\nselecting representative keyframes from each discovered cluster and leveraging\nthe VLM's generative capabilities for textual description, our framework\nautomatically produces a structured, multi-modal summary of the video content.\nThis approach provides an effective, interpretable, and model-agnostic pathway\nfor zero-shot, automated structural analysis of video content.",
      "authors": [
        {
          "name": "Shihao Ji",
          "affiliation": null
        },
        {
          "name": "Zihui Song",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16781v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16781v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16780v1",
      "title": "3D-GSRD: 3D Molecular Graph Auto-Encoder with Selective Re-mask Decoding",
      "abstract": "Masked graph modeling (MGM) is a promising approach for molecular\nrepresentation learning (MRL).However, extending the success of re-mask\ndecoding from 2D to 3D MGM is non-trivial, primarily due to two conflicting\nchallenges: avoiding 2D structure leakage to the decoder, while still providing\nsufficient 2D context for reconstructing re-masked atoms.To address these\nchallenges, we propose 3D-GSRD: a 3D Molecular Graph Auto-Encoder with\nSelective Re-mask Decoding. The core innovation of 3D-GSRD lies in its\nSelective Re-mask Decoding(SRD), which re-masks only 3D-relevant information\nfrom encoder representations while preserving the 2D graph structures.This SRD\nis synergistically integrated with a 3D Relational-Transformer(3D-ReTrans)\nencoder alongside a structure-independent decoder. We analyze that SRD,\ncombined with the structure-independent decoder, enhances the encoder's role in\nMRL. Extensive experiments show that 3D-GSRD achieves strong downstream\nperformance, setting a new state-of-the-art on 7 out of 8 targets in the widely\nused MD17 molecular property prediction benchmark. The code is released at\nhttps://github.com/WuChang0124/3D-GSRD.",
      "authors": [
        {
          "name": "Chang Wu",
          "affiliation": null
        },
        {
          "name": "Zhiyuan Liu",
          "affiliation": null
        },
        {
          "name": "Wen Shu",
          "affiliation": null
        },
        {
          "name": "Liang Wang",
          "affiliation": null
        },
        {
          "name": "Yanchen Luo",
          "affiliation": null
        },
        {
          "name": "Wenqiang Lei",
          "affiliation": null
        },
        {
          "name": "Yatao Bian",
          "affiliation": null
        },
        {
          "name": "Junfeng Fang",
          "affiliation": null
        },
        {
          "name": "Xiang Wang",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16780v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16780v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16777v1",
      "title": "GS2POSE: Marry Gaussian Splatting to 6D Object Pose Estimation",
      "abstract": "Accurate 6D pose estimation of 3D objects is a fundamental task in computer\nvision, and current research typically predicts the 6D pose by establishing\ncorrespondences between 2D image features and 3D model features. However, these\nmethods often face difficulties with textureless objects and varying\nillumination conditions. To overcome these limitations, we propose GS2POSE, a\nnovel approach for 6D object pose estimation. GS2POSE formulates a pose\nregression algorithm inspired by the principles of Bundle Adjustment (BA). By\nleveraging Lie algebra, we extend the capabilities of 3DGS to develop a\npose-differentiable rendering pipeline, which iteratively optimizes the pose by\ncomparing the input image to the rendered image. Additionally, GS2POSE updates\ncolor parameters within the 3DGS model, enhancing its adaptability to changes\nin illumination. Compared to previous models, GS2POSE demonstrates accuracy\nimprovements of 1.4\\%, 2.8\\% and 2.5\\% on the T-LESS, LineMod-Occlusion and\nLineMod datasets, respectively.",
      "authors": [
        {
          "name": "Junbo Li",
          "affiliation": null
        },
        {
          "name": "Weimin Yuan",
          "affiliation": null
        },
        {
          "name": "Yinuo Wang",
          "affiliation": null
        },
        {
          "name": "Yue Zeng",
          "affiliation": null
        },
        {
          "name": "Shihao Shu",
          "affiliation": null
        },
        {
          "name": "Cai Meng",
          "affiliation": null
        },
        {
          "name": "Xiangzhi Bai",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16777v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16777v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16776v1",
      "title": "EMRRG: Efficient Fine-Tuning Pre-trained X-ray Mamba Networks for Radiology Report Generation",
      "abstract": "X-ray image-based medical report generation (MRG) is a pivotal area in\nartificial intelligence that can significantly reduce diagnostic burdens for\nclinicians and patient wait times. Existing MRG models predominantly rely on\nLarge Language Models (LLMs) to improve report generation, with limited\nexploration of pre-trained vision foundation models or advanced fine-tuning\ntechniques. Mainstream frameworks either avoid fine-tuning or utilize\nsimplistic methods like LoRA, often neglecting the potential of enhancing\ncross-attention mechanisms. Additionally, while Transformer-based models\ndominate vision-language tasks, non-Transformer architectures, such as the\nMamba network, remain underexplored for medical report generation, presenting a\npromising avenue for future research. In this paper, we propose EMRRG, a novel\nX-ray report generation framework that fine-tunes pre-trained Mamba networks\nusing parameter-efficient methods. Specifically, X-ray images are divided into\npatches, tokenized, and processed by an SSM-based vision backbone for feature\nextraction, with Partial LoRA yielding optimal performance. An LLM with a\nhybrid decoder generates the medical report, enabling end-to-end training and\nachieving strong results on benchmark datasets. Extensive experiments on three\nwidely used benchmark datasets fully validated the effectiveness of our\nproposed strategies for the X-ray MRG. The source code of this paper will be\nreleased on https://github.com/Event-AHU/Medical_Image_Analysis.",
      "authors": [
        {
          "name": "Mingzheng Zhang",
          "affiliation": null
        },
        {
          "name": "Jinfeng Gao",
          "affiliation": null
        },
        {
          "name": "Dan Xu",
          "affiliation": null
        },
        {
          "name": "Jiangrui Yu",
          "affiliation": null
        },
        {
          "name": "Yuhan Qiao",
          "affiliation": null
        },
        {
          "name": "Lan Chen",
          "affiliation": null
        },
        {
          "name": "Jin Tang",
          "affiliation": null
        },
        {
          "name": "Xiao Wang",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16776v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16776v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16774v1",
      "title": "Learning to play: A Multimodal Agent for 3D Game-Play",
      "abstract": "We argue that 3-D first-person video games are a challenging environment for\nreal-time multi-modal reasoning. We first describe our dataset of human\ngame-play, collected across a large variety of 3-D first-person games, which is\nboth substantially larger and more diverse compared to prior publicly disclosed\ndatasets, and contains text instructions. We demonstrate that we can learn an\ninverse dynamics model from this dataset, which allows us to impute actions on\na much larger dataset of publicly available videos of human game play that lack\nrecorded actions. We then train a text-conditioned agent for game playing using\nbehavior cloning, with a custom architecture capable of realtime inference on a\nconsumer GPU. We show the resulting model is capable of playing a variety of\n3-D games and responding to text input. Finally, we outline some of the\nremaining challenges such as long-horizon tasks and quantitative evaluation\nacross a large set of games.",
      "authors": [
        {
          "name": "Yuguang Yue",
          "affiliation": null
        },
        {
          "name": "Irakli Salia",
          "affiliation": null
        },
        {
          "name": "Samuel Hunt",
          "affiliation": null
        },
        {
          "name": "Christopher Green",
          "affiliation": null
        },
        {
          "name": "Wenzhe Shi",
          "affiliation": null
        },
        {
          "name": "Jonathan J Hunt",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16774v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16774v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16772v1",
      "title": "Region in Context: Text-condition Image editing with Human-like semantic reasoning",
      "abstract": "Recent research has made significant progress in localizing and editing image\nregions based on text. However, most approaches treat these regions in\nisolation, relying solely on local cues without accounting for how each part\ncontributes to the overall visual and semantic composition. This often results\nin inconsistent edits, unnatural transitions, or loss of coherence across the\nimage. In this work, we propose Region in Context, a novel framework for\ntext-conditioned image editing that performs multilevel semantic alignment\nbetween vision and language, inspired by the human ability to reason about\nedits in relation to the whole scene. Our method encourages each region to\nunderstand its role within the global image context, enabling precise and\nharmonized changes. At its core, the framework introduces a dual-level guidance\nmechanism: regions are represented with full-image context and aligned with\ndetailed region-level descriptions, while the entire image is simultaneously\nmatched to a comprehensive scene-level description generated by a large\nvision-language model. These descriptions serve as explicit verbal references\nof the intended content, guiding both local modifications and global structure.\nExperiments show that it produces more coherent and instruction-aligned\nresults. Code is available at:\nhttps://github.com/thuyvuphuong/Region-in-Context.git",
      "authors": [
        {
          "name": "Thuy Phuong Vu",
          "affiliation": null
        },
        {
          "name": "Dinh-Cuong Hoang",
          "affiliation": null
        },
        {
          "name": "Minhhuy Le",
          "affiliation": null
        },
        {
          "name": "Phan Xuan Tan",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16772v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16772v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16769v1",
      "title": "See or Say Graphs: Agent-Driven Scalable Graph Understanding with Vision-Language Models",
      "abstract": "Vision-language models (VLMs) have shown promise in graph understanding, but\nremain limited by input-token constraints, facing scalability bottlenecks and\nlacking effective mechanisms to coordinate textual and visual modalities. To\naddress these challenges, we propose GraphVista, a unified framework that\nenhances both scalability and modality coordination in graph understanding. For\nscalability, GraphVista organizes graph information hierarchically into a\nlightweight GraphRAG base, which retrieves only task-relevant textual\ndescriptions and high-resolution visual subgraphs, compressing redundant\ncontext while preserving key reasoning elements. For modality coordination,\nGraphVista introduces a planning agent that routes tasks to the most suitable\nmodality-using the text modality for simple property reasoning and the visual\nmodality for local and structurally complex reasoning grounded in explicit\ntopology. Extensive experiments demonstrate that GraphVista scales to large\ngraphs, up to $200\\times$ larger than those used in existing benchmarks, and\nconsistently outperforms existing textual, visual, and fusion-based methods,\nachieving up to $4.4\\times$ quality improvement over the state-of-the-art\nbaselines by fully exploiting the complementary strengths of both modalities.",
      "authors": [
        {
          "name": "Shuo Han",
          "affiliation": null
        },
        {
          "name": "Yukun Cao",
          "affiliation": null
        },
        {
          "name": "Zezhong Ding",
          "affiliation": null
        },
        {
          "name": "Zengyi Gao",
          "affiliation": null
        },
        {
          "name": "S Kevin Zhou",
          "affiliation": null
        },
        {
          "name": "Xike Xie",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16769v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16769v1",
      "primary_category": "cs.AI",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16765v1",
      "title": "WaMaIR: Image Restoration via Multiscale Wavelet Convolutions and Mamba-based Channel Modeling with Texture Enhancement",
      "abstract": "Image restoration is a fundamental and challenging task in computer vision,\nwhere CNN-based frameworks demonstrate significant computational efficiency.\nHowever, previous CNN-based methods often face challenges in adequately\nrestoring fine texture details, which are limited by the small receptive field\nof CNN structures and the lack of channel feature modeling. In this paper, we\npropose WaMaIR, which is a novel framework with a large receptive field for\nimage perception and improves the reconstruction of texture details in restored\nimages. Specifically, we introduce the Global Multiscale Wavelet Transform\nConvolutions (GMWTConvs) for expandding the receptive field to extract image\nfeatures, preserving and enriching texture features in model inputs. Meanwhile,\nwe propose the Mamba-Based Channel-Aware Module (MCAM), explicitly designed to\ncapture long-range dependencies within feature channels, which enhancing the\nmodel sensitivity to color, edges, and texture information. Additionally, we\npropose Multiscale Texture Enhancement Loss (MTELoss) for image restoration to\nguide the model in preserving detailed texture structures effectively.\nExtensive experiments confirm that WaMaIR outperforms state-of-the-art methods,\nachieving better image restoration and efficient computational performance of\nthe model.",
      "authors": [
        {
          "name": "Shengyu Zhu",
          "affiliation": null
        },
        {
          "name": "Fan",
          "affiliation": null
        },
        {
          "name": "Fuxuan Zhang",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16765v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16765v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16761v1",
      "title": "Enhancing Language Agent Strategic Reasoning through Self-Play in Adversarial Games",
      "abstract": "Existing language agents often encounter difficulties in dynamic adversarial\ngames due to poor strategic reasoning. To mitigate this limitation, a promising\napproach is to allow agents to learn from game interactions automatically,\nwithout relying on costly expert-labeled data. Unlike static environments where\nagents receive fixed feedback or rewards, selecting appropriate opponents in\ndynamic adversarial games can significantly impact learning performance.\nHowever, the discussion of opponents in adversarial environments remains an\narea under exploration. In this paper, we propose a Step-level poliCy\nOptimization method through Play-And-Learn, SCO-PAL. Leveraging SCO-PAL, we\nconduct a detailed analysis of opponent selection by setting opponents at\ndifferent levels and find that self-play is the most effective way to improve\nstrategic reasoning in such adversarial environments. Utilizing SCO-PAL with\nself-play, we increase the average win rate against four opponents by\napproximately 30% compared to baselines and achieve a 54.76% win rate against\nGPT-4 in six adversarial games.",
      "authors": [
        {
          "name": "Yikai Zhang",
          "affiliation": null
        },
        {
          "name": "Ye Rong",
          "affiliation": null
        },
        {
          "name": "Siyu Yuan",
          "affiliation": null
        },
        {
          "name": "Jiangjie Chen",
          "affiliation": null
        },
        {
          "name": "Jian Xie",
          "affiliation": null
        },
        {
          "name": "Yanghua Xiao",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16761v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16761v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16757v1",
      "title": "SAMOSA: Sharpness Aware Minimization for Open Set Active learning",
      "abstract": "Modern machine learning solutions require extensive data collection where\nlabeling remains costly. To reduce this burden, open set active learning\napproaches aim to select informative samples from a large pool of unlabeled\ndata that includes irrelevant or unknown classes. In this context, we propose\nSharpness Aware Minimization for Open Set Active Learning (SAMOSA) as an\neffective querying algorithm. Building on theoretical findings concerning the\nimpact of data typicality on the generalization properties of traditional\nstochastic gradient descent (SGD) and sharpness-aware minimization (SAM),\nSAMOSA actively queries samples based on their typicality. SAMOSA effectively\nidentifies atypical samples that belong to regions of the embedding manifold\nclose to the model decision boundaries. Therefore, SAMOSA prioritizes the\nsamples that are (i) highly informative for the targeted classes, and (ii)\nuseful for distinguishing between targeted and unwanted classes. Extensive\nexperiments show that SAMOSA achieves up to 3% accuracy improvement over the\nstate of the art across several datasets, while not introducing computational\noverhead. The source code of our experiments is available at:\nhttps://anonymous.4open.science/r/samosa-DAF4",
      "authors": [
        {
          "name": "Young In Kim",
          "affiliation": null
        },
        {
          "name": "Andrea Agiollo",
          "affiliation": null
        },
        {
          "name": "Rajiv Khanna",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16757v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16757v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16756v1",
      "title": "End-to-end Listen, Look, Speak and Act",
      "abstract": "Human interaction is inherently multimodal and full-duplex: we listen while\nwatching, speak while acting, and fluidly adapt to turn-taking and\ninterruptions. Realizing these capabilities is essential for building models\nsimulating humans. We present ELLSA (End-to-end Listen, Look, Speak and Act),\nwhich, to our knowledge, is the first full-duplex, end-to-end model that\nsimultaneously perceives and generates across vision, text, speech, and action\nwithin a single architecture, enabling interaction patterns previously out of\nreach, yielding more natural, human-like behaviors. At its core is a novel\nSA-MoE architecture (Self-Attention Mixture-of-Experts) that routes each\nmodality to specialized experts and fuses them through a unified attention\nbackbone. This provides a generalizable solution for joint multimodal\nperception and concurrent generation, leveraging strong pre-trained components\nwhile enabling efficient modality integration and mitigating modality\ninterference. On speech-interaction and robot-manipulation benchmarks, ELLSA\nmatches modality-specific baselines, while uniquely supporting advanced\nmultimodal and full-duplex behaviors such as dialogue and action turn-taking,\ndefective instruction rejection, speaking-while-acting, context-grounded visual\nquestion answering, and action barge-ins. We contend that ELLSA represents a\nstep toward more natural and general interactive intelligence, contributing to\nthe broader pursuit of artificial general intelligence. All data, code and\nmodel checkpoints will be released upon acceptance.",
      "authors": [
        {
          "name": "Siyin Wang",
          "affiliation": null
        },
        {
          "name": "Wenyi Yu",
          "affiliation": null
        },
        {
          "name": "Xianzhao Chen",
          "affiliation": null
        },
        {
          "name": "Xiaohai Tian",
          "affiliation": null
        },
        {
          "name": "Jun Zhang",
          "affiliation": null
        },
        {
          "name": "Lu Lu",
          "affiliation": null
        },
        {
          "name": "Chao Zhang",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.RO",
        "eess.AS"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16756v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16756v1",
      "primary_category": "cs.AI",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16753v1",
      "title": "ELMM: Efficient Lightweight Multimodal Large Language Models for Multimodal Knowledge Graph Completion",
      "abstract": "Multimodal Knowledge Graphs (MKGs) extend traditional knowledge graphs by\nincorporating visual and textual modalities, enabling richer and more\nexpressive entity representations. However, existing MKGs often suffer from\nincompleteness, which hinder their effectiveness in downstream tasks.\nTherefore, multimodal knowledge graph completion (MKGC) task is receiving\nincreasing attention. While large language models (LLMs) have shown promise for\nknowledge graph completion (KGC), their application to the multimodal setting\nremains underexplored. Moreover, applying Multimodal Large Language Models\n(MLLMs) to the task of MKGC introduces significant challenges: (1) the large\nnumber of image tokens per entity leads to semantic noise and modality\nconflicts, and (2) the high computational cost of processing large token\ninputs. To address these issues, we propose Efficient Lightweight Multimodal\nLarge Language Models (ELMM) for MKGC. ELMM proposes a Multi-view Visual Token\nCompressor (MVTC) based on multi-head attention mechanism, which adaptively\ncompresses image tokens from both textual and visual views, thereby effectively\nreducing redundancy while retaining necessary information and avoiding modality\nconflicts. Additionally, we design an attention pruning strategy to remove\nredundant attention layers from MLLMs, thereby significantly reducing the\ninference cost. We further introduce a linear projection to compensate for the\nperformance degradation caused by pruning. Extensive experiments on benchmark\nFB15k-237-IMG and WN18-IMG demonstrate that ELMM achieves state-of-the-art\nperformance while substantially improving computational efficiency,\nestablishing a new paradigm for multimodal knowledge graph completion.",
      "authors": [
        {
          "name": "Wei Huang",
          "affiliation": null
        },
        {
          "name": "Peining Li",
          "affiliation": null
        },
        {
          "name": "Meiyu Liang",
          "affiliation": null
        },
        {
          "name": "Xu Hou",
          "affiliation": null
        },
        {
          "name": "Junping Du",
          "affiliation": null
        },
        {
          "name": "Yingxia Shao",
          "affiliation": null
        },
        {
          "name": "Guanhua Ye",
          "affiliation": null
        },
        {
          "name": "Wu Liu",
          "affiliation": null
        },
        {
          "name": "Kangkang Lu",
          "affiliation": null
        },
        {
          "name": "Yang Yu",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.AI",
        "68T30",
        "H.3.3"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16753v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16753v1",
      "primary_category": "cs.AI",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16752v1",
      "title": "Prominence-Aware Artifact Detection and Dataset for Image Super-Resolution",
      "abstract": "Generative image super-resolution (SR) is rapidly advancing in visual quality\nand detail restoration. As the capacity of SR models expands, however, so does\ntheir tendency to produce artifacts: incorrect, visually disturbing details\nthat reduce perceived quality. Crucially, their perceptual impact varies: some\nartifacts are barely noticeable while others strongly degrade the image. We\nargue that artifacts should be characterized by their prominence to human\nobservers rather than treated as uniform binary defects. Motivated by this, we\npresent a novel dataset of 1302 artifact examples from 11 contemporary image-SR\nmethods, where each artifact is paired with a crowdsourced prominence score.\nBuilding on this dataset, we train a lightweight regressor that produces\nspatial prominence heatmaps and outperforms existing methods at detecting\nprominent artifacts. We release the dataset and code to facilitate\nprominence-aware evaluation and mitigation of SR artifacts.",
      "authors": [
        {
          "name": "Ivan Molodetskikh",
          "affiliation": null
        },
        {
          "name": "Kirill Malyshev",
          "affiliation": null
        },
        {
          "name": "Mark Mirgaleev",
          "affiliation": null
        },
        {
          "name": "Nikita Zagainov",
          "affiliation": null
        },
        {
          "name": "Evgeney Bogatyrev",
          "affiliation": null
        },
        {
          "name": "Dmitriy Vatolin",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16752v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16752v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16751v1",
      "title": "Visual Autoregressive Models Beat Diffusion Models on Inference Time Scaling",
      "abstract": "While inference-time scaling through search has revolutionized Large Language\nModels, translating these gains to image generation has proven difficult.\nRecent attempts to apply search strategies to continuous diffusion models show\nlimited benefits, with simple random sampling often performing best. We\ndemonstrate that the discrete, sequential nature of visual autoregressive\nmodels enables effective search for image generation. We show that beam search\nsubstantially improves text-to-image generation, enabling a 2B parameter\nautoregressive model to outperform a 12B parameter diffusion model across\nbenchmarks. Systematic ablations show that this advantage comes from the\ndiscrete token space, which allows early pruning and computational reuse, and\nour verifier analysis highlights trade-offs between speed and reasoning\ncapability. These findings suggest that model architecture, not just scale, is\ncritical for inference-time optimization in visual generation.",
      "authors": [
        {
          "name": "Erik Riise",
          "affiliation": null
        },
        {
          "name": "Mehmet Onurcan Kaya",
          "affiliation": null
        },
        {
          "name": "Dim P. Papadopoulos",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16751v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16751v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16750v1",
      "title": "On Robust hypothesis testing with respect to Hellinger distance",
      "abstract": "We study the hypothesis testing problem where the observed samples need not\ncome from either of the specified hypotheses (distributions). In such a\nsituation, we would like our test to be robust to this misspecification and\noutput the distribution closer in Hellinger distance. If the underlying\ndistribution is close to being equidistant from the hypotheses, then this would\nnot be possible. Our main result is quantifying how close the underlying\ndistribution has to be to either of the hypotheses. We also study the composite\ntesting problem, where each hypothesis is a Hellinger ball around a fixed\ndistribution. A generalized likelihood ratio test is known to work for this\nproblem. We give an alternate test for the same.",
      "authors": [
        {
          "name": "Eeshan Modak",
          "affiliation": null
        }
      ],
      "categories": [
        "math.ST",
        "cs.IT",
        "math.IT",
        "stat.ML",
        "stat.TH"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16750v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16750v1",
      "primary_category": "math.ST",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16747v1",
      "title": "An Efficient Semantic Segmentation Decoder for In-Car or Distributed Applications",
      "abstract": "Modern automotive systems leverage deep neural networks (DNNs) for semantic\nsegmentation and operate in two key application areas: (1) In-car, where the\nDNN solely operates in the vehicle without strict constraints on the data rate.\n(2) Distributed, where one DNN part operates in the vehicle and the other part\ntypically on a large-scale cloud platform with a particular constraint on\ntransmission bitrate efficiency. Typically, both applications share an image\nand source encoder, while each uses distinct (joint) source and task decoders.\nPrior work utilized convolutional neural networks for joint source and task\ndecoding but did not investigate transformer-based alternatives such as\nSegDeformer, which offer superior performance at the cost of higher\ncomputational complexity. In this work, we propose joint feature and task\ndecoding for SegDeformer, thereby enabling lower computational complexity in\nboth in-car and distributed applications, despite SegDeformer's computational\ndemands. This improves scalability in the cloud while reducing in-car\ncomputational complexity. For the in-car application, we increased the frames\nper second (fps) by up to a factor of $11.7$ ($1.4$ fps to $16.5$ fps) on\nCityscapes and by up to a factor of $3.5$ ($43.3$ fps to $154.3$ fps) on\nADE20K, while being on-par w.r.t.\\ the mean intersection over union (mIoU) of\nthe transformer-based baseline that doesn't compress by a source codec. For the\ndistributed application, we achieve state-of-the-art (SOTA) over a wide range\nof bitrates on the mIoU metric, while using only $0.14$\\% ($0.04$\\%) of cloud\nDNN parameters used in previous SOTA, reported on ADE20K (Cityscapes).",
      "authors": [
        {
          "name": "Danish Nazir",
          "affiliation": null
        },
        {
          "name": "Gowtham Sai Inti",
          "affiliation": null
        },
        {
          "name": "Timo Bartels",
          "affiliation": null
        },
        {
          "name": "Jan Piewek",
          "affiliation": null
        },
        {
          "name": "Thorsten Bagdonat",
          "affiliation": null
        },
        {
          "name": "Tim Fingscheidt",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16747v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16747v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16745v1",
      "title": "Kernel-Based Nonparametric Tests For Shape Constraints",
      "abstract": "We develop a reproducing kernel Hilbert space (RKHS) framework for\nnonparametric mean-variance optimization and inference on shape constraints of\nthe optimal rule. We derive statistical properties of the sample estimator and\nprovide rigorous theoretical guarantees, such as asymptotic consistency, a\nfunctional central limit theorem, and a finite-sample deviation bound that\nmatches the Monte Carlo rate up to regularization. Building on these findings,\nwe introduce a joint Wald-type statistic to test for shape constraints over\nfinite grids. The approach comes with an efficient computational procedure\nbased on a pivoted Cholesky factorization, facilitating scalability to large\ndatasets. Empirical tests suggest favorably of the proposed methodology.",
      "authors": [
        {
          "name": "Rohan Sen",
          "affiliation": null
        }
      ],
      "categories": [
        "stat.ML",
        "cs.LG",
        "math.ST",
        "stat.ME",
        "stat.TH",
        "62G10, 62G20, 62P05, 46E22"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16745v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16745v1",
      "primary_category": "stat.ML",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16743v1",
      "title": "Zero-Shot Performance Prediction for Probabilistic Scaling Laws",
      "abstract": "The prediction of learning curves for Natural Language Processing (NLP)\nmodels enables informed decision-making to meet specific performance\nobjectives, while reducing computational overhead and lowering the costs\nassociated with dataset acquisition and curation. In this work, we formulate\nthe prediction task as a multitask learning problem, where each task's data is\nmodelled as being organized within a two-layer hierarchy. To model the shared\ninformation and dependencies across tasks and hierarchical levels, we employ\nlatent variable multi-output Gaussian Processes, enabling to account for task\ncorrelations and supporting zero-shot prediction of learning curves (LCs). We\ndemonstrate that this approach facilitates the development of probabilistic\nscaling laws at lower costs. Applying an active learning strategy, LCs can be\nqueried to reduce predictive uncertainty and provide predictions close to\nground truth scaling laws. We validate our framework on three small-scale NLP\ndatasets with up to $30$ LCs. These are obtained from nanoGPT models, from\nbilingual translation using mBART and Transformer models, and from multilingual\ntranslation using M2M100 models of varying sizes.",
      "authors": [
        {
          "name": "Viktoria Schram",
          "affiliation": null
        },
        {
          "name": "Markus Hiller",
          "affiliation": null
        },
        {
          "name": "Daniel Beck",
          "affiliation": null
        },
        {
          "name": "Trevor Cohn",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16743v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16743v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16742v1",
      "title": "Surrogate Modeling and Explainable Artificial Intelligence for Complex Systems: A Workflow for Automated Simulation Exploration",
      "abstract": "Complex systems are increasingly explored through simulation-driven\nengineering workflows that combine physics-based and empirical models with\noptimization and analytics. Despite their power, these workflows face two\ncentral obstacles: (1) high computational cost, since accurate exploration\nrequires many expensive simulator runs; and (2) limited transparency and\nreliability when decisions rely on opaque blackbox components. We propose a\nworkflow that addresses both challenges by training lightweight emulators on\ncompact designs of experiments that (i) provide fast, low-latency\napproximations of expensive simulators, (ii) enable rigorous uncertainty\nquantification, and (iii) are adapted for global and local Explainable\nArtificial Intelligence (XAI) analyses. This workflow unifies every\nsimulation-based complex-system analysis tool, ranging from engineering design\nto agent-based models for socio-environmental understanding. In this paper, we\nproposea comparative methodology and practical recommendations for using\nsurrogate-based explainability tools within the proposed workflow. The\nmethodology supports continuous and categorical inputs, combines global-effect\nand uncertainty analyses with local attribution, and evaluates the consistency\nof explanations across surrogate models, thereby diagnosing surrogate adequacy\nand guiding further data collection or model refinement. We demonstrate the\napproach on two contrasting case studies: a multidisciplinary design analysis\nof a hybrid-electric aircraft and an agent-based model of urban segregation.\nResults show that the surrogate model and XAI coupling enables large-scale\nexploration in seconds, uncovers nonlinear interactions and emergent behaviors,\nidentifies key design and policy levers, and signals regions where surrogates\nrequire more data or alternative architectures.",
      "authors": [
        {
          "name": "Paul Saves",
          "affiliation": null
        },
        {
          "name": "Pramudita Satria Palar",
          "affiliation": null
        },
        {
          "name": "Muhammad Daffa Robani",
          "affiliation": null
        },
        {
          "name": "Nicolas Verstaevel",
          "affiliation": null
        },
        {
          "name": "Moncef Garouani",
          "affiliation": null
        },
        {
          "name": "Julien Aligon",
          "affiliation": null
        },
        {
          "name": "Benoit Gaudou",
          "affiliation": null
        },
        {
          "name": "Koji Shimoyama",
          "affiliation": null
        },
        {
          "name": "Joseph Morlier",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.AI",
        "cs.MA",
        "stat.ME"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16742v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16742v1",
      "primary_category": "cs.AI",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16735v1",
      "title": "A Control-Theoretic Approach to Dynamic Payment Routing for Success Rate Optimization",
      "abstract": "This paper introduces a control-theoretic framework for dynamic payment\nrouting, implemented within JUSPAY's Payment Orchestrator to maximize\ntransaction success rate. The routing system is modeled as a closed-loop\nfeedback controller continuously sensing gateway performance, computing\ncorrective actions, and dynamically routes transactions across gateway to\nensure operational resilience. The system leverages concepts from control\ntheory, reinforcement learning, and multi-armed bandit optimization to achieve\nboth short-term responsiveness and long-term stability. Rather than relying on\nexplicit PID regulation, the framework applies generalized feedback-based\nadaptation, ensuring that corrective actions remain proportional to observed\nperformance deviations and the computed gateway score gradually converges\ntoward the success rate. This hybrid approach unifies control theory and\nadaptive decision systems, enabling self-regulating transaction routing that\ndampens instability, and improves reliability. Live production results show an\nimprovement of up to 1.15% in success rate over traditional rule-based routing,\ndemonstrating the effectiveness of feedback-based control in payment systems.",
      "authors": [
        {
          "name": "Aniket Agrawal",
          "affiliation": null
        },
        {
          "name": "Harsharanga Patil",
          "affiliation": null
        }
      ],
      "categories": [
        "eess.SY",
        "cs.LG",
        "cs.SY",
        "93C40 (Primary) 68T05, 91B82 (Secondary)",
        "I.2.6; I.2.8; C.2.4; K.4.4"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16735v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16735v1",
      "primary_category": "eess.SY",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16732v1",
      "title": "A Comprehensive Survey on World Models for Embodied AI",
      "abstract": "Embodied AI requires agents that perceive, act, and anticipate how actions\nreshape future world states. World models serve as internal simulators that\ncapture environment dynamics, enabling forward and counterfactual rollouts to\nsupport perception, prediction, and decision making. This survey presents a\nunified framework for world models in embodied AI. Specifically, we formalize\nthe problem setting and learning objectives, and propose a three-axis taxonomy\nencompassing: (1) Functionality, Decision-Coupled vs. General-Purpose; (2)\nTemporal Modeling, Sequential Simulation and Inference vs. Global Difference\nPrediction; (3) Spatial Representation, Global Latent Vector, Token Feature\nSequence, Spatial Latent Grid, and Decomposed Rendering Representation. We\nsystematize data resources and metrics across robotics, autonomous driving, and\ngeneral video settings, covering pixel prediction quality, state-level\nunderstanding, and task performance. Furthermore, we offer a quantitative\ncomparison of state-of-the-art models and distill key open challenges,\nincluding the scarcity of unified datasets and the need for evaluation metrics\nthat assess physical consistency over pixel fidelity, the trade-off between\nmodel performance and the computational efficiency required for real-time\ncontrol, and the core modeling difficulty of achieving long-horizon temporal\nconsistency while mitigating error accumulation. Finally, we maintain a curated\nbibliography at https://github.com/Li-Zn-H/AwesomeWorldModels.",
      "authors": [
        {
          "name": "Xinqing Li",
          "affiliation": null
        },
        {
          "name": "Xin He",
          "affiliation": null
        },
        {
          "name": "Le Zhang",
          "affiliation": null
        },
        {
          "name": "Yun Liu",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16732v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16732v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16730v1",
      "title": "UKANFormer: Noise-Robust Semantic Segmentation for Coral Reef Mapping via a Kolmogorov-Arnold Network-Transformer Hybrid",
      "abstract": "Coral reefs are vital yet fragile ecosystems that require accurate\nlarge-scale mapping for effective conservation. Although global products such\nas the Allen Coral Atlas provide unprecedented coverage of global coral reef\ndistri-bution, their predictions are frequently limited in spatial precision\nand semantic consistency, especially in regions requiring fine-grained boundary\ndelineation. To address these challenges, we propose UKANFormer, a novel\nse-mantic segmentation model designed to achieve high-precision mapping under\nnoisy supervision derived from Allen Coral Atlas. Building upon the UKAN\narchitecture, UKANFormer incorporates a Global-Local Transformer (GL-Trans)\nblock in the decoder, enabling the extraction of both global semantic\nstructures and local boundary details. In experiments, UKANFormer achieved a\ncoral-class IoU of 67.00% and pixel accuracy of 83.98%, outperforming\nconventional baselines under the same noisy labels setting. Remarkably, the\nmodel produces predictions that are visually and structurally more accurate\nthan the noisy labels used for training. These results challenge the notion\nthat data quality directly limits model performance, showing that architectural\ndesign can mitigate label noise and sup-port scalable mapping under imperfect\nsupervision. UKANFormer provides a foundation for ecological monitoring where\nreliable labels are scarce.",
      "authors": [
        {
          "name": "Tianyang Dou",
          "affiliation": null
        },
        {
          "name": "Ming Li",
          "affiliation": null
        },
        {
          "name": "Jiangying Qin",
          "affiliation": null
        },
        {
          "name": "Xuan Liao",
          "affiliation": null
        },
        {
          "name": "Jiageng Zhong",
          "affiliation": null
        },
        {
          "name": "Armin Gruen",
          "affiliation": null
        },
        {
          "name": "Mengyi Deng",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16730v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16730v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16729v1",
      "title": "Vision-Centric 4D Occupancy Forecasting and Planning via Implicit Residual World Models",
      "abstract": "End-to-end autonomous driving systems increasingly rely on vision-centric\nworld models to understand and predict their environment. However, a common\nineffectiveness in these models is the full reconstruction of future scenes,\nwhich expends significant capacity on redundantly modeling static backgrounds.\nTo address this, we propose IR-WM, an Implicit Residual World Model that\nfocuses on modeling the current state and evolution of the world. IR-WM first\nestablishes a robust bird's-eye-view representation of the current state from\nthe visual observation. It then leverages the BEV features from the previous\ntimestep as a strong temporal prior and predicts only the \"residual\", i.e., the\nchanges conditioned on the ego-vehicle's actions and scene context. To\nalleviate error accumulation over time, we further apply an alignment module to\ncalibrate semantic and dynamic misalignments. Moreover, we investigate\ndifferent forecasting-planning coupling schemes and demonstrate that the\nimplicit future state generated by world models substantially improves planning\naccuracy. On the nuScenes benchmark, IR-WM achieves top performance in both 4D\noccupancy forecasting and trajectory planning.",
      "authors": [
        {
          "name": "Jianbiao Mei",
          "affiliation": null
        },
        {
          "name": "Yu Yang",
          "affiliation": null
        },
        {
          "name": "Xuemeng Yang",
          "affiliation": null
        },
        {
          "name": "Licheng Wen",
          "affiliation": null
        },
        {
          "name": "Jiajun Lv",
          "affiliation": null
        },
        {
          "name": "Botian Shi",
          "affiliation": null
        },
        {
          "name": "Yong Liu",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16729v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16729v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16728v1",
      "title": "Local regression on path spaces with signature metrics",
      "abstract": "We study nonparametric regression and classification for path-valued data. We\nintroduce a functional Nadaraya-Watson estimator that combines the signature\ntransform from rough path theory with local kernel regression. The signature\ntransform provides a principled way to encode sequential data through iterated\nintegrals, enabling direct comparison of paths in a natural metric space. Our\napproach leverages signature-induced distances within the classical kernel\nregression framework, achieving computational efficiency while avoiding the\nscalability bottlenecks of large-scale kernel matrix operations. We establish\nfinite-sample convergence bounds demonstrating favorable statistical properties\nof signature-based distances compared to traditional metrics in\ninfinite-dimensional settings. We propose robust signature variants that\nprovide stability against outliers, enhancing practical performance.\nApplications to both synthetic and real-world data - including stochastic\ndifferential equation learning and time series classification - demonstrate\ncompetitive accuracy while offering significant computational advantages over\nexisting methods.",
      "authors": [
        {
          "name": "Christian Bayer",
          "affiliation": null
        },
        {
          "name": "Davit Gogolashvili",
          "affiliation": null
        },
        {
          "name": "Luca Pelizzari",
          "affiliation": null
        }
      ],
      "categories": [
        "stat.ML",
        "cs.LG",
        "math.PR",
        "stat.ME",
        "60L10, 60L20, 62G05, 62G08"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16728v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16728v1",
      "primary_category": "stat.ML",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16727v1",
      "title": "Beacon: Single-Turn Diagnosis and Mitigation of Latent Sycophancy in Large Language Models",
      "abstract": "Large language models internalize a structural trade-off between truthfulness\nand obsequious flattery, emerging from reward optimization that conflates\nhelpfulness with polite submission. This latent bias, known as sycophancy,\nmanifests as a preference for user agreement over principled reasoning. We\nintroduce Beacon, a single-turn forced-choice benchmark that isolates this bias\nindependent of conversational context, enabling precise measurement of the\ntension between factual accuracy and submissive bias. Evaluations across twelve\nstate-of-the-art models reveal that sycophancy decomposes into stable\nlinguistic and affective sub-biases, each scaling with model capacity. We\nfurther propose prompt-level and activation-level interventions that modulate\nthese biases in opposing directions, exposing the internal geometry of\nalignment as a dynamic manifold between truthfulness and socially compliant\njudgment. Beacon reframes sycophancy as a measurable form of normative\nmisgeneralization, providing a reproducible foundation for studying and\nmitigating alignment drift in large-scale generative systems.",
      "authors": [
        {
          "name": "Sanskar Pandey",
          "affiliation": null
        },
        {
          "name": "Ruhaan Chopra",
          "affiliation": null
        },
        {
          "name": "Angkul Puniya",
          "affiliation": null
        },
        {
          "name": "Sohom Pal",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16727v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16727v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16724v1",
      "title": "A Comprehensive Survey on Reinforcement Learning-based Agentic Search: Foundations, Roles, Optimizations, Evaluations, and Applications",
      "abstract": "The advent of large language models (LLMs) has transformed information access\nand reasoning through open-ended natural language interaction. However, LLMs\nremain limited by static knowledge, factual hallucinations, and the inability\nto retrieve real-time or domain-specific information. Retrieval-Augmented\nGeneration (RAG) mitigates these issues by grounding model outputs in external\nevidence, but traditional RAG pipelines are often single turn and heuristic,\nlacking adaptive control over retrieval and reasoning. Recent advances in\nagentic search address these limitations by enabling LLMs to plan, retrieve,\nand reflect through multi-step interaction with search environments. Within\nthis paradigm, reinforcement learning (RL) offers a powerful mechanism for\nadaptive and self-improving search behavior. This survey provides the first\ncomprehensive overview of \\emph{RL-based agentic search}, organizing the\nemerging field along three complementary dimensions: (i) What RL is for\n(functional roles), (ii) How RL is used (optimization strategies), and (iii)\nWhere RL is applied (scope of optimization). We summarize representative\nmethods, evaluation protocols, and applications, and discuss open challenges\nand future directions toward building reliable and scalable RL driven agentic\nsearch systems. We hope this survey will inspire future research on the\nintegration of RL and agentic search. Our repository is available at\nhttps://github.com/ventr1c/Awesome-RL-based-Agentic-Search-Papers.",
      "authors": [
        {
          "name": "Minhua Lin",
          "affiliation": null
        },
        {
          "name": "Zongyu Wu",
          "affiliation": null
        },
        {
          "name": "Zhichao Xu",
          "affiliation": null
        },
        {
          "name": "Hui Liu",
          "affiliation": null
        },
        {
          "name": "Xianfeng Tang",
          "affiliation": null
        },
        {
          "name": "Qi He",
          "affiliation": null
        },
        {
          "name": "Charu Aggarwal",
          "affiliation": null
        },
        {
          "name": "Hui Liu",
          "affiliation": null
        },
        {
          "name": "Xiang Zhang",
          "affiliation": null
        },
        {
          "name": "Suhang Wang",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16724v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16724v1",
      "primary_category": "cs.AI",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16720v1",
      "title": "Beyond Pipelines: A Survey of the Paradigm Shift toward Model-Native Agentic AI",
      "abstract": "The rapid evolution of agentic AI marks a new phase in artificial\nintelligence, where Large Language Models (LLMs) no longer merely respond but\nact, reason, and adapt. This survey traces the paradigm shift in building\nagentic AI: from Pipeline-based systems, where planning, tool use, and memory\nare orchestrated by external logic, to the emerging Model-native paradigm,\nwhere these capabilities are internalized within the model's parameters. We\nfirst position Reinforcement Learning (RL) as the algorithmic engine enabling\nthis paradigm shift. By reframing learning from imitating static data to\noutcome-driven exploration, RL underpins a unified solution of LLM + RL + Task\nacross language, vision and embodied domains. Building on this, the survey\nsystematically reviews how each capability -- Planning, Tool use, and Memory --\nhas evolved from externally scripted modules to end-to-end learned behaviors.\nFurthermore, it examines how this paradigm shift has reshaped major agent\napplications, specifically the Deep Research agent emphasizing long-horizon\nreasoning and the GUI agent emphasizing embodied interaction. We conclude by\ndiscussing the continued internalization of agentic capabilities like\nMulti-agent collaboration and Reflection, alongside the evolving roles of the\nsystem and model layers in future agentic AI. Together, these developments\noutline a coherent trajectory toward model-native agentic AI as an integrated\nlearning and interaction framework, marking the transition from constructing\nsystems that apply intelligence to developing models that grow intelligence\nthrough experience.",
      "authors": [
        {
          "name": "Jitao Sang",
          "affiliation": null
        },
        {
          "name": "Jinlin Xiao",
          "affiliation": null
        },
        {
          "name": "Jiarun Han",
          "affiliation": null
        },
        {
          "name": "Jilin Chen",
          "affiliation": null
        },
        {
          "name": "Xiaoyi Chen",
          "affiliation": null
        },
        {
          "name": "Shuyu Wei",
          "affiliation": null
        },
        {
          "name": "Yongjie Sun",
          "affiliation": null
        },
        {
          "name": "Yuhang Wang",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.AI"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16720v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16720v1",
      "primary_category": "cs.AI",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16719v1",
      "title": "LSTM-Based Forecasting and Analysis of EV Charging Demand in a Dense Urban Campus",
      "abstract": "This paper presents a framework for processing EV charging load data in order\nto forecast future load predictions using a Recurrent Neural Network,\nspecifically an LSTM. The framework processes a large set of raw data from\nmultiple locations and transforms it with normalization and feature extraction\nto train the LSTM. The pre-processing stage corrects for missing or incomplete\nvalues by interpolating and normalizing the measurements. This information is\nthen fed into a Long Short-Term Memory Model designed to capture the short-term\nfluctuations while also interpreting the long-term trends in the charging data.\nExperimental results demonstrate the model's ability to accurately predict\ncharging demand across multiple time scales (daily, weekly, and monthly),\nproviding valuable insights for infrastructure planning, energy management, and\ngrid integration of EV charging facilities. The system's modular design allows\nfor adaptation to different charging locations with varying usage patterns,\nmaking it applicable across diverse deployment scenarios.",
      "authors": [
        {
          "name": "Zak Ressler",
          "affiliation": null
        },
        {
          "name": "Marcus Grijalva",
          "affiliation": null
        },
        {
          "name": "Angelica Marie Ignacio",
          "affiliation": null
        },
        {
          "name": "Melanie Torres",
          "affiliation": null
        },
        {
          "name": "Abelardo Cuadra Rojas",
          "affiliation": null
        },
        {
          "name": "Rohollah Moghadam",
          "affiliation": null
        },
        {
          "name": "Mohammad Rasoul narimani",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "math.OC"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16719v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16719v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16718v1",
      "title": "U-Codec: Ultra Low Frame-rate Neural Speech Codec for Fast High-fidelity Speech Generation",
      "abstract": "We propose \\textbf{U-Codec}, an \\textbf{U}ltra low frame-rate neural speech\n\\textbf{Codec} that achieves high-fidelity reconstruction and fast speech\ngeneration at an extremely low frame-rate of 5Hz (5 frames per second). Extreme\ncompression at 5Hz typically leads to severe intelligibility and spectral\ndetail loss, we introduce a Transformer-based inter-frame long-term dependency\nmodule and systematically explore residual vector quantization (RVQ) depth and\ncodebook size to identify optimal configurations. Moreover, we apply U-Codec\ninto a large language model (LLM)-based auto-regressive TTS model, which\nleverages global and local hierarchical architecture to effectively capture\ndependencies across multi-layer tokens. We extend LLM-based TTS from 3-layer\nRVQ at 50Hz to 32-layer RVQ at 5Hz. Experimental results demonstrate that\nU-Codec improves LLM-based TTS inference speed by around 3 $\\times$ over\nhigh-frame-rate codecs while maintaining similarity and naturalness. These\nresults validate the feasibility of using highly compressed 5Hz discrete tokens\nfor fast and high-fidelity speech synthesis.",
      "authors": [
        {
          "name": "Xusheng Yang",
          "affiliation": null
        },
        {
          "name": "Long Zhou",
          "affiliation": null
        },
        {
          "name": "Wenfu Wang",
          "affiliation": null
        },
        {
          "name": "Kai Hu",
          "affiliation": null
        },
        {
          "name": "Shulin Feng",
          "affiliation": null
        },
        {
          "name": "Chenxing Li",
          "affiliation": null
        },
        {
          "name": "Meng Yu",
          "affiliation": null
        },
        {
          "name": "Dong Yu",
          "affiliation": null
        },
        {
          "name": "Yuexian Zou",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.SD",
        "cs.CL",
        "cs.LG"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16718v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16718v1",
      "primary_category": "cs.SD",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16716v1",
      "title": "DistilLock: Safeguarding LLMs from Unauthorized Knowledge Distillation on the Edge",
      "abstract": "Large Language Models (LLMs) have demonstrated strong performance across\ndiverse tasks, but fine-tuning them typically relies on cloud-based,\ncentralized infrastructures. This requires data owners to upload potentially\nsensitive data to external servers, raising serious privacy concerns. An\nalternative approach is to fine-tune LLMs directly on edge devices using local\ndata; however, this introduces a new challenge: the model owner must transfer\nproprietary models to the edge, which risks intellectual property (IP) leakage.\nTo address this dilemma, we propose DistilLock, a TEE-assisted fine-tuning\nframework that enables privacy-preserving knowledge distillation on the edge.\nIn DistilLock, a proprietary foundation model is executed within a trusted\nexecution environment (TEE) enclave on the data owner's device, acting as a\nsecure black-box teacher. This setup preserves both data privacy and model IP\nby preventing direct access to model internals. Furthermore, DistilLock employs\na model obfuscation mechanism to offload obfuscated weights to untrusted\naccelerators for efficient knowledge distillation without compromising\nsecurity. We demonstrate that DistilLock prevents unauthorized knowledge\ndistillation processes and model-stealing attacks while maintaining high\ncomputational efficiency, but offering a secure and practical solution for\nedge-based LLM personalization.",
      "authors": [
        {
          "name": "Asmita Mohanty",
          "affiliation": null
        },
        {
          "name": "Gezheng Kang",
          "affiliation": null
        },
        {
          "name": "Lei Gao",
          "affiliation": null
        },
        {
          "name": "Murali Annavaram",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CR",
        "cs.LG"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16716v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16716v1",
      "primary_category": "cs.CR",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16714v1",
      "title": "Eliciting Grounded Chain-of-Thought Reasoning in 3D Scenes",
      "abstract": "Existing research on 3D Large Language Models (LLMs) still struggles to\nachieve grounded question-answering, primarily due to the under-exploration of\nthe mech- anism of human-like scene-object grounded reasoning. This paper\nbridges the gap by presenting a novel framework. We first introduce a grounded\nChain-of- Thought reasoning method in 3D scenes (SCENECOT), decoupling a\ncomplex reasoning task into simpler and manageable problems, and building\ncorresponding visual clues based on multimodal expert modules. To enable such a\nmethod, we develop SCENECOT-185K, the first large-scale grounded CoT reasoning\ndataset, consisting of 185K high-quality instances. Extensive experiments\nacross various complex 3D scene reasoning benchmarks demonstrate that our new\nframework achieves strong performance with high grounding-QA coherence. To the\nbest of our knowledge, this is the first successful application of CoT\nreasoning to 3D scene understanding, enabling step-by-step human-like reasoning\nand showing potential for extension to broader 3D scene understanding\nscenarios.",
      "authors": [
        {
          "name": "Xiongkun Linghu",
          "affiliation": null
        },
        {
          "name": "Jiangyong Huang",
          "affiliation": null
        },
        {
          "name": "Ziyu Zhu",
          "affiliation": null
        },
        {
          "name": "Baoxiong Jia",
          "affiliation": null
        },
        {
          "name": "Siyuan Huang",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16714v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16714v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16713v1",
      "title": "so much depends / upon / a whitespace: Why Whitespace Matters for Poets and LLMs",
      "abstract": "Whitespace is a critical component of poetic form, reflecting both adherence\nto standardized forms and rebellion against those forms. Each poem's whitespace\ndistribution reflects the artistic choices of the poet and is an integral\nsemantic and spatial feature of the poem. Yet, despite the popularity of poetry\nas both a long-standing art form and as a generation task for large language\nmodels (LLMs), whitespace has not received sufficient attention from the NLP\ncommunity. Using a corpus of 19k English-language published poems from Poetry\nFoundation, we investigate how 4k poets have used whitespace in their works. We\nrelease a subset of 2.8k public-domain poems with preserved formatting to\nfacilitate further research in this area. We compare whitespace usage in the\npublished poems to (1) 51k LLM-generated poems, and (2) 12k unpublished poems\nposted in an online community. We also explore whitespace usage across time\nperiods, poetic forms, and data sources. Additionally, we find that different\ntext processing methods can result in significantly different representations\nof whitespace in poetry data, motivating us to use these poems and whitespace\npatterns to discuss implications for the processing strategies used to assemble\npretraining datasets for LLMs.",
      "authors": [
        {
          "name": "Sriharsh Bhyravajjula",
          "affiliation": null
        },
        {
          "name": "Melanie Walsh",
          "affiliation": null
        },
        {
          "name": "Anna Preus",
          "affiliation": null
        },
        {
          "name": "Maria Antoniak",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16713v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16713v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16712v1",
      "title": "The Chameleon Nature of LLMs: Quantifying Multi-Turn Stance Instability in Search-Enabled Language Models",
      "abstract": "Integration of Large Language Models with search/retrieval engines has become\nubiquitous, yet these systems harbor a critical vulnerability that undermines\ntheir reliability. We present the first systematic investigation of \"chameleon\nbehavior\" in LLMs: their alarming tendency to shift stances when presented with\ncontradictory questions in multi-turn conversations (especially in\nsearch-enabled LLMs). Through our novel Chameleon Benchmark Dataset, comprising\n17,770 carefully crafted question-answer pairs across 1,180 multi-turn\nconversations spanning 12 controversial domains, we expose fundamental flaws in\nstate-of-the-art systems. We introduce two theoretically grounded metrics: the\nChameleon Score (0-1) that quantifies stance instability, and Source Re-use\nRate (0-1) that measures knowledge diversity. Our rigorous evaluation of\nLlama-4-Maverick, GPT-4o-mini, and Gemini-2.5-Flash reveals consistent\nfailures: all models exhibit severe chameleon behavior (scores 0.391-0.511),\nwith GPT-4o-mini showing the worst performance. Crucially, small\nacross-temperature variance (less than 0.004) suggests the effect is not a\nsampling artifact. Our analysis uncovers the mechanism: strong correlations\nbetween source re-use rate and confidence (r=0.627) and stance changes\n(r=0.429) are statistically significant (p less than 0.05), indicating that\nlimited knowledge diversity makes models pathologically deferential to query\nframing. These findings highlight the need for comprehensive consistency\nevaluation before deploying LLMs in healthcare, legal, and financial systems\nwhere maintaining coherent positions across interactions is critical for\nreliable decision support.",
      "authors": [
        {
          "name": "Shivam Ratnakar",
          "affiliation": null
        },
        {
          "name": "Sanjay Raghavendra",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16712v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16712v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16709v1",
      "title": "HumanCM: One Step Human Motion Prediction",
      "abstract": "We present HumanCM, a one-step human motion prediction framework built upon\nconsistency models. Instead of relying on multi-step denoising as in\ndiffusion-based methods, HumanCM performs efficient single-step generation by\nlearning a self-consistent mapping between noisy and clean motion states. The\nframework adopts a Transformer-based spatiotemporal architecture with temporal\nembeddings to model long-range dependencies and preserve motion coherence.\nExperiments on Human3.6M and HumanEva-I demonstrate that HumanCM achieves\ncomparable or superior accuracy to state-of-the-art diffusion models while\nreducing inference steps by up to two orders of magnitude.",
      "authors": [
        {
          "name": "Liu Haojie",
          "affiliation": null
        },
        {
          "name": "Gao Suixiang",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16709v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16709v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16708v1",
      "title": "Natural Language Processing Applications in Cardiology: A Narrative Review",
      "abstract": "Cardiovascular disease has become increasingly prevalent in modern society\nand has a significant effect on global health and well-being. Heart-related\nconditions are intricate, multifaceted disorders, which may be influenced by a\ncombination of genetic predispositions, lifestyle choices, and various\nsocioeconomic and clinical factors. Information regarding these potentially\ncomplex interrelationships is dispersed among diverse types of textual data,\nwhich include patient narratives, medical records, and scientific literature,\namong others. Natural language processing (NLP) techniques have increasingly\nbeen adopted as a powerful means to analyse and make sense of this vast amount\nof unstructured data. This, in turn, can allow healthcare professionals to gain\ndeeper insights into the cardiology field, which has the potential to\nrevolutionize current approaches to the diagnosis, treatment, and prevention of\ncardiac problems. This review provides a detailed overview of NLP research in\ncardiology between 2014 and 2025. We queried six literature databases to find\narticles describing the application of NLP techniques in the context of a range\nof different cardiovascular diseases. Following a rigorous screening process,\nwe identified a total of 265 relevant articles. We analysed each article from\nmultiple dimensions, i.e., NLP paradigm types, cardiology-related task types,\ncardiovascular disease types, and data source types. Our analysis reveals\nconsiderable diversity within each of these dimensions, thus demonstrating the\nconsiderable breadth of NLP research within the field. We also perform a\ntemporal analysis, which illustrates the evolution and changing trends in NLP\nmethods employed over the last decade that we cover. To our knowledge, the\nreview constitutes the most comprehensive overview of NLP research in\ncardiology to date.",
      "authors": [
        {
          "name": "Kailai Yang",
          "affiliation": null
        },
        {
          "name": "Yan Leng",
          "affiliation": null
        },
        {
          "name": "Xin Zhang",
          "affiliation": null
        },
        {
          "name": "Tianlin Zhang",
          "affiliation": null
        },
        {
          "name": "Paul Thompson",
          "affiliation": null
        },
        {
          "name": "Bernard Keavney",
          "affiliation": null
        },
        {
          "name": "Maciej Tomaszewski",
          "affiliation": null
        },
        {
          "name": "Sophia Ananiadou",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16708v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16708v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16704v1",
      "title": "Connecting Domains and Contrasting Samples: A Ladder for Domain Generalization",
      "abstract": "Distribution shifts between training and testing samples frequently occur in\npractice and impede model generalization performance. This crucial challenge\nthereby motivates studies on domain generalization (DG), which aim to predict\nthe label on unseen target domain data by solely using data from source\ndomains. It is intuitive to conceive the class-separated representations\nlearned in contrastive learning (CL) are able to improve DG, while the reality\nis quite the opposite: users observe directly applying CL deteriorates the\nperformance. We analyze the phenomenon with the insights from CL theory and\ndiscover lack of intra-class connectivity in the DG setting causes the\ndeficiency. We thus propose a new paradigm, domain-connecting contrastive\nlearning (DCCL), to enhance the conceptual connectivity across domains and\nobtain generalizable representations for DG. On the data side, more aggressive\ndata augmentation and cross-domain positive samples are introduced to improve\nintra-class connectivity. On the model side, to better embed the unseen test\ndomains, we propose model anchoring to exploit the intra-class connectivity in\npre-trained representations and complement the anchoring with generative\ntransformation loss. Extensive experiments on five standard DG benchmarks are\nperformed. The results verify that DCCL outperforms state-of-the-art baselines\neven without domain supervision. The detailed model implementation and the code\nare provided through https://github.com/weitianxin/DCCL",
      "authors": [
        {
          "name": "Tianxin Wei",
          "affiliation": null
        },
        {
          "name": "Yifan Chen",
          "affiliation": null
        },
        {
          "name": "Xinrui He",
          "affiliation": null
        },
        {
          "name": "Wenxuan Bao",
          "affiliation": null
        },
        {
          "name": "Jingrui He",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16704v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16704v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16703v1",
      "title": "On the Granularity of Causal Effect Identifiability",
      "abstract": "The classical notion of causal effect identifiability is defined in terms of\ntreatment and outcome variables. In this note, we consider the identifiability\nof state-based causal effects: how an intervention on a particular state of\ntreatment variables affects a particular state of outcome variables. We\ndemonstrate that state-based causal effects may be identifiable even when\nvariable-based causal effects may not. Moreover, we show that this separation\noccurs only when additional knowledge -- such as context-specific\nindependencies and conditional functional dependencies -- is available. We\nfurther examine knowledge that constrains the states of variables, and show\nthat such knowledge does not improve identifiability on its own but can improve\nboth variable-based and state-based identifiability when combined with other\nknowledge such as context-specific independencies. Our findings highlight\nsituations where causal effects of interest may be estimable from observational\ndata and this identifiability may be missed by existing variable-based\nframeworks.",
      "authors": [
        {
          "name": "Yizuo Chen",
          "affiliation": null
        },
        {
          "name": "Adnan Darwiche",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ME"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16703v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16703v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16702v1",
      "title": "SDPA++: A General Framework for Self-Supervised Denoising with Patch Aggregation",
      "abstract": "Optical Coherence Tomography (OCT) is a widely used non-invasive imaging\ntechnique that provides detailed three-dimensional views of the retina, which\nare essential for the early and accurate diagnosis of ocular diseases.\nConsequently, OCT image analysis and processing have emerged as key research\nareas in biomedical imaging. However, acquiring paired datasets of clean and\nreal-world noisy OCT images for supervised denoising models remains a\nformidable challenge due to intrinsic speckle noise and practical constraints\nin clinical imaging environments. To address these issues, we propose SDPA++: A\nGeneral Framework for Self-Supervised Denoising with Patch Aggregation. Our\nnovel approach leverages only noisy OCT images by first generating\npseudo-ground-truth images through self-fusion and self-supervised denoising.\nThese refined images then serve as targets to train an ensemble of denoising\nmodels using a patch-based strategy that effectively enhances image clarity.\nPerformance improvements are validated via metrics such as Contrast-to-Noise\nRatio (CNR), Mean Square Ratio (MSR), Texture Preservation (TP), and Edge\nPreservation (EP) on the real-world dataset from the IEEE SPS Video and Image\nProcessing Cup. Notably, the VIP Cup dataset contains only real-world noisy OCT\nimages without clean references, highlighting our method's potential for\nimproving image quality and diagnostic outcomes in clinical practice.",
      "authors": [
        {
          "name": "Huy Minh Nhat Nguyen",
          "affiliation": null
        },
        {
          "name": "Triet Hoang Minh Dao",
          "affiliation": null
        },
        {
          "name": "Chau Vinh Hoang Truong",
          "affiliation": null
        },
        {
          "name": "Cuong Tuan Nguyen",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16702v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16702v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16701v1",
      "title": "An Agentic Framework with LLMs for Solving Complex Vehicle Routing Problems",
      "abstract": "Complex vehicle routing problems (VRPs) remain a fundamental challenge,\ndemanding substantial expert effort for intent interpretation and algorithm\ndesign. While large language models (LLMs) offer a promising path toward\nautomation, current approaches still rely on external intervention, which\nrestrict autonomy and often lead to execution errors and low solution\nfeasibility. To address these challenges, we propose an Agentic Framework with\nLLMs (AFL) for solving complex vehicle routing problems, achieving full\nautomation from problem instance to solution. AFL directly extracts knowledge\nfrom raw inputs and enables self-contained code generation without handcrafted\nmodules or external solvers. To improve trustworthiness, AFL decomposes the\noverall pipeline into three manageable subtasks and employs four specialized\nagents whose coordinated interactions enforce cross-functional consistency and\nlogical soundness. Extensive experiments on 60 complex VRPs, ranging from\nstandard benchmarks to practical variants, validate the effectiveness and\ngenerality of our framework, showing comparable performance against\nmeticulously designed algorithms. Notably, it substantially outperforms\nexisting LLM-based baselines in both code reliability and solution feasibility,\nachieving rates close to 100% on the evaluated benchmarks.",
      "authors": [
        {
          "name": "Ni Zhang",
          "affiliation": null
        },
        {
          "name": "Zhiguang Cao",
          "affiliation": null
        },
        {
          "name": "Jianan Zhou",
          "affiliation": null
        },
        {
          "name": "Cong Zhang",
          "affiliation": null
        },
        {
          "name": "Yew-Soon Ong",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.AI"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16701v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16701v1",
      "primary_category": "cs.AI",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16695v1",
      "title": "Resolution-Aware Retrieval Augmented Zero-Shot Forecasting",
      "abstract": "Zero-shot forecasting aims to predict outcomes for previously unseen\nconditions without direct historical data, posing a significant challenge for\ntraditional forecasting methods. We introduce a Resolution-Aware\nRetrieval-Augmented Forecasting model that enhances predictive accuracy by\nleveraging spatial correlations and temporal frequency characteristics. By\ndecomposing signals into different frequency components, our model employs\nresolution-aware retrieval, where lower-frequency components rely on broader\nspatial context, while higher-frequency components focus on local influences.\nThis allows the model to dynamically retrieve relevant data and adapt to new\nlocations with minimal historical context.\n  Applied to microclimate forecasting, our model significantly outperforms\ntraditional forecasting methods, numerical weather prediction models, and\nmodern foundation time series models, achieving 71% lower MSE than HRRR and 34%\nlower MSE than Chronos on the ERA5 dataset.\n  Our results highlight the effectiveness of retrieval-augmented and\nresolution-aware strategies, offering a scalable and data-efficient solution\nfor zero-shot forecasting in microclimate modeling and beyond.",
      "authors": [
        {
          "name": "Iman Deznabi",
          "affiliation": null
        },
        {
          "name": "Peeyush Kumar",
          "affiliation": null
        },
        {
          "name": "Madalina Fiterau",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cs.IR"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16695v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16695v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16694v1",
      "title": "CLIP: Client-Side Invariant Pruning for Mitigating Stragglers in Secure Federated Learning",
      "abstract": "Secure federated learning (FL) preserves data privacy during distributed\nmodel training. However, deploying such frameworks across heterogeneous devices\nresults in performance bottlenecks, due to straggler clients with limited\ncomputational or network capabilities, slowing training for all participating\nclients. This paper introduces the first straggler mitigation technique for\nsecure aggregation with deep neural networks. We propose CLIP, a client-side\ninvariant neuron pruning technique coupled with network-aware pruning, that\naddresses compute and network bottlenecks due to stragglers during training\nwith minimal accuracy loss. Our technique accelerates secure FL training by 13%\nto 34% across multiple datasets (CIFAR10, Shakespeare, FEMNIST) with an\naccuracy impact of between 1.3% improvement to 2.6% reduction.",
      "authors": [
        {
          "name": "Anthony DiMaggio",
          "affiliation": null
        },
        {
          "name": "Raghav Sharma",
          "affiliation": null
        },
        {
          "name": "Gururaj Saileshwar",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cs.CR",
        "cs.DC"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16694v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16694v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16688v1",
      "title": "Pursuing Minimal Sufficiency in Spatial Reasoning",
      "abstract": "Spatial reasoning, the ability to ground language in 3D understanding,\nremains a persistent challenge for Vision-Language Models (VLMs). We identify\ntwo fundamental bottlenecks: inadequate 3D understanding capabilities stemming\nfrom 2D-centric pre-training, and reasoning failures induced by redundant 3D\ninformation. To address these, we first construct a Minimal Sufficient Set\n(MSS) of information before answering a given question: a compact selection of\n3D perception results from \\textit{expert models}. We introduce MSSR (Minimal\nSufficient Spatial Reasoner), a dual-agent framework that implements this\nprinciple. A Perception Agent programmatically queries 3D scenes using a\nversatile perception toolbox to extract sufficient information, including a\nnovel SOG (Situated Orientation Grounding) module that robustly extracts\nlanguage-grounded directions. A Reasoning Agent then iteratively refines this\ninformation to pursue minimality, pruning redundant details and requesting\nmissing ones in a closed loop until the MSS is curated. Extensive experiments\ndemonstrate that our method, by explicitly pursuing both sufficiency and\nminimality, significantly improves accuracy and achieves state-of-the-art\nperformance across two challenging benchmarks. Furthermore, our framework\nproduces interpretable reasoning paths, offering a promising source of\nhigh-quality training data for future models. Source code is available at\nhttps://github.com/gyj155/mssr.",
      "authors": [
        {
          "name": "Yejie Guo",
          "affiliation": null
        },
        {
          "name": "Yunzhong Hou",
          "affiliation": null
        },
        {
          "name": "Wufei Ma",
          "affiliation": null
        },
        {
          "name": "Meng Tang",
          "affiliation": null
        },
        {
          "name": "Ming-Hsuan Yang",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16688v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16688v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16687v1",
      "title": "High-Dimensional Privacy-Utility Dynamics of Noisy Stochastic Gradient Descent on Least Squares",
      "abstract": "The interplay between optimization and privacy has become a central theme in\nprivacy-preserving machine learning. Noisy stochastic gradient descent (SGD)\nhas emerged as a cornerstone algorithm, particularly in large-scale settings.\nThese variants of gradient methods inject carefully calibrated noise into each\nupdate to achieve differential privacy, the gold standard notion of rigorous\nprivacy guarantees. Prior work primarily provides various bounds on statistical\nrisk and privacy loss for noisy SGD, yet the \\textit{exact} behavior of the\nprocess remains unclear, particularly in high-dimensional settings. This work\nleverages a diffusion approach to analyze noisy SGD precisely, providing a\ncontinuous-time perspective that captures both statistical risk evolution and\nprivacy loss dynamics in high dimensions. Moreover, we study a variant of noisy\nSGD that does not require explicit knowledge of gradient sensitivity, unlike\nexisting work that assumes or enforces sensitivity through gradient clipping.\nSpecifically, we focus on the least squares problem with $\\ell_2$\nregularization.",
      "authors": [
        {
          "name": "Shurong Lin",
          "affiliation": null
        },
        {
          "name": "Eric D. Kolaczyk",
          "affiliation": null
        },
        {
          "name": "Adam Smith",
          "affiliation": null
        },
        {
          "name": "Elliot Paquette",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16687v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16687v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16686v1",
      "title": "Investigating the Impact of Rationales for LLMs on Natural Language Understanding",
      "abstract": "Chain-of-thought (CoT) rationales, which provide step-by-step reasoning to\nderive final answers, benefit LLMs in both inference and training.\nIncorporating rationales, either by generating them before answering during\ninference, or by placing them before or after the original answers during\ntraining - significantly improves model performance on mathematical, symbolic\nand commonsense reasoning tasks. However, most work focuses on the role of\nrationales in these reasoning tasks, overlooking their potential impact on\nother important tasks like natural language understanding (NLU) tasks. In this\nwork, we raise the question: Can rationales similarly benefit NLU tasks? To\nconduct a systematic exploration, we construct NLURC, a comprehensive and\nhigh-quality NLU dataset collection with rationales, and develop various\nrationale-augmented methods. Through exploring the applicability of these\nmethods on NLU tasks using the dataset, we uncover several potentially\nsurprising findings: (1) CoT inference shifts from hindering NLU performance to\nsurpassing direct label prediction as model size grows, indicating a positive\ncorrelation. (2) Most rationale-augmented training methods perform worse than\nlabel-only training, with one specially designed method consistently achieving\nimprovements. (3) LLMs trained with rationales achieve significant performance\ngains on unseen NLU tasks, rivaling models ten times their size, while\ndelivering interpretability on par with commercial LLMs.",
      "authors": [
        {
          "name": "Wenhang Shi",
          "affiliation": null
        },
        {
          "name": "Shuqing Bian",
          "affiliation": null
        },
        {
          "name": "Yiren Chen",
          "affiliation": null
        },
        {
          "name": "Xinyi Zhang",
          "affiliation": null
        },
        {
          "name": "Zhe Zhao",
          "affiliation": null
        },
        {
          "name": "Pengfei Hu",
          "affiliation": null
        },
        {
          "name": "Wei Lu",
          "affiliation": null
        },
        {
          "name": "Xiaoyong Du",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16686v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16686v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16685v1",
      "title": "Temporal Understanding under Deictic Frame of Reference",
      "abstract": "Understanding time is fundamental to human cognition, where temporal\nexperience is often conceptualized through spatial metaphors grounded in\nsensory-motor experience. For example, \"summer is approaching\" parallels \"We\nare approaching the summer\". In such expressions, humans rely on a frame of\nreference (FoR) to interpret meaning relative to a particular viewpoint.\nExtending this concept to time, a temporal frame of reference (t-FoR) defines\nhow temporal relations are perceived relative to an experiencer's moment of\n\"now\". While Large Language Models (LLMs) have shown remarkable advances in\nnatural language understanding, their ability to interpret and reason about\ntime remains limited. In this work, we introduce TUuD (Temporal Understanding\nunder Deictic t-FoR), a framework that evaluates how LLMs interpret time-event\nand event-event relations when the reference point of \"now\" dynamically shifts\nalong a timeline. Following recent work on temporal cognition\n\\cite{li2025other}, LLMs are prompted to rate the similarity between the\ncurrent moment and a target event from 0.00 (completely dissimilar) to 1.00\n(highly similar), where similarity quantifies perceived temporal alignment\nbetween the two points. Our results show that four evaluated LLMs exhibit\nmeasurable adaptation to a deictic t-FoR, with similarity ratings peaking\naround the present and decreasing toward past and future events. The\nadaptation, however, weakens beyond near-term contexts, suggesting that while\nLLMs display partial human-like temporal cognition, their temporal reasoning\nremains sensitive to reference-frame shifts and temporal distance.",
      "authors": [
        {
          "name": "Damin Zhang",
          "affiliation": null
        },
        {
          "name": "Julia Rayz",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16685v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16685v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16684v1",
      "title": "Filtering of Small Components for Isosurface Generation",
      "abstract": "Let $f: \\mathbb{R}^3 \\rightarrow \\mathbb{R}$ be a scalar field. An isosurface\nis a piecewise linear approximation of a level set $f^{-1}(\\sigma)$ for some\n$\\sigma \\in \\mathbb{R}$ built from some regular grid sampling of $f$.\nIsosurfaces constructed from scanned data such as CT scans or MRIs often\ncontain extremely small components that distract from the visualization and do\nnot form part of any geometric model produced from the data. Simple\nprefiltering of the data can remove such small components while having no\neffect on the large components that form the body of the visualization. We\npresent experimental results on such filtering.",
      "authors": [
        {
          "name": "Devin Zhao",
          "affiliation": null
        },
        {
          "name": "Rephael Wenger",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.GR",
        "cs.CV",
        "I.3"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16684v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16684v1",
      "primary_category": "cs.GR",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16677v1",
      "title": "Renaissance of RNNs in Streaming Clinical Time Series: Compact Recurrence Remains Competitive with Transformers",
      "abstract": "We present a compact, strictly causal benchmark for streaming clinical time\nseries on the MIT--BIH Arrhythmia Database using per-second heart rate. Two\ntasks are studied under record-level, non-overlapping splits: near-term\ntachycardia risk (next ten seconds) and one-step heart rate forecasting. We\ncompare a GRU-D (RNN) and a Transformer under matched training budgets against\nstrong non-learned baselines. Evaluation is calibration-aware for\nclassification and proper for forecasting, with temperature scaling and grouped\nbootstrap confidence intervals. On MIT-BIH, GRU-D slightly surpasses the\nTransformer for tachycardia risk, while the Transformer clearly lowers\nforecasting error relative to GRU-D and persistence. Our results show that, in\nlongitudinal monitoring, model choice is task-dependent: compact RNNs remain\ncompetitive for short-horizon risk scoring, whereas compact Transformers\ndeliver clearer gains for point forecasting.",
      "authors": [
        {
          "name": "Ran Tong",
          "affiliation": null
        },
        {
          "name": "Jiaqi Liu",
          "affiliation": null
        },
        {
          "name": "Su Liu",
          "affiliation": null
        },
        {
          "name": "Xin Hu",
          "affiliation": null
        },
        {
          "name": "Lanruo Wang",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16677v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16677v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16676v1",
      "title": "Active Target Discovery under Uninformative Prior: The Power of Permanent and Transient Memory",
      "abstract": "In many scientific and engineering fields, where acquiring high-quality data\nis expensive--such as medical imaging, environmental monitoring, and remote\nsensing--strategic sampling of unobserved regions based on prior observations\nis crucial for maximizing discovery rates within a constrained budget. The rise\nof powerful generative models, such as diffusion models, has enabled active\ntarget discovery in partially observable environments by leveraging learned\npriors--probabilistic representations that capture underlying structure from\ndata. With guidance from sequentially gathered task-specific observations,\nthese models can progressively refine exploration and efficiently direct\nqueries toward promising regions. However, in domains where learning a strong\nprior is infeasible due to extremely limited data or high sampling cost (such\nas rare species discovery, diagnostics for emerging diseases, etc.), these\nmethods struggle to generalize. To overcome this limitation, we propose a novel\napproach that enables effective active target discovery even in settings with\nuninformative priors, ensuring robust exploration and adaptability in complex\nreal-world scenarios. Our framework is theoretically principled and draws\ninspiration from neuroscience to guide its design. Unlike black-box policies,\nour approach is inherently interpretable, providing clear insights into\ndecision-making. Furthermore, it guarantees a strong, monotonic improvement in\nprior estimates with each new observation, leading to increasingly accurate\nsampling and reinforcing both reliability and adaptability in dynamic settings.\nThrough comprehensive experiments and ablation studies across various domains,\nincluding species distribution modeling and remote sensing, we demonstrate that\nour method substantially outperforms baseline approaches.",
      "authors": [
        {
          "name": "Anindya Sarkar",
          "affiliation": null
        },
        {
          "name": "Binglin Ji",
          "affiliation": null
        },
        {
          "name": "Yevgeniy Vorobeychik",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16676v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16676v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16675v1",
      "title": "Infinite Neural Operators: Gaussian processes on functions",
      "abstract": "A variety of infinitely wide neural architectures (e.g., dense NNs, CNNs, and\ntransformers) induce Gaussian process (GP) priors over their outputs. These\nrelationships provide both an accurate characterization of the prior predictive\ndistribution and enable the use of GP machinery to improve the uncertainty\nquantification of deep neural networks. In this work, we extend this connection\nto neural operators (NOs), a class of models designed to learn mappings between\nfunction spaces. Specifically, we show conditions for when arbitrary-depth NOs\nwith Gaussian-distributed convolution kernels converge to function-valued GPs.\nBased on this result, we show how to compute the covariance functions of these\nNO-GPs for two NO parametrizations, including the popular Fourier neural\noperator (FNO). With this, we compute the posteriors of these GPs in regression\nscenarios, including PDE solution operators. This work is an important step\ntowards uncovering the inductive biases of current FNO architectures and opens\na path to incorporate novel inductive biases for use in kernel-based operator\nlearning methods.",
      "authors": [
        {
          "name": "Daniel Augusto de Souza",
          "affiliation": null
        },
        {
          "name": "Yuchen Zhu",
          "affiliation": null
        },
        {
          "name": "Harry Jake Cunningham",
          "affiliation": null
        },
        {
          "name": "Yuri Saporito",
          "affiliation": null
        },
        {
          "name": "Diego Mesquita",
          "affiliation": null
        },
        {
          "name": "Marc Peter Deisenroth",
          "affiliation": null
        }
      ],
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16675v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16675v1",
      "primary_category": "stat.ML",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16674v1",
      "title": "Evaluating protein binding interfaces with PUMBA",
      "abstract": "Protein-protein docking tools help in studying interactions between proteins,\nand are essential for drug, vaccine, and therapeutic development. However, the\naccuracy of a docking tool depends on a robust scoring function that can\nreliably differentiate between native and non-native complexes. PIsToN is a\nstate-of-the-art deep learning-based scoring function that uses Vision\nTransformers in its architecture. Recently, the Mamba architecture has\ndemonstrated exceptional performance in both natural language processing and\ncomputer vision, often outperforming Transformer-based models in their domains.\nIn this study, we introduce PUMBA (Protein-protein interface evaluation with\nVision Mamba), which improves PIsToN by replacing its Vision Transformer\nbackbone with Vision Mamba. This change allows us to leverage Mamba's efficient\nlong-range sequence modeling for sequences of image patches. As a result, the\nmodel's ability to capture both global and local patterns in protein-protein\ninterface features is significantly improved. Evaluation on several\nwidely-used, large-scale public datasets demonstrates that PUMBA consistently\noutperforms its original Transformer-based predecessor, PIsToN.",
      "authors": [
        {
          "name": "Azam Shirali",
          "affiliation": null
        },
        {
          "name": "Giri Narasimhan",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "q-bio.QM"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16674v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16674v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-19"
    },
    {
      "arxiv_id": "2510.16670v1",
      "title": "All You Need is One: Capsule Prompt Tuning with a Single Vector",
      "abstract": "Prompt-based learning has emerged as a parameter-efficient finetuning (PEFT)\napproach to facilitate Large Language Model (LLM) adaptation to downstream\ntasks by conditioning generation with task-aware guidance. Despite its\nsuccesses, current prompt-based learning methods heavily rely on laborious grid\nsearching for optimal prompt length and typically require considerable number\nof prompts, introducing additional computational burden. Worse yet, our pioneer\nfindings indicate that the task-aware prompt design is inherently limited by\nits absence of instance-aware information, leading to a subtle attention\ninterplay with the input sequence. In contrast, simply incorporating\ninstance-aware information as a part of the guidance can enhance the\nprompt-tuned model performance without additional fine-tuning. Moreover, we\nfind an interesting phenomenon, namely \"attention anchor\", that incorporating\ninstance-aware tokens at the earliest position of the sequence can successfully\npreserve strong attention to critical structural information and exhibit more\nactive attention interaction with all input tokens. In light of our\nobservation, we introduce Capsule Prompt-Tuning (CaPT), an efficient and\neffective solution that leverages off-the-shelf, informative instance semantics\ninto prompt-based learning. Our approach innovatively integrates both\ninstance-aware and task-aware information in a nearly parameter-free manner\n(i.e., one single capsule prompt). Empirical results demonstrate that our\nmethod can exhibit superior performance across various language tasks (e.g.,\n84.03\\% average accuracy on T5-Large), serving as an \"attention anchor,\" while\nenjoying high parameter efficiency (e.g., 0.003\\% of model parameters on\nLlama3.2-1B).",
      "authors": [
        {
          "name": "Yiyang Liu",
          "affiliation": null
        },
        {
          "name": "James C. Liang",
          "affiliation": null
        },
        {
          "name": "Heng Fan",
          "affiliation": null
        },
        {
          "name": "Wenhao Yang",
          "affiliation": null
        },
        {
          "name": "Yiming Cui",
          "affiliation": null
        },
        {
          "name": "Xiaotian Han",
          "affiliation": null
        },
        {
          "name": "Lifu Huang",
          "affiliation": null
        },
        {
          "name": "Dongfang Liu",
          "affiliation": null
        },
        {
          "name": "Qifan Wang",
          "affiliation": null
        },
        {
          "name": "Cheng Han",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published_date": "2025-10-19",
      "arxiv_url": "http://arxiv.org/abs/2510.16670v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16670v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-19"
    }
  ]
}