{
  "fetch_date": "2025-10-21 12:52:54 UTC",
  "target_date": "2025-10-18",
  "total_papers": 133,
  "categories": [
    "cs.AI",
    "cs.LG",
    "cs.CV",
    "cs.CL",
    "stat.ML"
  ],
  "papers": [
    {
      "arxiv_id": "2510.16664v1",
      "title": "HYDRA: HYbrid knowledge Distillation and spectral Reconstruction Algorithm for high channel hyperspectral camera applications",
      "abstract": "Hyperspectral images (HSI) promise to support a range of new applications in\ncomputer vision. Recent research has explored the feasibility of generalizable\nSpectral Reconstruction (SR), the problem of recovering a HSI from a natural\nthree-channel color image in unseen scenarios.\n  However, previous Multi-Scale Attention (MSA) works have only demonstrated\nsufficient generalizable results for very sparse spectra, while modern HSI\nsensors contain hundreds of channels.\n  This paper introduces a novel approach to spectral reconstruction via our\nHYbrid knowledge Distillation and spectral Reconstruction Architecture (HYDRA).\n  Using a Teacher model that encapsulates latent hyperspectral image data and a\nStudent model that learns mappings from natural images to the Teacher's encoded\ndomain, alongside a novel training method, we achieve high-quality spectral\nreconstruction.\n  This addresses key limitations of prior SR models, providing SOTA performance\nacross all metrics, including an 18\\% boost in accuracy, and faster inference\ntimes than current SOTA models at various channel depths.",
      "authors": [
        {
          "name": "Christopher Thirgood",
          "affiliation": null
        },
        {
          "name": "Oscar Mendez",
          "affiliation": null
        },
        {
          "name": "Erin Ling",
          "affiliation": null
        },
        {
          "name": "Jon Storey",
          "affiliation": null
        },
        {
          "name": "Simon Hadfield",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16664v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16664v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16663v1",
      "title": "Robust Dynamic Staffing with Predictions",
      "abstract": "We consider a natural dynamic staffing problem in which a decision-maker\nsequentially hires workers over a finite horizon to meet an unknown demand\nrevealed at the end. Predictions about demand arrive over time and become\nincreasingly accurate, while worker availability decreases. This creates a\nfundamental trade-off between hiring early to avoid understaffing (when workers\nare more available but forecasts are less reliable) and hiring late to avoid\noverstaffing (when forecasts are more accurate but availability is lower). This\nproblem is motivated by last-mile delivery operations, where companies such as\nAmazon rely on gig-economy workers whose availability declines closer to the\noperating day.\n  To address practical limitations of Bayesian models (in particular, to remain\nagnostic to the underlying forecasting method), we study this problem under\nadversarial predictions. In this model, sequential predictions are\nadversarially chosen uncertainty intervals that (approximately) contain the\ntrue demand. The objective is to minimize worst-case staffing imbalance cost.\nOur main result is a simple and computationally efficient online algorithm that\nis minimax optimal. We first characterize the minimax cost against a restricted\nadversary via a polynomial-size linear program, then show how to emulate this\nsolution in the general case. While our base model focuses on a single demand,\nwe extend the framework to multiple demands (with egalitarian/utilitarian\nobjectives), to settings with costly reversals of hiring decisions, and to\ninconsistent prediction intervals. We also introduce a practical \"re-solving\"\nvariant of our algorithm, which we prove is also minimax optimal. Finally we\nconduct numerical experiments showing that our algorithms outperform Bayesian\nheuristics in both cost and speed, and are competitive with (approximate or\nexact) Bayesian-optimal policies when those can be computed.",
      "authors": [
        {
          "name": "Yiding Feng",
          "affiliation": null
        },
        {
          "name": "Vahideh Manshadi",
          "affiliation": null
        },
        {
          "name": "Rad Niazadeh",
          "affiliation": null
        },
        {
          "name": "Saba Neyshabouri",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.DS",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16663v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16663v1",
      "primary_category": "cs.DS",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16662v1",
      "title": "Safire: Similarity Framework for Visualization Retrieval",
      "abstract": "Effective visualization retrieval necessitates a clear definition of\nsimilarity. Despite the growing body of work in specialized visualization\nretrieval systems, a systematic approach to understanding visualization\nsimilarity remains absent. We introduce the Similarity Framework for\nVisualization Retrieval (Safire), a conceptual model that frames visualization\nsimilarity along two dimensions: comparison criteria and representation\nmodalities. Comparison criteria identify the aspects that make visualizations\nsimilar, which we divide into primary facets (data, visual encoding,\ninteraction, style, metadata) and derived properties (data-centric and\nhuman-centric measures). Safire connects what to compare with how comparisons\nare executed through representation modalities. We categorize existing\nrepresentation approaches into four groups based on their levels of information\ncontent and visualization determinism: raster image, vector image,\nspecification, and natural language description, together guiding what is\ncomputable and comparable. We analyze several visualization retrieval systems\nusing Safire to demonstrate its practical value in clarifying similarity\nconsiderations. Our findings reveal how particular criteria and modalities\nalign across different use cases. Notably, the choice of representation\nmodality is not only an implementation detail but also an important decision\nthat shapes retrieval capabilities and limitations. Based on our analysis, we\nprovide recommendations and discuss broader implications for multimodal\nlearning, AI applications, and visualization reproducibility.",
      "authors": [
        {
          "name": "Huyen N. Nguyen",
          "affiliation": null
        },
        {
          "name": "Nils Gehlenborg",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.IR",
        "cs.LG",
        "H.1.2; H.3.3; I.3.6"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16662v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16662v1",
      "primary_category": "cs.HC",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16660v1",
      "title": "Universal and Transferable Attacks on Pathology Foundation Models",
      "abstract": "We introduce Universal and Transferable Adversarial Perturbations (UTAP) for\npathology foundation models that reveal critical vulnerabilities in their\ncapabilities. Optimized using deep learning, UTAP comprises a fixed and weak\nnoise pattern that, when added to a pathology image, systematically disrupts\nthe feature representation capabilities of multiple pathology foundation\nmodels. Therefore, UTAP induces performance drops in downstream tasks that\nutilize foundation models, including misclassification across a wide range of\nunseen data distributions. In addition to compromising the model performance,\nwe demonstrate two key features of UTAP: (1) universality: its perturbation can\nbe applied across diverse field-of-views independent of the dataset that UTAP\nwas developed on, and (2) transferability: its perturbation can successfully\ndegrade the performance of various external, black-box pathology foundation\nmodels - never seen before. These two features indicate that UTAP is not a\ndedicated attack associated with a specific foundation model or image dataset,\nbut rather constitutes a broad threat to various emerging pathology foundation\nmodels and their applications. We systematically evaluated UTAP across various\nstate-of-the-art pathology foundation models on multiple datasets, causing a\nsignificant drop in their performance with visually imperceptible modifications\nto the input images using a fixed noise pattern. The development of these\npotent attacks establishes a critical, high-standard benchmark for model\nrobustness evaluation, highlighting a need for advancing defense mechanisms and\npotentially providing the necessary assets for adversarial training to ensure\nthe safe and reliable deployment of AI in pathology.",
      "authors": [
        {
          "name": "Yuntian Wang",
          "affiliation": null
        },
        {
          "name": "Xilin Yang",
          "affiliation": null
        },
        {
          "name": "Che-Yung Shen",
          "affiliation": null
        },
        {
          "name": "Nir Pillar",
          "affiliation": null
        },
        {
          "name": "Aydogan Ozcan",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "cs.LG",
        "physics.med-ph"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16660v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16660v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16658v1",
      "title": "Foundation and Large-Scale AI Models in Neuroscience: A Comprehensive Review",
      "abstract": "The advent of large-scale artificial intelligence (AI) models has a\ntransformative effect on neuroscience research, which represents a paradigm\nshift from the traditional computational methods through the facilitation of\nend-to-end learning from raw brain signals and neural data. In this paper, we\nexplore the transformative effects of large-scale AI models on five major\nneuroscience domains: neuroimaging and data processing, brain-computer\ninterfaces and neural decoding, molecular neuroscience and genomic modeling,\nclinical assistance and translational frameworks, and disease-specific\napplications across neurological and psychiatric disorders. These models are\ndemonstrated to address major computational neuroscience challenges, including\nmultimodal neural data integration, spatiotemporal pattern interpretation, and\nthe derivation of translational frameworks for clinical deployment. Moreover,\nthe interaction between neuroscience and AI has become increasingly reciprocal,\nas biologically informed architectural constraints are now incorporated to\ndevelop more interpretable and computationally efficient models. This review\nhighlights both the notable promise of such technologies and key implementation\nconsiderations, with particular emphasis on rigorous evaluation frameworks,\neffective domain knowledge integration, and comprehensive ethical guidelines\nfor clinical use. Finally, a systematic listing of critical neuroscience\ndatasets used to derive and validate large-scale AI models across diverse\nresearch applications is provided.",
      "authors": [
        {
          "name": "Shihao Yang",
          "affiliation": null
        },
        {
          "name": "Xiying Huang",
          "affiliation": null
        },
        {
          "name": "Danilo Bernardo",
          "affiliation": null
        },
        {
          "name": "Jun-En Ding",
          "affiliation": null
        },
        {
          "name": "Andrew Michael",
          "affiliation": null
        },
        {
          "name": "Jingmei Yang",
          "affiliation": null
        },
        {
          "name": "Patrick Kwan",
          "affiliation": null
        },
        {
          "name": "Ashish Raj",
          "affiliation": null
        },
        {
          "name": "Feng Liu",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.AI",
        "cs.CE"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16658v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16658v1",
      "primary_category": "cs.AI",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16657v1",
      "title": "Escaping Model Collapse via Synthetic Data Verification: Near-term Improvements and Long-term Convergence",
      "abstract": "Synthetic data has been increasingly used to train frontier generative\nmodels. However, recent study raises key concerns that iteratively retraining a\ngenerative model on its self-generated synthetic data may keep deteriorating\nmodel performance, a phenomenon often coined model collapse. In this paper, we\ninvestigate ways to modify this synthetic retraining process to avoid model\ncollapse, and even possibly help reverse the trend from collapse to\nimprovement. Our key finding is that by injecting information through an\nexternal synthetic data verifier, whether a human or a better model, synthetic\nretraining will not cause model collapse. To develop principled understandings\nof the above insight, we situate our analysis in the foundational linear\nregression setting, showing that iterative retraining with verified synthetic\ndata can yield near-term improvements but ultimately drives the parameter\nestimate to the verifier's \"knowledge center\" in the long run. Our theory hence\npredicts that, unless the verifier is perfectly reliable, the early gains will\nplateau and may even reverse. Indeed, these theoretical insights are further\nconfirmed by our experiments on both linear regression as well as Variational\nAutoencoders (VAEs) trained on MNIST data.",
      "authors": [
        {
          "name": "Bingji Yi",
          "affiliation": null
        },
        {
          "name": "Qiyuan Liu",
          "affiliation": null
        },
        {
          "name": "Yuwei Cheng",
          "affiliation": null
        },
        {
          "name": "Haifeng Xu",
          "affiliation": null
        }
      ],
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16657v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16657v1",
      "primary_category": "stat.ML",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16656v1",
      "title": "Simulation-free Structure Learning for Stochastic Dynamics",
      "abstract": "Modeling dynamical systems and unraveling their underlying causal\nrelationships is central to many domains in the natural sciences. Various\nphysical systems, such as those arising in cell biology, are inherently\nhigh-dimensional and stochastic in nature, and admit only partial, noisy state\nmeasurements. This poses a significant challenge for addressing the problems of\nmodeling the underlying dynamics and inferring the network structure of these\nsystems. Existing methods are typically tailored either for structure learning\nor modeling dynamics at the population level, but are limited in their ability\nto address both problems together. In this work, we address both problems\nsimultaneously: we present StructureFlow, a novel and principled\nsimulation-free approach for jointly learning the structure and stochastic\npopulation dynamics of physical systems. We showcase the utility of\nStructureFlow for the tasks of structure learning from interventions and\ndynamical (trajectory) inference of conditional population dynamics. We\nempirically evaluate our approach on high-dimensional synthetic systems, a set\nof biologically plausible simulated systems, and an experimental single-cell\ndataset. We show that StructureFlow can learn the structure of underlying\nsystems while simultaneously modeling their conditional population dynamics --\na key step toward the mechanistic understanding of systems behavior.",
      "authors": [
        {
          "name": "Noah El Rimawi-Fine",
          "affiliation": null
        },
        {
          "name": "Adam Stecklov",
          "affiliation": null
        },
        {
          "name": "Lucas Nelson",
          "affiliation": null
        },
        {
          "name": "Mathieu Blanchette",
          "affiliation": null
        },
        {
          "name": "Alexander Tong",
          "affiliation": null
        },
        {
          "name": "Stephen Y. Zhang",
          "affiliation": null
        },
        {
          "name": "Lazar Atanackovic",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "q-bio.QM"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16656v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16656v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16652v1",
      "title": "ARCO-BO: Adaptive Resource-aware COllaborative Bayesian Optimization for Heterogeneous Multi-Agent Design",
      "abstract": "Modern scientific and engineering design increasingly involves distributed\noptimization, where agents such as laboratories, simulations, or industrial\npartners pursue related goals under differing conditions. These agents often\nface heterogeneities in objectives, evaluation budgets, and accessible design\nvariables, which complicates coordination and can lead to redundancy, poor\nresource use, and ineffective information sharing. Bayesian Optimization (BO)\nis a widely used decision-making framework for expensive black box functions,\nbut its single-agent formulation assumes centralized control and full data\nsharing. Recent collaborative BO methods relax these assumptions, yet they\noften require uniform resources, fully shared input spaces, and fixed task\nalignment, conditions rarely satisfied in practice. To address these\nchallenges, we introduce Adaptive Resource Aware Collaborative Bayesian\nOptimization (ARCO-BO), a framework that explicitly accounts for heterogeneity\nin multi-agent optimization. ARCO-BO combines three components: a similarity\nand optima-aware consensus mechanism for adaptive information sharing, a\nbudget-aware asynchronous sampling strategy for resource coordination, and a\npartial input space sharing for heterogeneous design spaces. Experiments on\nsynthetic and high-dimensional engineering problems show that ARCO-BO\nconsistently outperforms independent BO and existing collaborative BO via\nconsensus approach, achieving robust and efficient performance in complex\nmulti-agent settings.",
      "authors": [
        {
          "name": "Zihan Wang",
          "affiliation": null
        },
        {
          "name": "Yi-Ping Chen",
          "affiliation": null
        },
        {
          "name": "Tuba Dolar",
          "affiliation": null
        },
        {
          "name": "Wei Chen",
          "affiliation": null
        }
      ],
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16652v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16652v1",
      "primary_category": "stat.ML",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16645v1",
      "title": "Unleashing Diverse Thinking Modes in LLMs through Multi-Agent Collaboration",
      "abstract": "Large Language Models (LLMs) demonstrate strong performance but often lack\ninterpretable reasoning. This paper introduces the Multi-Agent Collaboration\nFramework for Diverse Thinking Modes (DiMo), which enhances both performance\nand interpretability by simulating a structured debate among four specialized\nLLM agents. Each agent embodies a distinct reasoning paradigm, allowing the\nframework to collaboratively explore diverse cognitive approaches. Through\niterative debate, agents challenge and refine initial responses, yielding more\nrobust conclusions and an explicit, auditable reasoning chain. Across six\nbenchmarks and under a unified open-source setup, DiMo improves accuracy over\nwidely used single-model and debate baselines, with the largest gains on math.\nWe position DiMo as a semantics-aware, Web-native multi-agent framework: it\nmodels human-machine intelligence with LLM agents that produce semantically\ntyped, URL-annotated evidence chains for explanations and user-friendly\ninteractions. Although our experiments use standard reasoning benchmarks, the\nframework is designed to be instantiated over Web corpora and knowledge graphs,\ncombining retrieval-augmented reasoning with structured justifications that\ndownstream systems can inspect and reuse.",
      "authors": [
        {
          "name": "Zhixuan He",
          "affiliation": null
        },
        {
          "name": "Yue Feng",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.MA"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16645v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16645v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16643v1",
      "title": "Structured Interfaces for Automated Reasoning with 3D Scene Graphs",
      "abstract": "In order to provide a robot with the ability to understand and react to a\nuser's natural language inputs, the natural language must be connected to the\nrobot's underlying representations of the world. Recently, large language\nmodels (LLMs) and 3D scene graphs (3DSGs) have become a popular choice for\ngrounding natural language and representing the world. In this work, we address\nthe challenge of using LLMs with 3DSGs to ground natural language. Existing\nmethods encode the scene graph as serialized text within the LLM's context\nwindow, but this encoding does not scale to large or rich 3DSGs. Instead, we\npropose to use a form of Retrieval Augmented Generation to select a subset of\nthe 3DSG relevant to the task. We encode a 3DSG in a graph database and provide\na query language interface (Cypher) as a tool to the LLM with which it can\nretrieve relevant data for language grounding. We evaluate our approach on\ninstruction following and scene question-answering tasks and compare against\nbaseline context window and code generation methods. Our results show that\nusing Cypher as an interface to 3D scene graphs scales significantly better to\nlarge, rich graphs on both local and cloud-based models. This leads to large\nperformance improvements in grounded language tasks while also substantially\nreducing the token count of the scene graph content. A video supplement is\navailable at https://www.youtube.com/watch?v=zY_YI9giZSA.",
      "authors": [
        {
          "name": "Aaron Ray",
          "affiliation": null
        },
        {
          "name": "Jacob Arkin",
          "affiliation": null
        },
        {
          "name": "Harel Biggie",
          "affiliation": null
        },
        {
          "name": "Chuchu Fan",
          "affiliation": null
        },
        {
          "name": "Luca Carlone",
          "affiliation": null
        },
        {
          "name": "Nicholas Roy",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO",
        "I.2.9; I.2.10; H.3.3"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16643v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16643v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16641v1",
      "title": "MultiVerse: A Multi-Turn Conversation Benchmark for Evaluating Large Vision and Language Models",
      "abstract": "Vision-and-Language Models (VLMs) have shown impressive capabilities on\nsingle-turn benchmarks, yet real-world applications often demand more intricate\nmulti-turn dialogues. Existing multi-turn datasets (e.g, MMDU, ConvBench) only\npartially capture the breadth and depth of conversational scenarios encountered\nby users. In this work, we introduce MultiVerse, a novel multi-turn\nconversation benchmark featuring 647 dialogues - each averaging four turns -\nderived from a diverse set of 12 popular VLM evaluation benchmarks. With 484\ntasks and 484 interaction goals, MultiVerse covers a wide range of topics, from\nfactual knowledge and perception to advanced reasoning tasks such as\nmathematics and coding. To facilitate robust assessment, we propose a\nchecklist-based evaluation method that leverages GPT-4o as the automated\nevaluator, measuring performance across 37 key aspects, including perceptual\naccuracy, linguistic clarity, and factual correctness. We evaluate 18 VLMs on\nMultiVerse, revealing that even the strongest models (e.g., GPT-4o) achieve\nonly a 50% success rate in complex multi-turn conversations, highlighting the\ndataset's challenging nature. Notably, we find that providing full dialogue\ncontext significantly enhances performance for smaller or weaker models,\nemphasizing the importance of in-context learning. We believe MultiVerse is a\nlandscape of evaluating multi-turn interaction abilities for VLMs.",
      "authors": [
        {
          "name": "Young-Jun Lee",
          "affiliation": null
        },
        {
          "name": "Byung-Kwan Lee",
          "affiliation": null
        },
        {
          "name": "Jianshu Zhang",
          "affiliation": null
        },
        {
          "name": "Yechan Hwang",
          "affiliation": null
        },
        {
          "name": "Byungsoo Ko",
          "affiliation": null
        },
        {
          "name": "Han-Gyu Kim",
          "affiliation": null
        },
        {
          "name": "Dongyu Yao",
          "affiliation": null
        },
        {
          "name": "Xuankun Rong",
          "affiliation": null
        },
        {
          "name": "Eojin Joo",
          "affiliation": null
        },
        {
          "name": "Seung-Ho Han",
          "affiliation": null
        },
        {
          "name": "Bowon Ko",
          "affiliation": null
        },
        {
          "name": "Ho-Jin Choi",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16641v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16641v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16637v1",
      "title": "A Versatile Framework for Designing Group-Sparse Adversarial Attacks",
      "abstract": "Existing adversarial attacks often neglect perturbation sparsity, limiting\ntheir ability to model structural changes and to explain how deep neural\nnetworks (DNNs) process meaningful input patterns. We propose ATOS (Attack\nThrough Overlapping Sparsity), a differentiable optimization framework that\ngenerates structured, sparse adversarial perturbations in element-wise,\npixel-wise, and group-wise forms. For white-box attacks on image classifiers,\nwe introduce the Overlapping Smoothed L0 (OSL0) function, which promotes\nconvergence to a stationary point while encouraging sparse, structured\nperturbations. By grouping channels and adjacent pixels, ATOS improves\ninterpretability and helps identify robust versus non-robust features. We\napproximate the L-infinity gradient using the logarithm of the sum of\nexponential absolute values to tightly control perturbation magnitude. On\nCIFAR-10 and ImageNet, ATOS achieves a 100% attack success rate while producing\nsignificantly sparser and more structurally coherent perturbations than prior\nmethods. The structured group-wise attack highlights critical regions from the\nnetwork's perspective, providing counterfactual explanations by replacing\nclass-defining regions with robust features from the target class.",
      "authors": [
        {
          "name": "Alireza Heshmati",
          "affiliation": null
        },
        {
          "name": "Saman Soleimani Roudi",
          "affiliation": null
        },
        {
          "name": "Sajjad Amini",
          "affiliation": null
        },
        {
          "name": "Shahrokh Ghaemmaghami",
          "affiliation": null
        },
        {
          "name": "Farokh Marvasti",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CR",
        "cs.LG",
        "eess.IV"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16637v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16637v1",
      "primary_category": "cs.CR",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16636v1",
      "title": "A three-step machine learning approach to predict market bubbles with financial news",
      "abstract": "This study presents a three-step machine learning framework to predict\nbubbles in the S&P 500 stock market by combining financial news sentiment with\nmacroeconomic indicators. Building on traditional econometric approaches, the\nproposed approach predicts bubble formation by integrating textual and\nquantitative data sources. In the first step, bubble periods in the S&P 500\nindex are identified using a right-tailed unit root test, a widely recognized\nreal-time bubble detection method. The second step extracts sentiment features\nfrom large-scale financial news articles using natural language processing\n(NLP) techniques, which capture investors' expectations and behavioral\npatterns. In the final step, ensemble learning methods are applied to predict\nbubble occurrences based on high sentiment-based and macroeconomic predictors.\nModel performance is evaluated through k-fold cross-validation and compared\nagainst benchmark machine learning algorithms. Empirical results indicate that\nthe proposed three-step ensemble approach significantly improves predictive\naccuracy and robustness, providing valuable early warning insights for\ninvestors, regulators, and policymakers in mitigating systemic financial risks.",
      "authors": [
        {
          "name": "Abraham Atsiwo",
          "affiliation": null
        }
      ],
      "categories": [
        "q-fin.ST",
        "cs.LG",
        "q-fin.CP"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16636v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16636v1",
      "primary_category": "q-fin.ST",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16635v1",
      "title": "Prompt Optimization via Retrieved Reasoning Assets and Multi-Agent Analysis",
      "abstract": "Prompt optimization has emerged as an effective alternative to retraining for\nimproving the performance of Large Language Models (LLMs). However, most\nexisting approaches treat evaluation as a black box, relying solely on\nnumerical scores while offering limited insight into why a prompt succeeds or\nfails. They also depend heavily on trial-and-error refinements, which are\ndifficult to interpret and control. In this paper, we introduce MA-SAPO, a\nMulti-Agent framework for Score-Aware Prompt Optimization. Compared to prior\nmethods, MA-SAPO explicitly couples evaluation outcomes with structured\nreasoning to guide systematic edits. The framework specifically consists of two\nstages: during the Reasoning Phase, agents collaboratively explain metric\nscores, diagnose weaknesses, and synthesize targeted refinements that are\nstored as reusable reasoning assets; during the Test Phase, agents retrieve\nthese assets to analyze optimized prompts and apply only evidence-grounded\nedits. By turning evaluation signals into interpretable reasoning chains,\nMA-SAPO produces prompt refinements that are more transparent, auditable, and\ncontrollable. Experiments on the HelpSteer1/2 benchmarks demonstrate consistent\nimprovements over single-pass prompting, retrieval-augmented baselines, and\nprior multi-agent strategies, validating the effectiveness of our approach.",
      "authors": [
        {
          "name": "Wonduk Seo",
          "affiliation": null
        },
        {
          "name": "Juhyeon Lee",
          "affiliation": null
        },
        {
          "name": "Junseo Koh",
          "affiliation": null
        },
        {
          "name": "Hyunjin An",
          "affiliation": null
        },
        {
          "name": "Jian Park",
          "affiliation": null
        },
        {
          "name": "Seunghyun Lee",
          "affiliation": null
        },
        {
          "name": "Haihua Chen",
          "affiliation": null
        },
        {
          "name": "Yi Bu",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.CL",
        "cs.HC",
        "cs.IR"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16635v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16635v1",
      "primary_category": "cs.MA",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16629v1",
      "title": "On the Impossibility of Retrain Equivalence in Machine Unlearning",
      "abstract": "Machine unlearning seeks to selectively remove the \"influence\" of specific\ntraining data on a model's outputs. The ideal goal is Retrain\nEquivalence--behavior identical to a model trained from scratch on only the\nretained data. This goal was formulated for models trained on i.i.d. data\nbatches, but modern pipelines often involve multi-stage training, with each\nstage having a distinct data distribution and objective. Examples include LLM\nfine-tuning for alignment, reasoning ability, etc. Our study shows via theory\nand experiments that this shift to multi-stage training introduces a\nfundamental barrier for machine unlearning. The theory indicates that the\noutcome of local unlearning--methods that only use gradients computed on the\nforget set--is path-dependent. That is, a model's behavior during unlearning is\ninfluenced by the order of its training stages during learning, making it\nimpossible for path-oblivious algorithms to universally achieve Retrain\nEquivalence. We empirically demonstrate the same phenomenon in LLM\npost-training across Llama and Qwen models (1B to 14B) with gradient ascent,\nNPO, and SimNPO local unlearning algorithms. Models fine-tuned via different\norderings of identical training stages diverge in behavior during unlearning,\nwith the degradation in GSM8K accuracy after unlearning varying by over 20%\nacross paths. We also observe that some learning paths consistently produce\nmodels that unlearn slowly. During unlearning, whether the probability mass\ngets squeezed into paraphrasing or alternative concepts is also path-dependent.\nThese results consistently show that Retrain Equivalence is an ill-posed target\nfor local unlearning algorithms, so long as the target models are trained in\nstages. In situations where access to models' training histories is hard, the\ncurrent work calls for rethinking the definition and desiderata of machine\nunlearning.",
      "authors": [
        {
          "name": "Jiatong Yu",
          "affiliation": null
        },
        {
          "name": "Yinghui He",
          "affiliation": null
        },
        {
          "name": "Anirudh Goyal",
          "affiliation": null
        },
        {
          "name": "Sanjeev Arora",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16629v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16629v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16624v1",
      "title": "Self-Supervised Learning to Fly using Efficient Semantic Segmentation and Metric Depth Estimation for Low-Cost Autonomous UAVs",
      "abstract": "This paper presents a vision-only autonomous flight system for small UAVs\noperating in controlled indoor environments. The system combines semantic\nsegmentation with monocular depth estimation to enable obstacle avoidance,\nscene exploration, and autonomous safe landing operations without requiring GPS\nor expensive sensors such as LiDAR. A key innovation is an adaptive scale\nfactor algorithm that converts non-metric monocular depth predictions into\naccurate metric distance measurements by leveraging semantic ground plane\ndetection and camera intrinsic parameters, achieving a mean distance error of\n14.4 cm. The approach uses a knowledge distillation framework where a\ncolor-based Support Vector Machine (SVM) teacher generates training data for a\nlightweight U-Net student network (1.6M parameters) capable of real-time\nsemantic segmentation. For more complex environments, the SVM teacher can be\nreplaced with a state-of-the-art segmentation model. Testing was conducted in a\ncontrolled 5x4 meter laboratory environment with eight cardboard obstacles\nsimulating urban structures. Extensive validation across 30 flight tests in a\nreal-world environment and 100 flight tests in a digital-twin environment\ndemonstrates that the combined segmentation and depth approach increases the\ndistance traveled during surveillance and reduces mission time while\nmaintaining 100% success rates. The system is further optimized through\nend-to-end learning, where a compact student neural network learns complete\nflight policies from demonstration data generated by our best-performing\nmethod, achieving an 87.5% autonomous mission success rate. This work advances\npractical vision-based drone navigation in structured environments,\ndemonstrating solutions for metric depth estimation and computational\nefficiency challenges that enable deployment on resource-constrained platforms.",
      "authors": [
        {
          "name": "Sebastian Mocanu",
          "affiliation": null
        },
        {
          "name": "Emil Slusanschi",
          "affiliation": null
        },
        {
          "name": "Marius Leordeanu",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16624v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16624v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16614v1",
      "title": "Count Counts: Motivating Exploration in LLM Reasoning with Count-based Intrinsic Rewards",
      "abstract": "Reinforcement Learning (RL) has become a compelling way to strengthen the\nmulti step reasoning ability of Large Language Models (LLMs). However,\nprevalent RL paradigms still lean on sparse outcome-based rewards and limited\nexploration, which often drives LLMs toward repetitive and suboptimal reasoning\npatterns. In this paper, we study the central question of how to design\nexploration for LLM reasoning and introduce MERCI (Motivating Exploration in\nLLM Reasoning with Count-based Intrinsic Rewards), a novel RL algorithm that\naugments policy optimization with a principled intrinsic reward. Building on\nthe idea of count-based exploration, MERCI leverages a lightweight Coin\nFlipping Network (CFN) to estimate the pseudo count and further epistemic\nuncertainty over reasoning trajectories, and converts them into an intrinsic\nreward that values novelty while preserving the learning signal from task\nrewards. We integrate MERCI into some advanced RL frameworks like Group\nRelative Policy Optimization (GRPO). Experiments on complex reasoning\nbenchmarks demonstrate that MERCI encourages richer and more varied chains of\nthought, significantly improves performance over strong baselines, and helps\nthe policy escape local routines to discover better solutions. It indicates\nthat our targeted intrinsic motivation can make exploration reliable for\nlanguage model reasoning.",
      "authors": [
        {
          "name": "Xuan Zhang",
          "affiliation": null
        },
        {
          "name": "Ruixiao Li",
          "affiliation": null
        },
        {
          "name": "Zhijian Zhou",
          "affiliation": null
        },
        {
          "name": "Long Li",
          "affiliation": null
        },
        {
          "name": "Yulei Qin",
          "affiliation": null
        },
        {
          "name": "Ke Li",
          "affiliation": null
        },
        {
          "name": "Xing Sun",
          "affiliation": null
        },
        {
          "name": "Xiaoyu Tan",
          "affiliation": null
        },
        {
          "name": "Chao Qu",
          "affiliation": null
        },
        {
          "name": "Yuan Qi",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.AI"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16614v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16614v1",
      "primary_category": "cs.AI",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16612v1",
      "title": "Accelerated Learning on Large Scale Screens using Generative Library Models",
      "abstract": "Biological machine learning is often bottlenecked by a lack of scaled data.\nOne promising route to relieving data bottlenecks is through high throughput\nscreens, which can experimentally test the activity of $10^6-10^{12}$ protein\nsequences in parallel. In this article, we introduce algorithms to optimize\nhigh throughput screens for data creation and model training. We focus on the\nlarge scale regime, where dataset sizes are limited by the cost of measurement\nand sequencing. We show that when active sequences are rare, we maximize\ninformation gain if we only collect positive examples of active sequences, i.e.\n$x$ with $y>0$. We can correct for the missing negative examples using a\ngenerative model of the library, producing a consistent and efficient estimate\nof the true $p(y | x)$. We demonstrate this approach in simulation and on a\nlarge scale screen of antibodies. Overall, co-design of experiments and\ninference lets us accelerate learning dramatically.",
      "authors": [
        {
          "name": "Eli N. Weinstein",
          "affiliation": null
        },
        {
          "name": "Andrei Slabodkin",
          "affiliation": null
        },
        {
          "name": "Mattia G. Gollub",
          "affiliation": null
        },
        {
          "name": "Elizabeth B. Wood",
          "affiliation": null
        }
      ],
      "categories": [
        "stat.ML",
        "cs.LG",
        "q-bio.BM"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16612v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16612v1",
      "primary_category": "stat.ML",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16611v1",
      "title": "A Deep Learning Framework for Real-Time Image Processing in Medical Diagnostics: Enhancing Accuracy and Speed in Clinical Applications",
      "abstract": "Medical imaging plays a vital role in modern diagnostics; however,\ninterpreting high-resolution radiological data remains time-consuming and\nsusceptible to variability among clinicians. Traditional image processing\ntechniques often lack the precision, robustness, and speed required for\nreal-time clinical use. To overcome these limitations, this paper introduces a\ndeep learning framework for real-time medical image analysis designed to\nenhance diagnostic accuracy and computational efficiency across multiple\nimaging modalities, including X-ray, CT, and MRI. The proposed system\nintegrates advanced neural network architectures such as U-Net, EfficientNet,\nand Transformer-based models with real-time optimization strategies including\nmodel pruning, quantization, and GPU acceleration. The framework enables\nflexible deployment on edge devices, local servers, and cloud infrastructures,\nensuring seamless interoperability with clinical systems such as PACS and EHR.\nExperimental evaluations on public benchmark datasets demonstrate\nstate-of-the-art performance, achieving classification accuracies above 92%,\nsegmentation Dice scores exceeding 91%, and inference times below 80\nmilliseconds. Furthermore, visual explanation tools such as Grad-CAM and\nsegmentation overlays enhance transparency and clinical interpretability. These\nresults indicate that the proposed framework can substantially accelerate\ndiagnostic workflows, reduce clinician workload, and support trustworthy AI\nintegration in time-critical healthcare environments.",
      "authors": [
        {
          "name": "Melika Filvantorkaman",
          "affiliation": null
        },
        {
          "name": "Maral Filvan Torkaman",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16611v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16611v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16609v1",
      "title": "Prior Makes It Possible: From Sublinear Graph Algorithms to LLM Test-Time Methods",
      "abstract": "Test-time augmentation, such as Retrieval-Augmented Generation (RAG) or tool\nuse, critically depends on an interplay between a model's parametric knowledge\nand externally retrieved information. However, the theoretical underpinnings of\nthis relationship remain poorly understood. Specifically, it is not clear how\nmuch pre-training knowledge is required to answer queries with a small number\nof augmentation steps, which is a desirable property in practice. To address\nthis question, we formulate multi-step reasoning as an $s$-$t$ connectivity\nproblem on a knowledge graph. We represent a model's pre-training parametric\nknowledge as a partial, potentially noisy subgraph. We view augmentation as\nquerying an oracle for true edges that augment the model's knowledge. Then, we\ncharacterize the necessary and sufficient number of augmentation steps for the\nmodel to generate an accurate answer given partial prior knowledge. One key\nresult shows a phase transition: if the prior knowledge graph over $n$ vertices\nis disconnected into small components, then finding a path via augmentation is\ninefficient and requires $\\Omega(\\sqrt{n})$ queries. On the other hand, once\nthe density of correct knowledge surpasses a threshold, forming a giant\ncomponent, we can find paths with an expected constant number of queries.",
      "authors": [
        {
          "name": "Avrim Blum",
          "affiliation": null
        },
        {
          "name": "Daniel Hsu",
          "affiliation": null
        },
        {
          "name": "Cyrus Rashtchian",
          "affiliation": null
        },
        {
          "name": "Donya Saless",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CC",
        "cs.DS"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16609v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16609v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16607v1",
      "title": "Asymptotically Stable Quaternion-valued Hopfield-structured Neural Network with Periodic Projection-based Supervised Learning Rules",
      "abstract": "Motivated by the geometric advantages of quaternions in representing\nrotations and postures, we propose a quaternion-valued supervised learning\nHopfield-structured neural network (QSHNN) with a fully connected structure\ninspired by the classic Hopfield neural network (HNN). Starting from a\ncontinuous-time dynamical model of HNNs, we extend the formulation to the\nquaternionic domain and establish the existence and uniqueness of fixed points\nwith asymptotic stability. For the learning rules, we introduce a periodic\nprojection strategy that modifies standard gradient descent by periodically\nprojecting each 4*4 block of the weight matrix onto the closest quaternionic\nstructure in the least-squares sense. This approach preserves both convergence\nand quaternionic consistency throughout training. Benefiting from this rigorous\nmathematical foundation, the experimental model implementation achieves high\naccuracy, fast convergence, and strong reliability across randomly generated\ntarget sets. Moreover, the evolution trajectories of the QSHNN exhibit\nwell-bounded curvature, i.e., sufficient smoothness, which is crucial for\napplications such as control systems or path planning modules in robotic arms,\nwhere joint postures are parameterized by quaternion neurons. Beyond these\napplication scenarios, the proposed model offers a practical implementation\nframework and a general mathematical methodology for designing neural networks\nunder hypercomplex or non-commutative algebraic structures.",
      "authors": [
        {
          "name": "Tianwei Wang",
          "affiliation": null
        },
        {
          "name": "Xinhui Ma",
          "affiliation": null
        },
        {
          "name": "Wei Pang",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16607v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16607v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16604v1",
      "title": "Fine-tuning of Large Language Models for Constituency Parsing Using a Sequence to Sequence Approach",
      "abstract": "Recent advances in natural language processing with large neural models have\nopened new possibilities for syntactic analysis based on machine learning. This\nwork explores a novel approach to phrase-structure analysis by fine-tuning\nlarge language models (LLMs) to translate an input sentence into its\ncorresponding syntactic structure. The main objective is to extend the\ncapabilities of MiSintaxis, a tool designed for teaching Spanish syntax.\nSeveral models from the Hugging Face repository were fine-tuned using training\ndata generated from the AnCora-ES corpus, and their performance was evaluated\nusing the F1 score. The results demonstrate high accuracy in phrase-structure\nanalysis and highlight the potential of this methodology.",
      "authors": [
        {
          "name": "Francisco Jose Cortes Delgado",
          "affiliation": null
        },
        {
          "name": "Eduardo Martinez Gracia",
          "affiliation": null
        },
        {
          "name": "Rafael Valencia Garcia",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL",
        "68T50",
        "I.2.7; I.2.6"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16604v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16604v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16601v1",
      "title": "Uncertain Knowledge Graph Completion via Semi-Supervised Confidence Distribution Learning",
      "abstract": "Uncertain knowledge graphs (UKGs) associate each triple with a confidence\nscore to provide more precise knowledge representations. Recently, since\nreal-world UKGs suffer from the incompleteness, uncertain knowledge graph (UKG)\ncompletion attracts more attention, aiming to complete missing triples and\nconfidences. Current studies attempt to learn UKG embeddings to solve this\nproblem, but they neglect the extremely imbalanced distributions of triple\nconfidences. This causes that the learnt embeddings are insufficient to\nhigh-quality UKG completion. Thus, in this paper, to address the above issue,\nwe propose a new semi-supervised Confidence Distribution Learning (ssCDL)\nmethod for UKG completion, where each triple confidence is transformed into a\nconfidence distribution to introduce more supervision information of different\nconfidences to reinforce the embedding learning process. ssCDL iteratively\nlearns UKG embedding by relational learning on labeled data (i.e., existing\ntriples with confidences) and unlabeled data with pseudo labels (i.e., unseen\ntriples with the generated confidences), which are predicted by meta-learning\nto augment the training data and rebalance the distribution of triple\nconfidences. Experiments on two UKG datasets demonstrate that ssCDL\nconsistently outperforms state-of-the-art baselines in different evaluation\nmetrics.",
      "authors": [
        {
          "name": "Tianxing Wu",
          "affiliation": null
        },
        {
          "name": "Shutong Zhu",
          "affiliation": null
        },
        {
          "name": "Jingting Wang",
          "affiliation": null
        },
        {
          "name": "Ning Xu",
          "affiliation": null
        },
        {
          "name": "Guilin Qi",
          "affiliation": null
        },
        {
          "name": "Haofen Wang",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.AI"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16601v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16601v1",
      "primary_category": "cs.AI",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16598v1",
      "title": "VisionSelector: End-to-End Learnable Visual Token Compression for Efficient Multimodal LLMs",
      "abstract": "Multimodal Large Language Models (MLLMs) encounter significant computational\nand memory bottlenecks from the massive number of visual tokens generated by\nhigh-resolution images or multi-image inputs. Previous token compression\ntechniques are often constrained by heuristic rules that risk discarding\ncritical information. They may suffer from biases, such as attention sinks,\nthat lead to sharp performance drops under aggressive compression ratios. To\naddress these limitations, we reformulate token compression as a lightweight\nplug-and-play framework that reformulates token compression into an end-to-end\nlearnable decision process. To be specific, we propose VisionSelector, a scorer\nmodule decoupled from the MLLM backbone that incorporates a differentiable\nTop-K mechanism and a curriculum annealing strategy to bridge the\ntraining-inference gap, enabling efficient and adaptive token selection various\narbitrary compression rates. Remarkably lightweight with only 12.85M trainable\nparameters, VisionSelector demonstrates generalization across various\ncompression rates and adaptively identifying critical tokens. This leads to\nsuperior performance across all compression budgets, evidenced by preserving\n100% accuracy on MME with 30% retention budget, outperforming prior methods by\n12.14% at 10% retention budget, and doubling prefill speed. Our code is\navailable at https://github.com/JulietChoo/VisionSelector .",
      "authors": [
        {
          "name": "Jiaying Zhu",
          "affiliation": null
        },
        {
          "name": "Yurui Zhu",
          "affiliation": null
        },
        {
          "name": "Xin Lu",
          "affiliation": null
        },
        {
          "name": "Wenrui Yan",
          "affiliation": null
        },
        {
          "name": "Dong Li",
          "affiliation": null
        },
        {
          "name": "Kunlin Liu",
          "affiliation": null
        },
        {
          "name": "Xueyang Fu",
          "affiliation": null
        },
        {
          "name": "Zheng-Jun Zha",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16598v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16598v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16596v1",
      "title": "SHIELD: Suppressing Hallucinations In LVLM Encoders via Bias and Vulnerability Defense",
      "abstract": "Large Vision-Language Models (LVLMs) excel in diverse cross-modal tasks.\nHowever, object hallucination, where models produce plausible but inaccurate\nobject descriptions, remains a significant challenge. In contrast to previous\nwork focusing on LLM components, this paper is the first to trace LVLM\nhallucinations to visual encoders and identifies three key issues: statistical\nbias, inherent bias, and vulnerability. To address these challenges, we propose\nSHIELD, a training-free framework that mitigates hallucinations through three\nstrategies: re-weighting visual tokens to reduce statistical bias, introducing\nnoise-derived tokens to counter inherent bias, and applying adversarial attacks\nwith contrastive decoding to address vulnerability. Experiments demonstrate\nthat SHIELD effectively mitigates object hallucinations across diverse\nbenchmarks and LVLM families. Moreover, SHIELD achieves strong performance on\nthe general LVLM benchmark, highlighting its broad applicability. Code will be\nreleased.",
      "authors": [
        {
          "name": "Yiyang Huang",
          "affiliation": null
        },
        {
          "name": "Liang Shi",
          "affiliation": null
        },
        {
          "name": "Yitian Zhang",
          "affiliation": null
        },
        {
          "name": "Yi Xu",
          "affiliation": null
        },
        {
          "name": "Yun Fu",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16596v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16596v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16591v1",
      "title": "Symmetry and Generalisation in Neural Approximations of Renormalisation Transformations",
      "abstract": "Deep learning models have proven enormously successful at using multiple\nlayers of representation to learn relevant features of structured data.\nEncoding physical symmetries into these models can improve performance on\ndifficult tasks, and recent work has motivated the principle of parameter\nsymmetry breaking and restoration as a unifying mechanism underlying their\nhierarchical learning dynamics. We evaluate the role of parameter symmetry and\nnetwork expressivity in the generalisation behaviour of neural networks when\nlearning a real-space renormalisation group (RG) transformation, using the\ncentral limit theorem (CLT) as a test case map. We consider simple multilayer\nperceptrons (MLPs) and graph neural networks (GNNs), and vary weight symmetries\nand activation functions across architectures. Our results reveal a competition\nbetween symmetry constraints and expressivity, with overly complex or\noverconstrained models generalising poorly. We analytically demonstrate this\npoor generalisation behaviour for certain constrained MLP architectures by\nrecasting the CLT as a cumulant recursion relation and making use of an\nestablished framework to propagate cumulants through MLPs. We also empirically\nvalidate an extension of this framework from MLPs to GNNs, elucidating the\ninternal information processing performed by these more complex models. These\nfindings offer new insight into the learning dynamics of symmetric networks and\ntheir limitations in modelling structured physical transformations.",
      "authors": [
        {
          "name": "Cassidy Ashworth",
          "affiliation": null
        },
        {
          "name": "Pietro Li\u00f2",
          "affiliation": null
        },
        {
          "name": "Francesco Caso",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cond-mat.stat-mech",
        "cs.AI",
        "stat.ML"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16591v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16591v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16590v1",
      "title": "Atom-anchored LLMs speak Chemistry: A Retrosynthesis Demonstration",
      "abstract": "Applications of machine learning in chemistry are often limited by the\nscarcity and expense of labeled data, restricting traditional supervised\nmethods. In this work, we introduce a framework for molecular reasoning using\ngeneral-purpose Large Language Models (LLMs) that operates without requiring\nlabeled training data. Our method anchors chain-of-thought reasoning to the\nmolecular structure by using unique atomic identifiers. First, the LLM performs\na one-shot task to identify relevant fragments and their associated chemical\nlabels or transformation classes. In an optional second step, this\nposition-aware information is used in a few-shot task with provided class\nexamples to predict the chemical transformation. We apply our framework to\nsingle-step retrosynthesis, a task where LLMs have previously underperformed.\nAcross academic benchmarks and expert-validated drug discovery molecules, our\nwork enables LLMs to achieve high success rates in identifying chemically\nplausible reaction sites ($\\geq90\\%$), named reaction classes ($\\geq40\\%$), and\nfinal reactants ($\\geq74\\%$). Beyond solving complex chemical tasks, our work\nalso provides a method to generate theoretically grounded synthetic datasets by\nmapping chemical knowledge onto the molecular structure and thereby addressing\ndata scarcity.",
      "authors": [
        {
          "name": "Alan Kai Hassen",
          "affiliation": null
        },
        {
          "name": "Andrius Bernatavicius",
          "affiliation": null
        },
        {
          "name": "Antonius P. A. Janssen",
          "affiliation": null
        },
        {
          "name": "Mike Preuss",
          "affiliation": null
        },
        {
          "name": "Gerard J. P. van Westen",
          "affiliation": null
        },
        {
          "name": "Djork-Arn\u00e9 Clevert",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.BM"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16590v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16590v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16588v1",
      "title": "Copy-Augmented Representation for Structure Invariant Template-Free Retrosynthesis",
      "abstract": "Retrosynthesis prediction is fundamental to drug discovery and chemical\nsynthesis, requiring the identification of reactants that can produce a target\nmolecule. Current template-free methods struggle to capture the structural\ninvariance inherent in chemical reactions, where substantial molecular\nscaffolds remain unchanged, leading to unnecessarily large search spaces and\nreduced prediction accuracy. We introduce C-SMILES, a novel molecular\nrepresentation that decomposes traditional SMILES into element-token pairs with\nfive special tokens, effectively minimizing editing distance between reactants\nand products. Building upon this representation, we incorporate a\ncopy-augmented mechanism that dynamically determines whether to generate new\ntokens or preserve unchanged molecular fragments from the product. Our approach\nintegrates SMILES alignment guidance to enhance attention consistency with\nground-truth atom mappings, enabling more chemically coherent predictions.\nComprehensive evaluation on USPTO-50K and large-scale USPTO-FULL datasets\ndemonstrates significant improvements: 67.2% top-1 accuracy on USPTO-50K and\n50.8% on USPTO-FULL, with 99.9% validity in generated molecules. This work\nestablishes a new paradigm for structure-aware molecular generation with direct\napplications in computational drug discovery.",
      "authors": [
        {
          "name": "Jiaxi Zhuang",
          "affiliation": null
        },
        {
          "name": "Yu Zhang",
          "affiliation": null
        },
        {
          "name": "Aimin Zhou",
          "affiliation": null
        },
        {
          "name": "Ying Qian",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16588v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16588v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16587v1",
      "title": "Multi-Marginal Schr\u00f6dinger Bridge Matching",
      "abstract": "Understanding the continuous evolution of populations from discrete temporal\nsnapshots is a critical research challenge, particularly in fields like\ndevelopmental biology and systems medicine where longitudinal tracking of\nindividual entities is often impossible. Such trajectory inference is vital for\nunraveling the mechanisms of dynamic processes. While Schr\\\"odinger Bridge (SB)\noffer a potent framework, their traditional application to pairwise time points\ncan be insufficient for systems defined by multiple intermediate snapshots.\nThis paper introduces Multi-Marginal Schr\\\"odinger Bridge Matching (MSBM), a\nnovel algorithm specifically designed for the multi-marginal SB problem. MSBM\nextends iterative Markovian fitting (IMF) to effectively handle multiple\nmarginal constraints. This technique ensures robust enforcement of all\nintermediate marginals while preserving the continuity of the learned global\ndynamics across the entire trajectory. Empirical validations on synthetic data\nand real-world single-cell RNA sequencing datasets demonstrate the competitive\nor superior performance of MSBM in capturing complex trajectories and\nrespecting intermediate distributions, all with notable computational\nefficiency.",
      "authors": [
        {
          "name": "Byoungwoo Park",
          "affiliation": null
        },
        {
          "name": "Juho Lee",
          "affiliation": null
        }
      ],
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16587v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16587v1",
      "primary_category": "stat.ML",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16582v1",
      "title": "Can Knowledge-Graph-based Retrieval Augmented Generation Really Retrieve What You Need?",
      "abstract": "Retrieval-Augmented Generation (RAG) based on knowledge graphs (KGs) enhances\nlarge language models (LLMs) by providing structured and interpretable external\nknowledge. However, existing KG-based RAG methods struggle to retrieve accurate\nand diverse information from text-rich KGs for complex real-world queries.\nProcess Reward Models (PRMs) offer a way to align the retrieval process of\nKG-based RAG with query-specific knowledge requirements, but they heavily rely\non process-level supervision signals that are expensive and hard to obtain on\nKGs. To address this challenge, we propose GraphFlow, a framework that\nefficiently retrieves accurate and diverse knowledge required for real-world\nqueries from text-rich KGs. GraphFlow employs a transition-based flow matching\nobjective to jointly optimize a retrieval policy and a flow estimator. The flow\nestimator factorizes the reward of the retrieval outcome into the intermediate\nretrieval states. Such reward factorization guides the retrieval policy to\nretrieve candidates from KGs in proportion to their reward. This allows\nGraphFlow to explore high-quality regions of KGs that yield diverse and\nrelevant results. We evaluate GraphFlow on the STaRK benchmark, which includes\nreal-world queries from multiple domains over text-rich KGs. GraphFlow\noutperforms strong KG-RAG baselines, including GPT-4o, by 10% on average in hit\nrate and recall. It also shows strong generalization to unseen KGs,\ndemonstrating its effectiveness and robustness.",
      "authors": [
        {
          "name": "Junchi Yu",
          "affiliation": null
        },
        {
          "name": "Yujie Liu",
          "affiliation": null
        },
        {
          "name": "Jindong Gu",
          "affiliation": null
        },
        {
          "name": "Philip Torr",
          "affiliation": null
        },
        {
          "name": "Dongzhan Zhou",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.AI"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16582v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16582v1",
      "primary_category": "cs.AI",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16581v1",
      "title": "Patronus: Safeguarding Text-to-Image Models against White-Box Adversaries",
      "abstract": "Text-to-image (T2I) models, though exhibiting remarkable creativity in image\ngeneration, can be exploited to produce unsafe images. Existing safety\nmeasures, e.g., content moderation or model alignment, fail in the presence of\nwhite-box adversaries who know and can adjust model parameters, e.g., by\nfine-tuning. This paper presents a novel defensive framework, named Patronus,\nwhich equips T2I models with holistic protection to defend against white-box\nadversaries. Specifically, we design an internal moderator that decodes unsafe\ninput features into zero vectors while ensuring the decoding performance of\nbenign input features. Furthermore, we strengthen the model alignment with a\ncarefully designed non-fine-tunable learning mechanism, ensuring the T2I model\nwill not be compromised by malicious fine-tuning. We conduct extensive\nexperiments to validate the intactness of the performance on safe content\ngeneration and the effectiveness of rejecting unsafe content generation.\nResults also confirm the resilience of Patronus against various fine-tuning\nattacks by white-box adversaries.",
      "authors": [
        {
          "name": "Xinfeng Li",
          "affiliation": null
        },
        {
          "name": "Shengyuan Pang",
          "affiliation": null
        },
        {
          "name": "Jialin Wu",
          "affiliation": null
        },
        {
          "name": "Jiangyi Deng",
          "affiliation": null
        },
        {
          "name": "Huanlong Zhong",
          "affiliation": null
        },
        {
          "name": "Yanjiao Chen",
          "affiliation": null
        },
        {
          "name": "Jie Zhang",
          "affiliation": null
        },
        {
          "name": "Wenyuan Xu",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CR",
        "cs.CV"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16581v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16581v1",
      "primary_category": "cs.CR",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16573v1",
      "title": "AI-Generated Text Detection in Low-Resource Languages: A Case Study on Urdu",
      "abstract": "Large Language Models (LLMs) are now capable of generating text that closely\nresembles human writing, making them powerful tools for content creation, but\nthis growing ability has also made it harder to tell whether a piece of text\nwas written by a human or by a machine. This challenge becomes even more\nserious for languages like Urdu, where there are very few tools available to\ndetect AI-generated text. To address this gap, we propose a novel AI-generated\ntext detection framework tailored for the Urdu language. A balanced dataset\ncomprising 1,800 humans authored, and 1,800 AI generated texts, sourced from\nmodels such as Gemini, GPT-4o-mini, and Kimi AI was developed. Detailed\nlinguistic and statistical analysis was conducted, focusing on features such as\ncharacter and word counts, vocabulary richness (Type Token Ratio), and N-gram\npatterns, with significance evaluated through t-tests and MannWhitney U tests.\nThree state-of-the-art multilingual transformer models such as\nmdeberta-v3-base, distilbert-base-multilingualcased, and xlm-roberta-base were\nfine-tuned on this dataset. The mDeBERTa-v3-base achieved the highest\nperformance, with an F1-score 91.29 and accuracy of 91.26% on the test set.\nThis research advances efforts in contesting misinformation and academic\nmisconduct in Urdu-speaking communities and contributes to the broader\ndevelopment of NLP tools for low resource languages.",
      "authors": [
        {
          "name": "Muhammad Ammar",
          "affiliation": null
        },
        {
          "name": "Hadiya Murad Hadi",
          "affiliation": null
        },
        {
          "name": "Usman Majeed Butt",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16573v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16573v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16572v1",
      "title": "Ripple Effect Protocol: Coordinating Agent Populations",
      "abstract": "Modern AI agents can exchange messages using protocols such as A2A and ACP,\nyet these mechanisms emphasize communication over coordination. As agent\npopulations grow, this limitation produces brittle collective behavior, where\nindividually smart agents converge on poor group outcomes. We introduce the\nRipple Effect Protocol (REP), a coordination protocol in which agents share not\nonly their decisions but also lightweight sensitivities - signals expressing\nhow their choices would change if key environmental variables shifted. These\nsensitivities ripple through local networks, enabling groups to align faster\nand more stably than with agent-centric communication alone. We formalize REP's\nprotocol specification, separating required message schemas from optional\naggregation rules, and evaluate it across scenarios with varying incentives and\nnetwork topologies. Benchmarks across three domains: (i) supply chain cascades\n(Beer Game), (ii) preference aggregation in sparse networks (Movie Scheduling),\nand (iii) sustainable resource allocation (Fishbanks) show that REP improves\ncoordination accuracy and efficiency over A2A by 41 to 100%, while flexibly\nhandling multimodal sensitivity signals from LLMs. By making coordination a\nprotocol-level capability, REP provides scalable infrastructure for the\nemerging Internet of Agents",
      "authors": [
        {
          "name": "Ayush Chopra",
          "affiliation": null
        },
        {
          "name": "Aman Sharma",
          "affiliation": null
        },
        {
          "name": "Feroz Ahmad",
          "affiliation": null
        },
        {
          "name": "Luca Muscariello",
          "affiliation": null
        },
        {
          "name": "Vijoy Pandey",
          "affiliation": null
        },
        {
          "name": "Ramesh Raskar",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16572v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16572v1",
      "primary_category": "cs.AI",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16567v1",
      "title": "Hallucination Benchmark for Speech Foundation Models",
      "abstract": "Hallucinations in automatic speech recognition (ASR) systems refer to fluent\nand coherent transcriptions produced by neural ASR models that are completely\nunrelated to the underlying acoustic input (i.e., the speech signal). While\nsimilar to conventional decoding errors in potentially compromising the\nusability of transcriptions for downstream applications, hallucinations can be\nmore detrimental due to their preservation of syntactically and semantically\nplausible structure. This apparent coherence can mislead subsequent processing\nstages and introduce serious risks, particularly in critical domains such as\nhealthcare and law. Conventional evaluation metrics are primarily centered on\nerror-based metrics and fail to distinguish between phonetic inaccuracies and\nhallucinations. Consequently, there is a critical need for new evaluation\nframeworks that can effectively identify and assess models with a heightened\npropensity for generating hallucinated content. To this end, we introduce\nSHALLOW, the first benchmark framework that systematically categorizes and\nquantifies hallucination phenomena in ASR along four complementary axes:\nlexical, phonetic, morphological, and semantic. We define targeted metrics\nwithin each category to produce interpretable profiles of model behavior.\nThrough evaluation across various architectures and speech domains, we have\nfound that SHALLOW metrics correlate strongly with word error rate (WER) when\nrecognition quality is high (i.e., low WER). Still, this correlation weakens\nsubstantially as WER increases. SHALLOW, therefore, captures fine-grained error\npatterns that WER fails to distinguish under degraded and challenging\nconditions. Our framework supports specific diagnosis of model weaknesses and\nprovides feedback for model improvement beyond what aggregate error rates can\noffer.",
      "authors": [
        {
          "name": "Alkis Koudounas",
          "affiliation": null
        },
        {
          "name": "Moreno La Quatra",
          "affiliation": null
        },
        {
          "name": "Manuel Giollo",
          "affiliation": null
        },
        {
          "name": "Sabato Marco Siniscalchi",
          "affiliation": null
        },
        {
          "name": "Elena Baralis",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16567v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16567v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16565v1",
      "title": "Language over Content: Tracing Cultural Understanding in Multilingual Large Language Models",
      "abstract": "Large language models (LLMs) are increasingly used across diverse cultural\ncontexts, making accurate cultural understanding essential. Prior evaluations\nhave mostly focused on output-level performance, obscuring the factors that\ndrive differences in responses, while studies using circuit analysis have\ncovered few languages and rarely focused on culture. In this work, we trace\nLLMs' internal cultural understanding mechanisms by measuring activation path\noverlaps when answering semantically equivalent questions under two conditions:\nvarying the target country while fixing the question language, and varying the\nquestion language while fixing the country. We also use same-language country\npairs to disentangle language from cultural aspects. Results show that internal\npaths overlap more for same-language, cross-country questions than for\ncross-language, same-country questions, indicating strong language-specific\npatterns. Notably, the South Korea-North Korea pair exhibits low overlap and\nhigh variability, showing that linguistic similarity does not guarantee aligned\ninternal representation.",
      "authors": [
        {
          "name": "Seungho Cho",
          "affiliation": null
        },
        {
          "name": "Changgeon Ko",
          "affiliation": null
        },
        {
          "name": "Eui Jun Hwang",
          "affiliation": null
        },
        {
          "name": "Junmyeong Lee",
          "affiliation": null
        },
        {
          "name": "Huije Lee",
          "affiliation": null
        },
        {
          "name": "Jong C. Park",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16565v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16565v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16559v1",
      "title": "BuildArena: A Physics-Aligned Interactive Benchmark of LLMs for Engineering Construction",
      "abstract": "Engineering construction automation aims to transform natural language\nspecifications into physically viable structures, requiring complex integrated\nreasoning under strict physical constraints. While modern LLMs possess broad\nknowledge and strong reasoning capabilities that make them promising candidates\nfor this domain, their construction competencies remain largely unevaluated. To\naddress this gap, we introduce BuildArena, the first physics-aligned\ninteractive benchmark designed for language-driven engineering construction. It\ncontributes to the community in four aspects: (1) a highly customizable\nbenchmarking framework for in-depth comparison and analysis of LLMs; (2) an\nextendable task design strategy spanning static and dynamic mechanics across\nmultiple difficulty tiers; (3) a 3D Spatial Geometric Computation Library for\nsupporting construction based on language instructions; (4) a baseline LLM\nagentic workflow that effectively evaluates diverse model capabilities. On\neight frontier LLMs, BuildArena comprehensively evaluates their capabilities\nfor language-driven and physics-grounded construction automation. The project\npage is at https://build-arena.github.io/.",
      "authors": [
        {
          "name": "Tian Xia",
          "affiliation": null
        },
        {
          "name": "Tianrun Gao",
          "affiliation": null
        },
        {
          "name": "Wenhao Deng",
          "affiliation": null
        },
        {
          "name": "Long Wei",
          "affiliation": null
        },
        {
          "name": "Xiaowei Qian",
          "affiliation": null
        },
        {
          "name": "Yixian Jiang",
          "affiliation": null
        },
        {
          "name": "Chenglei Yu",
          "affiliation": null
        },
        {
          "name": "Tailin Wu",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.AI"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16559v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16559v1",
      "primary_category": "cs.AI",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16558v1",
      "title": "Toward Understanding Security Issues in the Model Context Protocol Ecosystem",
      "abstract": "The Model Context Protocol (MCP) is an emerging open standard that enables\nAI-powered applications to interact with external tools through structured\nmetadata. A rapidly growing ecosystem has formed around MCP, including a wide\nrange of MCP hosts (i.e., Cursor, Windsurf, Claude Desktop, and Cline), MCP\nregistries (i.e., mcp.so, MCP Market, MCP Store, Pulse MCP, Smithery, and npm),\nand thousands of community-contributed MCP servers. Although the MCP ecosystem\nis gaining traction, there has been little systematic study of its architecture\nand associated security risks. In this paper, we present the first\ncomprehensive security analysis of the MCP ecosystem. We decompose MCP\necosystem into three core components: hosts, registries, and servers, and study\nthe interactions and trust relationships among them. Users search for servers\non registries and configure them in the host, which translates LLM-generated\noutput into external tool invocations provided by the servers and executes\nthem. Our qualitative analysis reveals that hosts lack output verification\nmechanisms for LLM-generated outputs, enabling malicious servers to manipulate\nmodel behavior and induce a variety of security threats, including but not\nlimited to sensitive data exfiltration. We uncover a wide range of\nvulnerabilities that enable attackers to hijack servers, due to the lack of a\nvetted server submission process in registries. To support our analysis, we\ncollect and analyze a dataset of 67,057 servers from six public registries. Our\nquantitative analysis demonstrates that a substantial number of servers can be\nhijacked by attackers. Finally, we propose practical defense strategies for MCP\nhosts, registries, and users. We responsibly disclosed our findings to affected\nhosts and registries.",
      "authors": [
        {
          "name": "Xiaofan Li",
          "affiliation": null
        },
        {
          "name": "Xing Gao",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16558v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16558v1",
      "primary_category": "cs.CR",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16556v1",
      "title": "Fit for Purpose? Deepfake Detection in the Real World",
      "abstract": "The rapid proliferation of AI-generated content, driven by advances in\ngenerative adversarial networks, diffusion models, and multimodal large\nlanguage models, has made the creation and dissemination of synthetic media\neffortless, heightening the risks of misinformation, particularly political\ndeepfakes that distort truth and undermine trust in political institutions. In\nturn, governments, research institutions, and industry have strongly promoted\ndeepfake detection initiatives as solutions. Yet, most existing models are\ntrained and validated on synthetic, laboratory-controlled datasets, limiting\ntheir generalizability to the kinds of real-world political deepfakes\ncirculating on social platforms that affect the public. In this work, we\nintroduce the first systematic benchmark based on the Political Deepfakes\nIncident Database, a curated collection of real-world political deepfakes\nshared on social media since 2018. Our study includes a systematic evaluation\nof state-of-the-art deepfake detectors across academia, government, and\nindustry. We find that the detectors from academia and government perform\nrelatively poorly. While paid detection tools achieve relatively higher\nperformance than free-access models, all evaluated detectors struggle to\ngeneralize effectively to authentic political deepfakes, and are vulnerable to\nsimple manipulations, especially in the video domain. Results urge the need for\npolitically contextualized deepfake detection frameworks to better safeguard\nthe public in real-world settings.",
      "authors": [
        {
          "name": "Guangyu Lin",
          "affiliation": null
        },
        {
          "name": "Li Lin",
          "affiliation": null
        },
        {
          "name": "Christina P. Walker",
          "affiliation": null
        },
        {
          "name": "Daniel S. Schiff",
          "affiliation": null
        },
        {
          "name": "Shu Hu",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16556v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16556v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16555v1",
      "title": "Urban-R1: Reinforced MLLMs Mitigate Geospatial Biases for Urban General Intelligence",
      "abstract": "Rapid urbanization intensifies the demand for Urban General Intelligence\n(UGI), referring to AI systems that can understand and reason about complex\nurban environments. Recent studies have built urban foundation models using\nsupervised fine-tuning (SFT) of LLMs and MLLMs, yet these models exhibit\npersistent geospatial bias, producing regionally skewed predictions and limited\ngeneralization. To this end, we propose Urban-R1, a reinforcement\nlearning-based post-training framework that aligns MLLMs with the objectives of\nUGI. Urban-R1 adopts Group Relative Policy Optimization (GRPO) to optimize\nreasoning across geographic groups and employs urban region profiling as a\nproxy task to provide measurable rewards from multimodal urban data. Extensive\nexperiments across diverse regions and tasks show that Urban-R1 effectively\nmitigates geo-bias and improves cross-region generalization, outperforming both\nSFT-trained and closed-source models. Our results highlight reinforcement\nlearning alignment as a promising pathway toward equitable and trustworthy\nurban intelligence.",
      "authors": [
        {
          "name": "Qiongyan Wang",
          "affiliation": null
        },
        {
          "name": "Xingchen Zou",
          "affiliation": null
        },
        {
          "name": "Yutian Jiang",
          "affiliation": null
        },
        {
          "name": "Haomin Wen",
          "affiliation": null
        },
        {
          "name": "Jiaheng Wei",
          "affiliation": null
        },
        {
          "name": "Qingsong Wen",
          "affiliation": null
        },
        {
          "name": "Yuxuan Liang",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16555v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16555v1",
      "primary_category": "cs.AI",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16552v1",
      "title": "LANPO: Bootstrapping Language and Numerical Feedback for Reinforcement Learning in LLMs",
      "abstract": "Reinforcement learning in large language models (LLMs) often relies on scalar\nrewards, a practice that discards valuable textual rationale buried in the\nrollouts, forcing the model to explore \\textit{de novo} with each attempt and\nhindering sample efficiency. While LLMs can uniquely learn from language\nfeedback provided in-context, naively integrating on-line experiences into RL\ntraining presents a paradox: feedback from the same problem risks information\nleakage and memorization, while feedback from different problems often leads to\nbehavior collapse due to irrelevant context. To resolve this tension, we\npropose \\textbf{Language-And-Numerical Policy Optimization (LANPO)}, a\nframework that cleanly separates the roles of feedback: language guides\nexploration, while numerical rewards drive optimization. LANPO builds a dynamic\nexperience pool from past trials and introduces two principles to ensure\nfeedback is effective: \\emph{Reward-Agnostic Reflection} for safe intra-sample\nself-correction and \\emph{Relevant Abstraction} to distill generalizable\nlessons from inter-sample experiences. Across mathematical reasoning\nbenchmarks, LANPO enables 7B and 14B models to significantly outperform strong\nbaselines trained with GRPO in test accuracy. Our work provides a robust method\nfor integrating historical experiences into the LLM RL loop, creating more\neffective and data-efficient learning agents.",
      "authors": [
        {
          "name": "Ang Li",
          "affiliation": null
        },
        {
          "name": "Yifei Wang",
          "affiliation": null
        },
        {
          "name": "Zhihang Yuan",
          "affiliation": null
        },
        {
          "name": "Stefanie Jegelka",
          "affiliation": null
        },
        {
          "name": "Yisen Wang",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16552v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16552v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16551v1",
      "title": "From Reviews to Actionable Insights: An LLM-Based Approach for Attribute and Feature Extraction",
      "abstract": "This research proposes a systematic, large language model (LLM) approach for\nextracting product and service attributes, features, and associated sentiments\nfrom customer reviews. Grounded in marketing theory, the framework\ndistinguishes perceptual attributes from actionable features, producing\ninterpretable and managerially actionable insights. We apply the methodology to\n20,000 Yelp reviews of Starbucks stores and evaluate eight prompt variants on a\nrandom subset of reviews. Model performance is assessed through agreement with\nhuman annotations and predictive validity for customer ratings. Results show\nhigh consistency between LLMs and human coders and strong predictive validity,\nconfirming the reliability of the approach. Human coders required a median of\nsix minutes per review, whereas the LLM processed each in two seconds,\ndelivering comparable insights at a scale unattainable through manual coding.\nManagerially, the analysis identifies attributes and features that most\nstrongly influence customer satisfaction and their associated sentiments,\nenabling firms to pinpoint \"joy points,\" address \"pain points,\" and design\ntargeted interventions. We demonstrate how structured review data can power an\nactionable marketing dashboard that tracks sentiment over time and across\nstores, benchmarks performance, and highlights high-leverage features for\nimprovement. Simulations indicate that enhancing sentiment for key service\nfeatures could yield 1-2% average revenue gains per store.",
      "authors": [
        {
          "name": "Khaled Boughanmi",
          "affiliation": null
        },
        {
          "name": "Kamel Jedidi",
          "affiliation": null
        },
        {
          "name": "Nour Jedidi",
          "affiliation": null
        }
      ],
      "categories": [
        "stat.ML",
        "cs.LG",
        "econ.EM"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16551v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16551v1",
      "primary_category": "stat.ML",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16549v1",
      "title": "ReviewGuard: Enhancing Deficient Peer Review Detection via LLM-Driven Data Augmentation",
      "abstract": "Peer review serves as the gatekeeper of science, yet the surge in submissions\nand widespread adoption of large language models (LLMs) in scholarly evaluation\npresent unprecedented challenges. Recent work has focused on using LLMs to\nimprove review efficiency or generate insightful review content. However,\nunchecked deficient reviews from both human experts and AI systems threaten to\nsystematically undermine the peer review ecosystem and compromise academic\nintegrity. To address this critical issue, we introduce ReviewGuard, an\nautomated system for detecting and categorizing deficient reviews. ReviewGuard\nemploys a comprehensive four-stage LLM-driven framework that: (1) collects ICLR\nand NeurIPS papers with their corresponding reviews from OpenReview; (2)\nannotates review types using GPT-4.1 with human validation; (3) addresses class\nimbalance and data scarcity through LLM-driven synthetic data augmentation,\nproducing a final corpus of 6,634 papers, 24,657 real reviews, and 46,438\nsynthetic reviews; and (4) fine-tunes both encoder-based models and open source\nLLMs. We perform comprehensive feature analysis of the structure and quality of\nthe review text. Compared to sufficient reviews, deficient reviews demonstrate\nlower rating scores, higher self-reported confidence, reduced structural\ncomplexity, and a higher proportion of negative sentiment. AI-generated text\ndetection reveals that, since ChatGPT's emergence, AI-generated reviews have\nincreased dramatically. In the evaluation of deficient review detection models,\nmixed training with synthetic and real review data provides substantial\nenhancements to recall and F1 scores on the binary task. This study presents\nthe first LLM-driven system for detecting deficient peer reviews, providing\nevidence to inform AI governance in peer review while offering valuable\ninsights into human-AI collaboration to maintain academic integrity.",
      "authors": [
        {
          "name": "Haoxuan Zhang",
          "affiliation": null
        },
        {
          "name": "Ruochi Li",
          "affiliation": null
        },
        {
          "name": "Sarthak Shrestha",
          "affiliation": null
        },
        {
          "name": "Shree Harshini Mamidala",
          "affiliation": null
        },
        {
          "name": "Revanth Putta",
          "affiliation": null
        },
        {
          "name": "Arka Krishan Aggarwal",
          "affiliation": null
        },
        {
          "name": "Ting Xiao",
          "affiliation": null
        },
        {
          "name": "Junhua Ding",
          "affiliation": null
        },
        {
          "name": "Haihua Chen",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16549v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16549v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16548v1",
      "title": "NeurIPT: Foundation Model for Neural Interfaces",
      "abstract": "Electroencephalography (EEG) has wide-ranging applications, from clinical\ndiagnosis to brain-computer interfaces (BCIs). With the increasing volume and\nvariety of EEG data, there has been growing interest in establishing foundation\nmodels (FMs) to scale up and generalize neural decoding. Despite showing early\npotential, applying FMs to EEG remains challenging due to substantial\ninter-subject, inter-task, and inter-condition variability, as well as diverse\nelectrode configurations across recording setups. To tackle these open\nchallenges, we propose NeurIPT, a foundation model developed for diverse\nEEG-based Neural Interfaces with a Pre-trained Transformer by capturing both\nhomogeneous and heterogeneous spatio-temporal characteristics inherent in EEG\nsignals. Temporally, we introduce Amplitude-Aware Masked Pretraining (AAMP),\nmasking based on signal amplitude rather than random intervals, to learn robust\nrepresentations across varying signal intensities beyond local interpolation.\nMoreover, this temporal representation is enhanced by a Progressive\nMixture-of-Experts (PMoE) architecture, where specialized expert subnetworks\nare progressively introduced at deeper layers, adapting effectively to the\ndiverse temporal characteristics of EEG signals. Spatially, NeurIPT leverages\nthe 3D physical coordinates of electrodes, enabling effective transfer of\nembedding across varying EEG settings, and develops Intra-Inter Lobe Pooling\n(IILP) during fine-tuning to efficiently exploit regional brain features.\nEmpirical evaluations across eight downstream BCI datasets, via fine-tuning,\ndemonstrated NeurIPT consistently achieved state-of-the-art performance,\nhighlighting its broad applicability and robust generalization. Our work pushes\nforward the state of FMs in EEG and offers insights into scalable and\ngeneralizable neural information processing systems.",
      "authors": [
        {
          "name": "Zitao Fang",
          "affiliation": null
        },
        {
          "name": "Chenxuan Li",
          "affiliation": null
        },
        {
          "name": "Hongting Zhou",
          "affiliation": null
        },
        {
          "name": "Shuyang Yu",
          "affiliation": null
        },
        {
          "name": "Guodong Du",
          "affiliation": null
        },
        {
          "name": "Ashwaq Qasem",
          "affiliation": null
        },
        {
          "name": "Yang Lu",
          "affiliation": null
        },
        {
          "name": "Jing Li",
          "affiliation": null
        },
        {
          "name": "Junsong Zhang",
          "affiliation": null
        },
        {
          "name": "Sim Kuan Goh",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16548v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16548v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16547v1",
      "title": "Predicting life satisfaction using machine learning and explainable AI",
      "abstract": "Life satisfaction is a crucial facet of human well-being. Hence, research on\nlife satisfaction is incumbent for understanding how individuals experience\ntheir lives and influencing interventions targeted at enhancing mental health\nand well-being. Life satisfaction has traditionally been measured using analog,\ncomplicated, and frequently error-prone methods. These methods raise questions\nconcerning validation and propagation. However, this study demonstrates the\npotential for machine learning algorithms to predict life satisfaction with a\nhigh accuracy of 93.80% and a 73.00% macro F1-score. The dataset comes from a\ngovernment survey of 19000 people aged 16-64 years in Denmark. Using feature\nlearning techniques, 27 significant questions for assessing contentment were\nextracted, making the study highly reproducible, simple, and easily\ninterpretable. Furthermore, clinical and biomedical large language models\n(LLMs) were explored for predicting life satisfaction by converting tabular\ndata into natural language sentences through mapping and adding meaningful\ncounterparts, achieving an accuracy of 93.74% and macro F1-score of 73.21%. It\nwas found that life satisfaction prediction is more closely related to the\nbiomedical domain than the clinical domain. Ablation studies were also\nconducted to understand the impact of data resampling and feature selection\ntechniques on model performance. Moreover, the correlation between primary\ndeterminants with different age brackets was analyzed, and it was found that\nhealth condition is the most important determinant across all ages. This study\ndemonstrates how machine learning, large language models and XAI can jointly\ncontribute to building trust and understanding in using AI to investigate human\nbehavior, with significant ramifications for academics and professionals\nworking to quantify and comprehend subjective well-being.",
      "authors": [
        {
          "name": "Alif Elham Khan",
          "affiliation": null
        },
        {
          "name": "Mohammad Junayed Hasan",
          "affiliation": null
        },
        {
          "name": "Humayra Anjum",
          "affiliation": null
        },
        {
          "name": "Nabeel Mohammed",
          "affiliation": null
        },
        {
          "name": "Sifat Momen",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16547v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16547v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16541v1",
      "title": "Watch Where You Move: Region-aware Dynamic Aggregation and Excitation for Gait Recognition",
      "abstract": "Deep learning-based gait recognition has achieved great success in various\napplications. The key to accurate gait recognition lies in considering the\nunique and diverse behavior patterns in different motion regions, especially\nwhen covariates affect visual appearance. However, existing methods typically\nuse predefined regions for temporal modeling, with fixed or equivalent temporal\nscales assigned to different types of regions, which makes it difficult to\nmodel motion regions that change dynamically over time and adapt to their\nspecific patterns. To tackle this problem, we introduce a Region-aware Dynamic\nAggregation and Excitation framework (GaitRDAE) that automatically searches for\nmotion regions, assigns adaptive temporal scales and applies corresponding\nattention. Specifically, the framework includes two core modules: the\nRegion-aware Dynamic Aggregation (RDA) module, which dynamically searches the\noptimal temporal receptive field for each region, and the Region-aware Dynamic\nExcitation (RDE) module, which emphasizes the learning of motion regions\ncontaining more stable behavior patterns while suppressing attention to static\nregions that are more susceptible to covariates. Experimental results show that\nGaitRDAE achieves state-of-the-art performance on several benchmark datasets.",
      "authors": [
        {
          "name": "Binyuan Huang",
          "affiliation": null
        },
        {
          "name": "Yongdong Luo",
          "affiliation": null
        },
        {
          "name": "Xianda Guo",
          "affiliation": null
        },
        {
          "name": "Xiawu Zheng",
          "affiliation": null
        },
        {
          "name": "Zheng Zhu",
          "affiliation": null
        },
        {
          "name": "Jiahui Pan",
          "affiliation": null
        },
        {
          "name": "Chengju Zhou",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16541v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16541v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16540v1",
      "title": "Enhancing Compositional Reasoning in CLIP via Reconstruction and Alignment of Text Descriptions",
      "abstract": "Despite recent advances, vision-language models trained with standard\ncontrastive objectives still struggle with compositional reasoning -- the\nability to understand structured relationships between visual and linguistic\nelements. This shortcoming is largely due to the tendency of the text encoder\nto focus on individual words rather than their relations, a limitation\nreinforced by contrastive training that primarily aligns words with visual\nobjects. In this paper, we introduce REconstruction and Alignment of text\nDescriptions (READ), a fine-tuning method designed to enhance compositional\nreasoning by adding two auxiliary objectives to the contrastive learning: (1) a\ntoken-level reconstruction objective, where a frozen pre-trained decoder\nreconstructs alternative captions based on the embedding of the original\ncaption; and (2) a sentence-level alignment objective, which explicitly aligns\nparaphrased sentences in the embedding space. We show that READ-CLIP, a model\nderived by applying the READ method to the pre-trained CLIP model, achieves the\nstate-of-the-art performance across five major compositional reasoning\nbenchmarks, outperforming the strongest conventional fine-tuning baseline by up\nto 4.1%. Furthermore, applying the READ to existing CLIP variants (including\nNegCLIP and FSC-CLIP) also improves performance on these benchmarks.\nQuantitative and qualitative analyses reveal that our proposed objectives --\nreconstruction and alignment -- offer complementary benefits: the former\nencourages the encoder to capture relationships between words within a caption,\nwhile the latter ensures consistent representations for paraphrases expressed\nwith different wording.",
      "authors": [
        {
          "name": "Jihoon Kwon",
          "affiliation": null
        },
        {
          "name": "Kyle Min",
          "affiliation": null
        },
        {
          "name": "Jy-yong Sohn",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16540v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16540v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16536v1",
      "title": "Few-Label Multimodal Modeling of SNP Variants and ECG Phenotypes Using Large Language Models for Cardiovascular Risk Stratification",
      "abstract": "Cardiovascular disease (CVD) risk stratification remains a major challenge\ndue to its multifactorial nature and limited availability of high-quality\nlabeled datasets. While genomic and electrophysiological data such as SNP\nvariants and ECG phenotypes are increasingly accessible, effectively\nintegrating these modalities in low-label settings is non-trivial. This\nchallenge arises from the scarcity of well-annotated multimodal datasets and\nthe high dimensionality of biological signals, which limit the effectiveness of\nconventional supervised models. To address this, we present a few-label\nmultimodal framework that leverages large language models (LLMs) to combine\ngenetic and electrophysiological information for cardiovascular risk\nstratification. Our approach incorporates a pseudo-label refinement strategy to\nadaptively distill high-confidence labels from weakly supervised predictions,\nenabling robust model fine-tuning with only a small set of ground-truth\nannotations. To enhance the interpretability, we frame the task as a Chain of\nThought (CoT) reasoning problem, prompting the model to produce clinically\nrelevant rationales alongside predictions. Experimental results demonstrate\nthat the integration of multimodal inputs, few-label supervision, and CoT\nreasoning improves robustness and generalizability across diverse patient\nprofiles. Experimental results using multimodal SNP variants and ECG-derived\nfeatures demonstrated comparable performance to models trained on the full\ndataset, underscoring the promise of LLM-based few-label multimodal modeling\nfor advancing personalized cardiovascular care.",
      "authors": [
        {
          "name": "Niranjana Arun Menon",
          "affiliation": null
        },
        {
          "name": "Yulong Li",
          "affiliation": null
        },
        {
          "name": "Iqra Farooq",
          "affiliation": null
        },
        {
          "name": "Sara Ahmed",
          "affiliation": null
        },
        {
          "name": "Muhammad Awais",
          "affiliation": null
        },
        {
          "name": "Imran Razzak",
          "affiliation": null
        }
      ],
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "cs.LG"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16536v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16536v1",
      "primary_category": "q-bio.QM",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16533v1",
      "title": "Hey Pentti, We Did It Again!: Differentiable vector-symbolic types that prove polynomial termination",
      "abstract": "We present a typed computer language, Doug, in which all typed programs may\nbe proved to halt in polynomial time, encoded in a vector-symbolic architecture\n(VSA). Doug is just an encoding of the light linear functional programming\nlanguage (LLFPL) described by (Schimanski2009, ch. 7). The types of Doug are\nencoded using a slot-value encoding scheme based on holographic declarative\nmemory (HDM; Kelly, 2020). The terms of Doug are encoded using a variant of the\nLisp VSA defined by (Flanagan, 2024). Doug allows for some points on the\nembedding space of a neural network to be interpreted as types, where the types\nof nearby points are similar both in structure and content. Types in Doug are\ntherefore learnable by a neural network. Following (Chollet, 2019), (Card,\n1983), and (Newell, 1981), we view skill as the application of a procedure, or\nprogram of action, that causes a goal to be satisfied. Skill acquisition may\ntherefore be expressed as program synthesis. Using Doug, we hope to describe a\nform of learning of skilled behaviour that follows a human-like pace of skill\nacquisition (i.e., substantially faster than brute force; Heathcote, 2000),\nexceeding the efficiency of all currently existing approaches (Kaplan, 2020;\nJones, 2021; Chollet, 2024). Our approach brings us one step closer to modeling\nhuman mental representations, as they must actually exist in the brain, and\nthose representations' acquisition, as they are actually learned.",
      "authors": [
        {
          "name": "Eilene Tomkins-Flanagan",
          "affiliation": null
        },
        {
          "name": "Connor Hanley",
          "affiliation": null
        },
        {
          "name": "Mary A. Kelly",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16533v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16533v1",
      "primary_category": "cs.AI",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16530v1",
      "title": "Realizing LLMs' Causal Potential Requires Science-Grounded, Novel Benchmarks",
      "abstract": "Recent claims of strong performance by Large Language Models (LLMs) on causal\ndiscovery are undermined by a key flaw: many evaluations rely on benchmarks\nlikely included in pretraining corpora. Thus, apparent success suggests that\nLLM-only methods, which ignore observational data, outperform classical\nstatistical approaches. We challenge this narrative by asking: Do LLMs truly\nreason about causal structure, and how can we measure it without memorization\nconcerns? Can they be trusted for real-world scientific discovery? We argue\nthat realizing LLMs' potential for causal analysis requires two shifts: (P.1)\ndeveloping robust evaluation protocols based on recent scientific studies to\nguard against dataset leakage, and (P.2) designing hybrid methods that combine\nLLM-derived knowledge with data-driven statistics. To address P.1, we encourage\nevaluating discovery methods on novel, real-world scientific studies. We\noutline a practical recipe for extracting causal graphs from recent\npublications released after an LLM's training cutoff, ensuring relevance and\npreventing memorization while capturing both established and novel relations.\nCompared to benchmarks like BNLearn, where LLMs achieve near-perfect accuracy,\nthey perform far worse on our curated graphs, underscoring the need for\nstatistical grounding. Supporting P.2, we show that using LLM predictions as\npriors for the classical PC algorithm significantly improves accuracy over both\nLLM-only and purely statistical methods. We call on the community to adopt\nscience-grounded, leakage-resistant benchmarks and invest in hybrid causal\ndiscovery methods suited to real-world inquiry.",
      "authors": [
        {
          "name": "Ashutosh Srivastava",
          "affiliation": null
        },
        {
          "name": "Lokesh Nagalapatti",
          "affiliation": null
        },
        {
          "name": "Gautam Jajoo",
          "affiliation": null
        },
        {
          "name": "Aniket Vashishtha",
          "affiliation": null
        },
        {
          "name": "Parameswari Krishnamurthy",
          "affiliation": null
        },
        {
          "name": "Amit Sharma",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16530v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16530v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16518v1",
      "title": "DIV-Nav: Open-Vocabulary Spatial Relationships for Multi-Object Navigation",
      "abstract": "Advances in open-vocabulary semantic mapping and object navigation have\nenabled robots to perform an informed search of their environment for an\narbitrary object. However, such zero-shot object navigation is typically\ndesigned for simple queries with an object name like \"television\" or \"blue\nrug\". Here, we consider more complex free-text queries with spatial\nrelationships, such as \"find the remote on the table\" while still leveraging\nrobustness of a semantic map. We present DIV-Nav, a real-time navigation system\nthat efficiently addresses this problem through a series of relaxations: i)\nDecomposing natural language instructions with complex spatial constraints into\nsimpler object-level queries on a semantic map, ii) computing the Intersection\nof individual semantic belief maps to identify regions where all objects\nco-exist, and iii) Validating the discovered objects against the original,\ncomplex spatial constrains via a LVLM. We further investigate how to adapt the\nfrontier exploration objectives of online semantic mapping to such spatial\nsearch queries to more effectively guide the search process. We validate our\nsystem through extensive experiments on the MultiON benchmark and real-world\ndeployment on a Boston Dynamics Spot robot using a Jetson Orin AGX. More\ndetails and videos are available at https://anonsub42.github.io/reponame/",
      "authors": [
        {
          "name": "Jes\u00fas Ortega-Peimbert",
          "affiliation": null
        },
        {
          "name": "Finn Lukas Busch",
          "affiliation": null
        },
        {
          "name": "Timon Homberger",
          "affiliation": null
        },
        {
          "name": "Quantao Yang",
          "affiliation": null
        },
        {
          "name": "Olov Andersson",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16518v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16518v1",
      "primary_category": "cs.RO",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16514v1",
      "title": "Image Categorization and Search via a GAT Autoencoder and Representative Models",
      "abstract": "We propose a method for image categorization and retrieval that leverages\ngraphs and a graph attention network (GAT)-based autoencoder. Our approach is\nrepresentative-centric, that is, we execute the categorization and retrieval\nprocess via the representative models we construct for the images and image\ncategories. We utilize a graph where nodes represent images (or their\nrepresentatives) and edges capture similarity relationships. GAT highlights\nimportant features and relationships between images, enabling the autoencoder\nto construct context-aware latent representations that capture the key features\nof each image relative to its neighbors. We obtain category representatives\nfrom these embeddings and categorize a query image by comparing its\nrepresentative to the category representatives. We then retrieve the most\nsimilar image to the query image within its identified category. We demonstrate\nthe effectiveness of our representative-centric approach through experiments\nwith both the GAT autoencoders and standard feature-based techniques.",
      "authors": [
        {
          "name": "Duygu Sap",
          "affiliation": null
        },
        {
          "name": "Martin Lotz",
          "affiliation": null
        },
        {
          "name": "Connor Mattinson",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16514v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16514v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16513v1",
      "title": "eDCF: Estimating Intrinsic Dimension using Local Connectivity",
      "abstract": "Modern datasets often contain high-dimensional features exhibiting complex\ndependencies. To effectively analyze such data, dimensionality reduction\nmethods rely on estimating the dataset's intrinsic dimension (id) as a measure\nof its underlying complexity. However, estimating id is challenging due to its\ndependence on scale: at very fine scales, noise inflates id estimates, while at\ncoarser scales, estimates stabilize to lower, scale-invariant values. This\npaper introduces a novel, scalable, and parallelizable method called eDCF,\nwhich is based on Connectivity Factor (CF), a local connectivity-based metric,\nto robustly estimate intrinsic dimension across varying scales. Our method\nconsistently matches leading estimators, achieving comparable values of mean\nabsolute error (MAE) on synthetic benchmarks with noisy samples. Moreover, our\napproach also attains higher exact intrinsic dimension match rates, reaching up\nto 25.0% compared to 16.7% for MLE and 12.5% for TWO-NN, particularly excelling\nunder medium to high noise levels and large datasets. Further, we showcase our\nmethod's ability to accurately detect fractal geometries in decision\nboundaries, confirming its utility for analyzing realistic, structured data.",
      "authors": [
        {
          "name": "Dhruv Gupta",
          "affiliation": null
        },
        {
          "name": "Aditya Nagarsekar",
          "affiliation": null
        },
        {
          "name": "Vraj Shah",
          "affiliation": null
        },
        {
          "name": "Sujith Thomas",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16513v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16513v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16088v1",
      "title": "Differentiable, Bit-shifting, and Scalable Quantization without training neural network from scratch",
      "abstract": "Quantization of neural networks provides benefits of inference in less\ncompute and memory requirements. Previous work in quantization lack two\nimportant aspects which this work provides. First almost all previous work in\nquantization used a non-differentiable approach and for learning; the\nderivative is usually set manually in backpropogation which make the learning\nability of algorithm questionable, our approach is not just differentiable, we\nalso provide proof of convergence of our approach to the optimal neural\nnetwork. Second previous work in shift/logrithmic quantization either have\navoided activation quantization along with weight quantization or achieved less\naccuracy. Learning logrithmic quantize values of form $2^n$ requires the\nquantization function can scale to more than 1 bit quantization which is\nanother benifit of our quantization that it provides $n$ bits quantization as\nwell. Our approach when tested with image classification task using imagenet\ndataset, resnet18 and weight quantization only achieves less than 1 percent\naccuracy compared to full precision accuracy while taking only 15 epochs to\ntrain using shift bit quantization and achieves comparable to SOTA approaches\naccuracy in both weight and activation quantization using shift bit\nquantization in 15 training epochs with slightly higher(only higher cpu\ninstructions) inference cost compared to 1 bit quantization(without logrithmic\nquantization) and not requiring any higher precision multiplication.",
      "authors": [
        {
          "name": "Zia Badar",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "cs.LG",
        "stat.ML"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16088v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16088v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16511v1",
      "title": "Structured Temporal Causality for Interpretable Multivariate Time Series Anomaly Detection",
      "abstract": "Real-world multivariate time series anomalies are rare and often unlabeled.\nAdditionally, prevailing methods rely on increasingly complex architectures\ntuned to benchmarks, detecting only fragments of anomalous segments and\noverstating performance. In this paper, we introduce OracleAD, a simple and\ninterpretable unsupervised framework for multivariate time series anomaly\ndetection. OracleAD encodes each variable's past sequence into a single causal\nembedding to jointly predict the present time point and reconstruct the input\nwindow, effectively modeling temporal dynamics. These embeddings then undergo a\nself-attention mechanism to project them into a shared latent space and capture\nspatial relationships. These relationships are not static, since they are\nmodeled by a property that emerges from each variable's temporal dynamics. The\nprojected embeddings are aligned to a Stable Latent Structure (SLS)\nrepresenting normal-state relationships. Anomalies are identified using a dual\nscoring mechanism based on prediction error and deviation from the SLS,\nenabling fine-grained anomaly diagnosis at each time point and across\nindividual variables. Since any noticeable SLS deviation originates from\nembeddings that violate the learned temporal causality of normal data, OracleAD\ndirectly pinpoints the root-cause variables at the embedding level. OracleAD\nachieves state-of-the-art results across multiple real-world datasets and\nevaluation protocols, while remaining interpretable through SLS.",
      "authors": [
        {
          "name": "Dongchan Cho",
          "affiliation": null
        },
        {
          "name": "Jiho Han",
          "affiliation": null
        },
        {
          "name": "Keumyeong Kang",
          "affiliation": null
        },
        {
          "name": "Minsang Kim",
          "affiliation": null
        },
        {
          "name": "Honggyu Ryu",
          "affiliation": null
        },
        {
          "name": "Namsoon Jung",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16511v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16511v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16509v1",
      "title": "A Bayesian Framework for Symmetry Inference in Chaotic Attractors",
      "abstract": "Detecting symmetry from data is a fundamental problem in signal analysis,\nproviding insight into underlying structure and constraints. When data emerge\nas trajectories of dynamical systems, symmetries encode structural properties\nof the dynamics that enable model reduction, principled comparison across\nconditions, and detection of regime changes. While recent optimal transport\nmethods provide practical tools for data-driven symmetry detection in this\nsetting, they rely on deterministic thresholds and lack uncertainty\nquantification, limiting robustness to noise and ability to resolve\nhierarchical symmetry structures. We present a Bayesian framework that\nformulates symmetry detection as probabilistic model selection over a lattice\nof candidate subgroups, using a Gibbs posterior constructed from Wasserstein\ndistances between observed data and group-transformed copies. We establish\nthree theoretical guarantees: $(i)$ a Bayesian Occam's razor favoring minimal\nsymmetry consistent with data, $(ii)$ conjugation equivariance ensuring\nframe-independence, and $(iii)$ stability bounds under perturbations for\nrobustness to noise. Posterior inference is performed via Metropolis-Hastings\nsampling and numerical experiments on equivariant dynamical systems and\nsynthetic point clouds demonstrate accurate symmetry recovery under high noise\nand small sample sizes. An application to human gait dynamics reveals symmetry\nchanges induced by mechanical constraints, demonstrating the framework's\nutility for statistical inference in biomechanical and dynamical systems.",
      "authors": [
        {
          "name": "Ziad Ghanem",
          "affiliation": null
        },
        {
          "name": "Chang Hyunwoong",
          "affiliation": null
        },
        {
          "name": "Preskella Mrad",
          "affiliation": null
        }
      ],
      "categories": [
        "stat.ML",
        "math.DS"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16509v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16509v1",
      "primary_category": "stat.ML",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16508v1",
      "title": "OOS-DSD: Improving Out-of-stock Detection in Retail Images using Auxiliary Tasks",
      "abstract": "Out-of-stock (OOS) detection is a very important retail verification process\nthat aims to infer the unavailability of products in their designated areas on\nthe shelf. In this paper, we introduce OOS-DSD, a novel deep learning-based\nmethod that advances OOS detection through auxiliary learning. In particular,\nwe extend a well-established YOLOv8 object detection architecture with\nadditional convolutional branches to simultaneously detect OOS, segment\nproducts, and estimate scene depth. While OOS detection and product\nsegmentation branches are trained using ground truth data, the depth estimation\nbranch is trained using pseudo-labeled annotations produced by the\nstate-of-the-art (SOTA) depth estimation model Depth Anything V2. Furthermore,\nsince the aforementioned pseudo-labeled depth estimates display relative depth,\nwe propose an appropriate depth normalization procedure that stabilizes the\ntraining process. The experimental results show that the proposed method\nsurpassed the performance of the SOTA OOS detection methods by 1.8% of the mean\naverage precision (mAP). In addition, ablation studies confirm the\neffectiveness of auxiliary learning and the proposed depth normalization\nprocedure, with the former increasing mAP by 3.7% and the latter by 4.2%.",
      "authors": [
        {
          "name": "Franko \u0160iki\u0107",
          "affiliation": null
        },
        {
          "name": "Sven Lon\u010dari\u0107",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16508v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16508v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16505v1",
      "title": "PRISMM-Bench: A Benchmark of Peer-Review Grounded Multimodal Inconsistencies",
      "abstract": "Large Multimodal Models (LMMs) are increasingly applied to scientific\nresearch, yet it remains unclear whether they can reliably understand and\nreason over the multimodal complexity of papers. A central challenge lies in\ndetecting and resolving inconsistencies across text, figures, tables, and\nequations, issues that are often subtle, domain-specific, and ultimately\nundermine clarity, reproducibility, and trust. Existing benchmarks overlook\nthis issue, either isolating single modalities or relying on synthetic errors\nthat fail to capture real-world complexity. We introduce PRISMM-Bench\n(Peer-Review-sourced Inconsistency Set for Multimodal Models), the first\nbenchmark grounded in real reviewer-flagged inconsistencies in scientific\npapers. Through a multi-stage pipeline of review mining, LLM-assisted filtering\nand human verification, we curate 262 inconsistencies from 242 papers. Based on\nthis set, we design three tasks, namely inconsistency identification, remedy\nand pair matching, which assess a model's capacity to detect, correct, and\nreason over inconsistencies across different modalities. Furthermore, to\naddress the notorious problem of choice-only shortcuts in multiple-choice\nevaluation, where models exploit answer patterns without truly understanding\nthe question, we further introduce structured JSON-based answer representations\nthat minimize linguistic biases by reducing reliance on superficial stylistic\ncues. We benchmark 21 leading LMMs, including large open-weight models\n(GLM-4.5V 106B, InternVL3 78B) and proprietary models (Gemini 2.5 Pro, GPT-5\nwith high reasoning). Results reveal strikingly low performance (26.1-54.2%),\nunderscoring the challenge of multimodal scientific reasoning and motivating\nprogress towards trustworthy scientific assistants.",
      "authors": [
        {
          "name": "Lukas Selch",
          "affiliation": null
        },
        {
          "name": "Yufang Hou",
          "affiliation": null
        },
        {
          "name": "M. Jehanzeb Mirza",
          "affiliation": null
        },
        {
          "name": "Sivan Doveh",
          "affiliation": null
        },
        {
          "name": "James Glass",
          "affiliation": null
        },
        {
          "name": "Rogerio Feris",
          "affiliation": null
        },
        {
          "name": "Wei Lin",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16505v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16505v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16499v1",
      "title": "Automated Composition of Agents: A Knapsack Approach for Agentic Component Selection",
      "abstract": "Designing effective agentic systems requires the seamless composition and\nintegration of agents, tools, and models within dynamic and uncertain\nenvironments. Most existing methods rely on static, semantic retrieval\napproaches for tool or agent discovery. However, effective reuse and\ncomposition of existing components remain challenging due to incomplete\ncapability descriptions and the limitations of retrieval methods. Component\nselection suffers because the decisions are not based on capability, cost, and\nreal-time utility. To address these challenges, we introduce a structured,\nautomated framework for agentic system composition that is inspired by the\nknapsack problem. Our framework enables a composer agent to systematically\nidentify, select, and assemble an optimal set of agentic components by jointly\nconsidering performance, budget constraints, and compatibility. By dynamically\ntesting candidate components and modeling their utility in real-time, our\napproach streamlines the assembly of agentic systems and facilitates scalable\nreuse of resources. Empirical evaluation with Claude 3.5 Sonnet across five\nbenchmarking datasets shows that our online-knapsack-based composer\nconsistently lies on the Pareto frontier, achieving higher success rates at\nsignificantly lower component costs compared to our baselines. In the\nsingle-agent setup, the online knapsack composer shows a success rate\nimprovement of up to 31.6% in comparison to the retrieval baselines. In\nmulti-agent systems, the online knapsack composer increases success rate from\n37% to 87% when agents are selected from an agent inventory of 100+ agents. The\nsubstantial performance gap confirms the robust adaptability of our method\nacross diverse domains and budget constraints.",
      "authors": [
        {
          "name": "Michelle Yuan",
          "affiliation": null
        },
        {
          "name": "Khushbu Pahwa",
          "affiliation": null
        },
        {
          "name": "Shuaichen Chang",
          "affiliation": null
        },
        {
          "name": "Mustafa Kaba",
          "affiliation": null
        },
        {
          "name": "Jiarong Jiang",
          "affiliation": null
        },
        {
          "name": "Xiaofei Ma",
          "affiliation": null
        },
        {
          "name": "Yi Zhang",
          "affiliation": null
        },
        {
          "name": "Monica Sunkara",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16499v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16499v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16497v1",
      "title": "Edge-Based Speech Transcription and Synthesis for Kinyarwanda and Swahili Languages",
      "abstract": "This paper presents a novel framework for speech transcription and synthesis,\nleveraging edge-cloud parallelism to enhance processing speed and accessibility\nfor Kinyarwanda and Swahili speakers. It addresses the scarcity of powerful\nlanguage processing tools for these widely spoken languages in East African\ncountries with limited technological infrastructure. The framework utilizes the\nWhisper and SpeechT5 pre-trained models to enable speech-to-text (STT) and\ntext-to-speech (TTS) translation. The architecture uses a cascading mechanism\nthat distributes the model inference workload between the edge device and the\ncloud, thereby reducing latency and resource usage, benefiting both ends. On\nthe edge device, our approach achieves a memory usage compression of 9.5% for\nthe SpeechT5 model and 14% for the Whisper model, with a maximum memory usage\nof 149 MB. Experimental results indicate that on a 1.7 GHz CPU edge device with\na 1 MB/s network bandwidth, the system can process a 270-character text in less\nthan a minute for both speech-to-text and text-to-speech transcription. Using\nreal-world survey data from Kenya, it is shown that the cascaded edge-cloud\narchitecture proposed could easily serve as an excellent platform for STT and\nTTS transcription with good accuracy and response time.",
      "authors": [
        {
          "name": "Pacome Simon Mbonimpa",
          "affiliation": null
        },
        {
          "name": "Diane Tuyizere",
          "affiliation": null
        },
        {
          "name": "Azizuddin Ahmed Biyabani",
          "affiliation": null
        },
        {
          "name": "Ozan K. Tonguz",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.DC",
        "cs.LG"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16497v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16497v1",
      "primary_category": "cs.DC",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16492v1",
      "title": "Check Yourself Before You Wreck Yourself: Selectively Quitting Improves LLM Agent Safety",
      "abstract": "As Large Language Model (LLM) agents increasingly operate in complex\nenvironments with real-world consequences, their safety becomes critical. While\nuncertainty quantification is well-studied for single-turn tasks, multi-turn\nagentic scenarios with real-world tool access present unique challenges where\nuncertainties and ambiguities compound, leading to severe or catastrophic risks\nbeyond traditional text generation failures. We propose using \"quitting\" as a\nsimple yet effective behavioral mechanism for LLM agents to recognize and\nwithdraw from situations where they lack confidence. Leveraging the ToolEmu\nframework, we conduct a systematic evaluation of quitting behavior across 12\nstate-of-the-art LLMs. Our results demonstrate a highly favorable\nsafety-helpfulness trade-off: agents prompted to quit with explicit\ninstructions improve safety by an average of +0.39 on a 0-3 scale across all\nmodels (+0.64 for proprietary models), while maintaining a negligible average\ndecrease of -0.03 in helpfulness. Our analysis demonstrates that simply adding\nexplicit quit instructions proves to be a highly effective safety mechanism\nthat can immediately be deployed in existing agent systems, and establishes\nquitting as an effective first-line defense mechanism for autonomous agents in\nhigh-stakes applications.",
      "authors": [
        {
          "name": "Vamshi Krishna Bonagiri",
          "affiliation": null
        },
        {
          "name": "Ponnurangam Kumaragurum",
          "affiliation": null
        },
        {
          "name": "Khanh Nguyen",
          "affiliation": null
        },
        {
          "name": "Benjamin Plaut",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16492v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16492v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16476v1",
      "title": "NP-Engine: Empowering Optimization Reasoning in Large Language Models with Verifiable Synthetic NP Problems",
      "abstract": "Large Language Models (LLMs) have shown strong reasoning capabilities, with\nmodels like OpenAI's O-series and DeepSeek R1 excelling at tasks such as\nmathematics, coding, logic, and puzzles through Reinforcement Learning with\nVerifiable Rewards (RLVR). However, their ability to solve more complex\noptimization problems - particularly NP-hard tasks - remains underexplored. To\nbridge this gap, we propose NP-ENGINE, the first comprehensive framework for\ntraining and evaluating LLMs on NP-hard problems. NP-ENGINE covers 10 tasks\nacross five domains, each equipped with (i) a controllable instance generator,\n(ii) a rule-based verifier, and (iii) a heuristic solver that provides\napproximate optimal solutions as ground truth. This\ngenerator-verifier-heuristic pipeline enables scalable and verifiable RLVR\ntraining under hierarchical difficulties. We also introduce NP-BENCH, a\nbenchmark derived from NP-ENGINE-DATA, specifically designed to evaluate LLMs'\nability to tackle NP-hard level reasoning problems, focusing not only on\nfeasibility but also on solution quality. Additionally, we present\nQWEN2.5-7B-NP, a model trained via zero-RLVR with curriculum learning on\nQwen2.5-7B-Instruct, which significantly outperforms GPT-4o on NP-BENCH and\nachieves SOTA performance with the same model size. Beyond in-domain tasks, we\ndemonstrate that RLVR training on NP-ENGINE-DATA enables strong out-of-domain\n(OOD) generalization to reasoning tasks (logic, puzzles, math, and knowledge),\nas well as non-reasoning tasks such as instruction following. We also observe a\nscaling trend: increasing task diversity improves OOD generalization. These\nfindings suggest that task-rich RLVR training is a promising direction for\nadvancing LLM's reasoning ability, revealing new insights into the scaling laws\nof RLVR.",
      "authors": [
        {
          "name": "Xiaozhe Li",
          "affiliation": null
        },
        {
          "name": "Xinyu Fang",
          "affiliation": null
        },
        {
          "name": "Shengyuan Ding",
          "affiliation": null
        },
        {
          "name": "Linyang Li",
          "affiliation": null
        },
        {
          "name": "Haodong Duan",
          "affiliation": null
        },
        {
          "name": "Qingwen Liu",
          "affiliation": null
        },
        {
          "name": "Kai Chen",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.AI"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16476v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16476v1",
      "primary_category": "cs.AI",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16474v1",
      "title": "SCALAR: Self-Calibrating Adaptive Latent Attention Representation Learning",
      "abstract": "High-dimensional, heterogeneous data with complex feature interactions pose\nsignificant challenges for traditional predictive modeling approaches. While\nProjection to Latent Structures (PLS) remains a popular technique, it struggles\nto model complex non-linear relationships, especially in multivariate systems\nwith high-dimensional correlation structures. This challenge is further\ncompounded by simultaneous interactions across multiple scales, where local\nprocessing fails to capture crossgroup dependencies. Additionally, static\nfeature weighting limits adaptability to contextual variations, as it ignores\nsample-specific relevance. To address these limitations, we propose a novel\nmethod that enhances predictive performance through novel architectural\ninnovations. Our architecture introduces an adaptive kernel-based attention\nmechanism that processes distinct feature groups separately before integration,\nenabling capture of local patterns while preserving global relationships.\nExperimental results show substantial improvements in performance metrics,\ncompared to the state-of-the-art methods across diverse datasets.",
      "authors": [
        {
          "name": "Farwa Abbas",
          "affiliation": null
        },
        {
          "name": "Hussain Ahmad",
          "affiliation": null
        },
        {
          "name": "Claudia Szabo",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16474v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16474v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16470v1",
      "title": "Declarative Techniques for NL Queries over Heterogeneous Data",
      "abstract": "In many industrial settings, users wish to ask questions in natural language,\nthe answers to which require assembling information from diverse structured\ndata sources. With the advent of Large Language Models (LLMs), applications can\nnow translate natural language questions into a set of API calls or database\ncalls, execute them, and combine the results into an appropriate natural\nlanguage response. However, these applications remain impractical in realistic\nindustrial settings because they do not cope with the data source heterogeneity\nthat typifies such environments. In this work, we simulate the heterogeneity of\nreal industry settings by introducing two extensions of the popular Spider\nbenchmark dataset that require a combination of database and API calls. Then,\nwe introduce a declarative approach to handling such data heterogeneity and\ndemonstrate that it copes with data source heterogeneity significantly better\nthan state-of-the-art LLM-based agentic or imperative code generation systems.\nOur augmented benchmarks are available to the research community.",
      "authors": [
        {
          "name": "Elham Khabiri",
          "affiliation": null
        },
        {
          "name": "Jeffrey O. Kephart",
          "affiliation": null
        },
        {
          "name": "Fenno F. Heath III",
          "affiliation": null
        },
        {
          "name": "Srideepika Jayaraman",
          "affiliation": null
        },
        {
          "name": "Fateh A. Tipu",
          "affiliation": null
        },
        {
          "name": "Yingjie Li",
          "affiliation": null
        },
        {
          "name": "Dhruv Shah",
          "affiliation": null
        },
        {
          "name": "Achille Fokoue",
          "affiliation": null
        },
        {
          "name": "Anu Bhamidipaty",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.DB",
        "cs.AI",
        "cs.SE"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16470v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16470v1",
      "primary_category": "cs.DB",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16466v1",
      "title": "ReviewSense: Transforming Customer Review Dynamics into Actionable Business Insights",
      "abstract": "As customer feedback becomes increasingly central to strategic growth, the\nability to derive actionable insights from unstructured reviews is essential.\nWhile traditional AI-driven systems excel at predicting user preferences, far\nless work has focused on transforming customer reviews into prescriptive,\nbusiness-facing recommendations. This paper introduces ReviewSense, a novel\nprescriptive decision support framework that leverages advanced large language\nmodels (LLMs) to transform customer reviews into targeted, actionable business\nrecommendations. By identifying key trends, recurring issues, and specific\nconcerns within customer sentiments, ReviewSense extends beyond\npreference-based systems to provide businesses with deeper insights for\nsustaining growth and enhancing customer loyalty. The novelty of this work lies\nin integrating clustering, LLM adaptation, and expert-driven evaluation into a\nunified, business-facing pipeline. Preliminary manual evaluations indicate\nstrong alignment between the model's recommendations and business objectives,\nhighlighting its potential for driving data-informed decision-making. This\nframework offers a new perspective on AI-driven sentiment analysis,\ndemonstrating its value in refining business strategies and maximizing the\nimpact of customer feedback.",
      "authors": [
        {
          "name": "Siddhartha Krothapalli",
          "affiliation": null
        },
        {
          "name": "Tridib Kumar Das",
          "affiliation": null
        },
        {
          "name": "Praveen Kumar",
          "affiliation": null
        },
        {
          "name": "Naveen Suravarpu",
          "affiliation": null
        },
        {
          "name": "Pratik Narang",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.AI"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16466v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16466v1",
      "primary_category": "cs.AI",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16463v1",
      "title": "HGC-Avatar: Hierarchical Gaussian Compression for Streamable Dynamic 3D Avatars",
      "abstract": "Recent advances in 3D Gaussian Splatting (3DGS) have enabled fast,\nphotorealistic rendering of dynamic 3D scenes, showing strong potential in\nimmersive communication. However, in digital human encoding and transmission,\nthe compression methods based on general 3DGS representations are limited by\nthe lack of human priors, resulting in suboptimal bitrate efficiency and\nreconstruction quality at the decoder side, which hinders their application in\nstreamable 3D avatar systems. We propose HGC-Avatar, a novel Hierarchical\nGaussian Compression framework designed for efficient transmission and\nhigh-quality rendering of dynamic avatars. Our method disentangles the Gaussian\nrepresentation into a structural layer, which maps poses to Gaussians via a\nStyleUNet-based generator, and a motion layer, which leverages the SMPL-X model\nto represent temporal pose variations compactly and semantically. This\nhierarchical design supports layer-wise compression, progressive decoding, and\ncontrollable rendering from diverse pose inputs such as video sequences or\ntext. Since people are most concerned with facial realism, we incorporate a\nfacial attention mechanism during StyleUNet training to preserve identity and\nexpression details under low-bitrate constraints. Experimental results\ndemonstrate that HGC-Avatar provides a streamable solution for rapid 3D avatar\nrendering, while significantly outperforming prior methods in both visual\nquality and compression efficiency.",
      "authors": [
        {
          "name": "Haocheng Tang",
          "affiliation": null
        },
        {
          "name": "Ruoke Yan",
          "affiliation": null
        },
        {
          "name": "Xinhui Yin",
          "affiliation": null
        },
        {
          "name": "Qi Zhang",
          "affiliation": null
        },
        {
          "name": "Xinfeng Zhang",
          "affiliation": null
        },
        {
          "name": "Siwei Ma",
          "affiliation": null
        },
        {
          "name": "Wen Gao",
          "affiliation": null
        },
        {
          "name": "Chuanmin Jia",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16463v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16463v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16462v1",
      "title": "Buzz, Choose, Forget: A Meta-Bandit Framework for Bee-Like Decision Making",
      "abstract": "We introduce a sequential reinforcement learning framework for imitation\nlearning designed to model heterogeneous cognitive strategies in pollinators.\nFocusing on honeybees, our approach leverages trajectory similarity to capture\nand forecast behavior across individuals that rely on distinct strategies: some\nexploiting numerical cues, others drawing on memory, or being influenced by\nenvironmental factors such as weather. Through empirical evaluation, we show\nthat state-of-the-art imitation learning methods often fail in this setting:\nwhen expert policies shift across memory windows or deviate from optimality,\nthese models overlook both fast and slow learning behaviors and cannot\nfaithfully reproduce key decision patterns. Moreover, they offer limited\ninterpretability, hindering biological insight. Our contribution addresses\nthese challenges by (i) introducing a model that minimizes predictive loss\nwhile identifying the effective memory horizon most consistent with behavioral\ndata, and (ii) ensuring full interpretability to enable biologists to analyze\nunderlying decision-making strategies and finally (iii) providing a\nmathematical framework linking bee policy search with bandit formulations under\nvarying exploration-exploitation dynamics, and releasing a novel dataset of 80\ntracked bees observed under diverse weather conditions. This benchmark\nfacilitates research on pollinator cognition and supports ecological governance\nby improving simulations of insect behavior in agroecosystems. Our findings\nshed new light on the learning strategies and memory interplay shaping\npollinator decision-making.",
      "authors": [
        {
          "name": "Emmanuelle Claeys",
          "affiliation": null
        },
        {
          "name": "Elena Kerjean",
          "affiliation": null
        },
        {
          "name": "Jean-Michel Loubes",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16462v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16462v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16458v1",
      "title": "Agree, Disagree, Explain: Decomposing Human Label Variation in NLI through the Lens of Explanations",
      "abstract": "Natural Language Inference datasets often exhibit human label variation. To\nbetter understand these variations, explanation-based approaches analyze the\nunderlying reasoning behind annotators' decisions. One such approach is the\nLiTEx taxonomy, which categorizes free-text explanations in English into\nreasoning types. However, previous work applying such taxonomies has focused on\nwithin-label variation: cases where annotators agree on the final NLI label but\nprovide different explanations. In contrast, this paper broadens the scope by\nexamining how annotators may diverge not only in the reasoning type but also in\nthe labeling step. We use explanations as a lens to decompose the reasoning\nprocess underlying NLI annotation and to analyze individual differences. We\napply LiTEx to two NLI English datasets and align annotation variation from\nmultiple aspects: NLI label agreement, explanation similarity, and taxonomy\nagreement, with an additional compounding factor of annotators' selection bias.\nWe observe instances where annotators disagree on the label but provide highly\nsimilar explanations, suggesting that surface-level disagreement may mask\nunderlying agreement in interpretation. Moreover, our analysis reveals\nindividual preferences in explanation strategies and label choices. These\nfindings highlight that agreement in reasoning types better reflects the\nsemantic similarity of free-text explanations than label agreement alone. Our\nfindings underscore the richness of reasoning-based explanations and the need\nfor caution in treating labels as ground truth.",
      "authors": [
        {
          "name": "Pingjun Hong",
          "affiliation": null
        },
        {
          "name": "Beiduo Chen",
          "affiliation": null
        },
        {
          "name": "Siyao Peng",
          "affiliation": null
        },
        {
          "name": "Marie-Catherine de Marneffe",
          "affiliation": null
        },
        {
          "name": "Benjamin Roth",
          "affiliation": null
        },
        {
          "name": "Barbara Plank",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16458v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16458v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16457v1",
      "title": "NavQ: Learning a Q-Model for Foresighted Vision-and-Language Navigation",
      "abstract": "In this work we concentrate on the task of goal-oriented Vision-and-Language\nNavigation (VLN). Existing methods often make decisions based on historical\ninformation, overlooking the future implications and long-term outcomes of the\nactions. In contrast, we aim to develop a foresighted agent. Specifically, we\ndraw upon Q-learning to train a Q-model using large-scale unlabeled trajectory\ndata, in order to learn the general knowledge regarding the layout and object\nrelations within indoor scenes. This model can generate a Q-feature, analogous\nto the Q-value in traditional Q-network, for each candidate action, which\ndescribes the potential future information that may be observed after taking\nthe specific action. Subsequently, a cross-modal future encoder integrates the\ntask-agnostic Q-feature with navigation instructions to produce a set of action\nscores reflecting future prospects. These scores, when combined with the\noriginal scores based on history, facilitate an A*-style searching strategy to\neffectively explore the regions that are more likely to lead to the\ndestination. Extensive experiments conducted on widely used goal-oriented VLN\ndatasets validate the effectiveness of the proposed method.",
      "authors": [
        {
          "name": "Peiran Xu",
          "affiliation": null
        },
        {
          "name": "Xicheng Gong",
          "affiliation": null
        },
        {
          "name": "Yadong MU",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16457v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16457v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16455v1",
      "title": "RAVEN: Robust Advertisement Video Violation Temporal Grounding via Reinforcement Reasoning",
      "abstract": "Advertisement (Ad) video violation detection is critical for ensuring\nplatform compliance, but existing methods struggle with precise temporal\ngrounding, noisy annotations, and limited generalization. We propose RAVEN, a\nnovel framework that integrates curriculum reinforcement learning with\nmultimodal large language models (MLLMs) to enhance reasoning and cognitive\ncapabilities for violation detection. RAVEN employs a progressive training\nstrategy, combining precisely and coarsely annotated data, and leverages Group\nRelative Policy Optimization (GRPO) to develop emergent reasoning abilities\nwithout explicit reasoning annotations. Multiple hierarchical sophisticated\nreward mechanism ensures precise temporal grounding and consistent category\nprediction. Experiments on industrial datasets and public benchmarks show that\nRAVEN achieves superior performances in violation category accuracy and\ntemporal interval localization. We also design a pipeline to deploy the RAVEN\non the online Ad services, and online A/B testing further validates its\npractical applicability, with significant improvements in precision and recall.\nRAVEN also demonstrates strong generalization, mitigating the catastrophic\nforgetting issue associated with supervised fine-tuning.",
      "authors": [
        {
          "name": "Deyi Ji",
          "affiliation": null
        },
        {
          "name": "Yuekui Yang",
          "affiliation": null
        },
        {
          "name": "Haiyang Wu",
          "affiliation": null
        },
        {
          "name": "Shaoping Ma",
          "affiliation": null
        },
        {
          "name": "Tianrun Chen",
          "affiliation": null
        },
        {
          "name": "Lanyun Zhu",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16455v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16455v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16450v1",
      "title": "Instance-Aware Pseudo-Labeling and Class-Focused Contrastive Learning for Weakly Supervised Domain Adaptive Segmentation of Electron Microscopy",
      "abstract": "Annotation-efficient segmentation of the numerous mitochondria instances from\nvarious electron microscopy (EM) images is highly valuable for biological and\nneuroscience research. Although unsupervised domain adaptation (UDA) methods\ncan help mitigate domain shifts and reduce the high costs of annotating each\ndomain, they typically have relatively low performance in practical\napplications. Thus, we investigate weakly supervised domain adaptation (WDA)\nthat utilizes additional sparse point labels on the target domain, which\nrequire minimal annotation effort and minimal expert knowledge. To take full\nuse of the incomplete and imprecise point annotations, we introduce a multitask\nlearning framework that jointly conducts segmentation and center detection with\na novel cross-teaching mechanism and class-focused cross-domain contrastive\nlearning. While leveraging unlabeled image regions is essential, we introduce\nsegmentation self-training with a novel instance-aware pseudo-label (IPL)\nselection strategy. Unlike existing methods that typically rely on pixel-wise\npseudo-label filtering, the IPL semantically selects reliable and diverse\npseudo-labels with the help of the detection task. Comprehensive validations\nand comparisons on challenging datasets demonstrate that our method outperforms\nexisting UDA and WDA methods, significantly narrowing the performance gap with\nthe supervised upper bound. Furthermore, under the UDA setting, our method also\nachieves substantial improvements over other UDA techniques.",
      "authors": [
        {
          "name": "Shan Xiong",
          "affiliation": null
        },
        {
          "name": "Jiabao Chen",
          "affiliation": null
        },
        {
          "name": "Ye Wang",
          "affiliation": null
        },
        {
          "name": "Jialin Peng",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16450v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16450v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16449v1",
      "title": "TrajSelector: Harnessing Latent Representations for Efficient and Effective Best-of-N in Large Reasoning Model",
      "abstract": "Large language models (LLMs) have shown remarkable progress in complex\nreasoning tasks, largely enabled by test-time scaling (TTS) paradigms that\nallocate additional compute during inference. Among these, external TTS\n(particularly the Best-of-N selection paradigm) yields scalable performance\nimprovements by selecting from multiple independently generated reasoning\ntrajectories. However, this approach faces key limitations: (i) the high\ncomputational overhead of deploying process reward models, (ii) the\nunderutilization of the LLM's intrinsic latent representations. We introduce\nTrajSelector, an efficient and effective Best-of-N framework that exploit the\nhidden states in the sampler LLM for process-level scoring. A lightweight\nverifier (with only 0.6B parameters) evaluates the quality of step-wise\ntrajectory, and then aggregates these scores to identify the optimal reasoning\ntrajectory. Our framework employs a fully data-driven, end-to-end training\nrecipe that eliminates reliance on massive step-level annotations. Experiential\nresults across five benchmarks demonstrate that TrajSelector delivers\nconsistent performance gains. In Best-of-32 settings, it surpasses majority\nvoting by 4.61% accuracy and outperforms existing process reward models by\n4.31% to 12.21%, all while maintaining lower inference costs.",
      "authors": [
        {
          "name": "Bin Yu",
          "affiliation": null
        },
        {
          "name": "Xinming Wang",
          "affiliation": null
        },
        {
          "name": "Shijie Lian",
          "affiliation": null
        },
        {
          "name": "Haotian Li",
          "affiliation": null
        },
        {
          "name": "Changti Wu",
          "affiliation": null
        },
        {
          "name": "Ruina Hu",
          "affiliation": null
        },
        {
          "name": "Bailing Wang",
          "affiliation": null
        },
        {
          "name": "Yuliang Wei",
          "affiliation": null
        },
        {
          "name": "Kai Chen",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16449v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16449v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16448v1",
      "title": "Input Domain Aware MoE: Decoupling Routing Decisions from Task Optimization in Mixture of Experts",
      "abstract": "Sparse Mixture of Experts (sMoE) has become a pivotal approach for scaling\nlarge vision-language models, offering substantial capacity while maintaining\ncomputational efficiency through dynamic, sparse activation of experts.\nHowever, existing routing mechanisms, typically based on similarity scoring,\nstruggle to effectively capture the underlying input structure. This limitation\nleads to a trade-off between expert specialization and balanced computation,\nhindering both scalability and performance. We propose Input Domain Aware MoE,\na novel routing framework that leverages a probabilistic mixture model to\nbetter partition the input space. By modeling routing probabilities as a\nmixture of distributions, our method enables experts to develop clear\nspecialization boundaries while achieving balanced utilization. Unlike\nconventional approaches, our routing mechanism is trained independently of\ntask-specific objectives, allowing for stable optimization and decisive expert\nassignments. Empirical results on vision-language tasks demonstrate that our\nmethod consistently outperforms existing sMoE approaches, achieving higher task\nperformance and improved expert utilization balance.",
      "authors": [
        {
          "name": "Yongxiang Hua",
          "affiliation": null
        },
        {
          "name": "Haoyu Cao",
          "affiliation": null
        },
        {
          "name": "Zhou Tao",
          "affiliation": null
        },
        {
          "name": "Bocheng Li",
          "affiliation": null
        },
        {
          "name": "Zihao Wu",
          "affiliation": null
        },
        {
          "name": "Chaohu Liu",
          "affiliation": null
        },
        {
          "name": "Linli Xu",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16448v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16448v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16446v1",
      "title": "VIPAMIN: Visual Prompt Initialization via Embedding Selection and Subspace Expansion",
      "abstract": "In the era of large-scale foundation models, fully fine-tuning pretrained\nnetworks for each downstream task is often prohibitively resource-intensive.\nPrompt tuning offers a lightweight alternative by introducing tunable prompts\nwhile keeping the backbone frozen. However, existing visual prompt tuning\nmethods often fail to specialize the prompts or enrich the representation\nspace--especially when applied to self-supervised backbones. We show that these\nlimitations become especially pronounced in challenging tasks and data-scarce\nsettings, where effective adaptation is most critical. In this work, we\nintroduce VIPAMIN, a visual prompt initialization strategy that enhances\nadaptation of self-supervised models by (1) aligning prompts with semantically\ninformative regions in the embedding space, and (2) injecting novel\nrepresentational directions beyond the pretrained subspace. Despite its\nsimplicity--requiring only a single forward pass and lightweight\noperations--VIPAMIN consistently improves performance across diverse tasks and\ndataset sizes, setting a new state of the art in visual prompt tuning. Our code\nis available at https://github.com/iamjaekyun/vipamin.",
      "authors": [
        {
          "name": "Jaekyun Park",
          "affiliation": null
        },
        {
          "name": "Hye Won Chung",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16446v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16446v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16445v1",
      "title": "Enhancing Rotated Object Detection via Anisotropic Gaussian Bounding Box and Bhattacharyya Distance",
      "abstract": "Detecting rotated objects accurately and efficiently is a significant\nchallenge in computer vision, particularly in applications such as aerial\nimagery, remote sensing, and autonomous driving. Although traditional object\ndetection frameworks are effective for axis-aligned objects, they often\nunderperform in scenarios involving rotated objects due to their limitations in\ncapturing orientation variations. This paper introduces an improved loss\nfunction aimed at enhancing detection accuracy and robustness by leveraging the\nGaussian bounding box representation and Bhattacharyya distance. In addition,\nwe advocate for the use of an anisotropic Gaussian representation to address\nthe issues associated with isotropic variance in square-like objects. Our\nproposed method addresses these challenges by incorporating a\nrotation-invariant loss function that effectively captures the geometric\nproperties of rotated objects. We integrate this proposed loss function into\nstate-of-the-art deep learning-based rotated object detection detectors, and\nextensive experiments demonstrated significant improvements in mean Average\nPrecision metrics compared to existing methods. The results highlight the\npotential of our approach to establish new benchmark in rotated object\ndetection, with implications for a wide range of applications requiring precise\nand reliable object localization irrespective of orientation.",
      "authors": [
        {
          "name": "Chien Thai",
          "affiliation": null
        },
        {
          "name": "Mai Xuan Trang",
          "affiliation": null
        },
        {
          "name": "Huong Ninh",
          "affiliation": null
        },
        {
          "name": "Hoang Hiep Ly",
          "affiliation": null
        },
        {
          "name": "Anh Son Le",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16445v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16445v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16444v1",
      "title": "RefAtomNet++: Advancing Referring Atomic Video Action Recognition using Semantic Retrieval based Multi-Trajectory Mamba",
      "abstract": "Referring Atomic Video Action Recognition (RAVAR) aims to recognize\nfine-grained, atomic-level actions of a specific person of interest conditioned\non natural language descriptions. Distinct from conventional action recognition\nand detection tasks, RAVAR emphasizes precise language-guided action\nunderstanding, which is particularly critical for interactive human action\nanalysis in complex multi-person scenarios. In this work, we extend our\npreviously introduced RefAVA dataset to RefAVA++, which comprises >2.9 million\nframes and >75.1k annotated persons in total. We benchmark this dataset using\nbaselines from multiple related domains, including atomic action localization,\nvideo question answering, and text-video retrieval, as well as our earlier\nmodel, RefAtomNet. Although RefAtomNet surpasses other baselines by\nincorporating agent attention to highlight salient features, its ability to\nalign and retrieve cross-modal information remains limited, leading to\nsuboptimal performance in localizing the target person and predicting\nfine-grained actions. To overcome the aforementioned limitations, we introduce\nRefAtomNet++, a novel framework that advances cross-modal token aggregation\nthrough a multi-hierarchical semantic-aligned cross-attention mechanism\ncombined with multi-trajectory Mamba modeling at the partial-keyword,\nscene-attribute, and holistic-sentence levels. In particular, scanning\ntrajectories are constructed by dynamically selecting the nearest visual\nspatial tokens at each timestep for both partial-keyword and scene-attribute\nlevels. Moreover, we design a multi-hierarchical semantic-aligned\ncross-attention strategy, enabling more effective aggregation of spatial and\ntemporal tokens across different semantic hierarchies. Experiments show that\nRefAtomNet++ establishes new state-of-the-art results. The dataset and code are\nreleased at https://github.com/KPeng9510/refAVA2.",
      "authors": [
        {
          "name": "Kunyu Peng",
          "affiliation": null
        },
        {
          "name": "Di Wen",
          "affiliation": null
        },
        {
          "name": "Jia Fu",
          "affiliation": null
        },
        {
          "name": "Jiamin Wu",
          "affiliation": null
        },
        {
          "name": "Kailun Yang",
          "affiliation": null
        },
        {
          "name": "Junwei Zheng",
          "affiliation": null
        },
        {
          "name": "Ruiping Liu",
          "affiliation": null
        },
        {
          "name": "Yufan Chen",
          "affiliation": null
        },
        {
          "name": "Yuqian Fu",
          "affiliation": null
        },
        {
          "name": "Danda Pani Paudel",
          "affiliation": null
        },
        {
          "name": "Luc Van Gool",
          "affiliation": null
        },
        {
          "name": "Rainer Stiefelhagen",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "cs.MM",
        "cs.RO",
        "eess.IV"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16444v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16444v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16443v1",
      "title": "Colliding with Adversaries at ECML-PKDD 2025 Model Robustness Competition 1st Prize Solution",
      "abstract": "This report presents the winning solution for Task 2 of Colliding with\nAdversaries: A Challenge on Robust Learning in High Energy Physics Discovery at\nECML-PKDD 2025. The goal of the challenge was to design and train a robust\nANN-based model capable of achieving high accuracy in a binary classification\ntask on both clean and adversarial data generated with the Random Distribution\nShuffle Attack (RDSA). Our solution consists of two components: a data\ngeneration phase and a robust model training phase. In the first phase, we\nproduced 15 million artificial training samples using a custom methodology\nderived from Random Distribution Shuffle Attack (RDSA). In the second phase, we\nintroduced a robust architecture comprising (i)a Feature Embedding Block with\nshared weights among features of the same type and (ii)a Dense Fusion Tail\nresponsible for the final prediction. Training this architecture on our\nadversarial dataset achieved a mixed accuracy score of 80\\%, exceeding the\nsecond-place solution by two percentage points.",
      "authors": [
        {
          "name": "Dimitris Stefanopoulos",
          "affiliation": null
        },
        {
          "name": "Andreas Voskou",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16443v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16443v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16442v1",
      "title": "EDVD-LLaMA: Explainable Deepfake Video Detection via Multimodal Large Language Model Reasoning",
      "abstract": "The rapid development of deepfake video technology has not only facilitated\nartistic creation but also made it easier to spread misinformation. Traditional\ndeepfake video detection (DVD) methods face issues such as a lack of\ntransparency in their principles and insufficient generalization capabilities\nto cope with evolving forgery techniques. This highlights an urgent need for\ndetectors that can identify forged content and provide verifiable reasoning\nexplanations. This paper proposes the explainable deepfake video detection\n(EDVD) task and designs the EDVD-LLaMA multimodal, a large language model\n(MLLM) reasoning framework, which provides traceable reasoning processes\nalongside accurate detection results and trustworthy explanations. Our approach\nfirst incorporates a Spatio-Temporal Subtle Information Tokenization (ST-SIT)\nto extract and fuse global and local cross-frame deepfake features, providing\nrich spatio-temporal semantic information input for MLLM reasoning. Second, we\nconstruct a Fine-grained Multimodal Chain-of-Thought (Fg-MCoT) mechanism, which\nintroduces facial feature data as hard constraints during the reasoning process\nto achieve pixel-level spatio-temporal video localization, suppress\nhallucinated outputs, and enhance the reliability of the chain of thought. In\naddition, we build an Explainable Reasoning FF++ benchmark dataset\n(ER-FF++set), leveraging structured data to annotate videos and ensure quality\ncontrol, thereby supporting dual supervision for reasoning and detection.\nExtensive experiments demonstrate that EDVD-LLaMA achieves outstanding\nperformance and robustness in terms of detection accuracy, explainability, and\nits ability to handle cross-forgery methods and cross-dataset scenarios.\nCompared to previous DVD methods, it provides a more explainable and superior\nsolution. The source code and dataset will be publicly available.",
      "authors": [
        {
          "name": "Haoran Sun",
          "affiliation": null
        },
        {
          "name": "Chen Cai",
          "affiliation": null
        },
        {
          "name": "Huiping Zhuang",
          "affiliation": null
        },
        {
          "name": "Kong Aik Lee",
          "affiliation": null
        },
        {
          "name": "Lap-Pui Chau",
          "affiliation": null
        },
        {
          "name": "Yi Wang",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16442v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16442v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16440v1",
      "title": "Colliding with Adversaries at ECML-PKDD 2025 Adversarial Attack Competition 1st Prize Solution",
      "abstract": "This report presents the winning solution for Task 1 of Colliding with\nAdversaries: A Challenge on Robust Learning in High Energy Physics Discovery at\nECML-PKDD 2025. The task required designing an adversarial attack against a\nprovided classification model that maximizes misclassification while minimizing\nperturbations. Our approach employs a multi-round gradient-based strategy that\nleverages the differentiable structure of the model, augmented with random\ninitialization and sample-mixing techniques to enhance effectiveness. The\nresulting attack achieved the best results in perturbation size and fooling\nsuccess rate, securing first place in the competition.",
      "authors": [
        {
          "name": "Dimitris Stefanopoulos",
          "affiliation": null
        },
        {
          "name": "Andreas Voskou",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cs.CR"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16440v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16440v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16439v1",
      "title": "FrugalPrompt: Reducing Contextual Overhead in Large Language Models via Token Attribution",
      "abstract": "Large language models (LLMs) owe much of their stellar performance to\nexpansive input contexts, yet such verbosity inflates monetary costs, carbon\nfootprint, and inference-time latency. Much of this overhead manifests from the\nredundant low-utility tokens present in typical prompts, as only a fraction of\ntokens typically carries the majority of the semantic weight. We address this\ninefficiency by introducing FrugalPrompt, a novel prompt compression framework\nfor LLMs, which retains only the most semantically significant tokens.\nLeveraging two state-of-the-art token attribution methods, GlobEnc and DecompX,\nwe assign salience scores to every token in an input sequence, rank them to\npreserve the top-k% tokens in their original order, and obtain a sparse\nfrugalized prompt. We evaluate the approach across four NLP tasks: Sentiment\nAnalysis, Commonsense QA, Summarization, and Mathematical Reasoning, using a\nsuite of frontier LLMs. For the first three tasks, a 20% prompt reduction\nincurs only a marginal loss in task performance, demonstrating that\ncontemporary LLMs can reconstruct elided context from high-salience cues. In\ncontrast, performance on mathematical reasoning deteriorates sharply,\nreflecting a stronger dependence on complete token continuity. Further analysis\nwith bottom-k% and random-k% tokens reveals asymmetric performance patterns\nthat may suggest potential task contamination effects, wherein models may\nresort to shallow memorized patterns from pretraining exposure for conventional\nNLP tasks. We posit that our work contributes to a more nuanced understanding\nof LLM behavior in performance-efficiency trade-offs, and delineate the\nboundary between tasks tolerant to contextual sparsity and those requiring\nexhaustive context. Our source code and models are available at:\nhttps://github.com/Starscream-11813/Frugal-ICL",
      "authors": [
        {
          "name": "Syed Rifat Raiyan",
          "affiliation": null
        },
        {
          "name": "Md Farhan Ishmam",
          "affiliation": null
        },
        {
          "name": "Abdullah Al Imran",
          "affiliation": null
        },
        {
          "name": "Mohammad Ali Moni",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16439v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16439v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16438v1",
      "title": "LightGlueStick: a Fast and Robust Glue for Joint Point-Line Matching",
      "abstract": "Lines and points are complementary local features, whose combination has\nproven effective for applications such as SLAM and Structure-from-Motion. The\nbackbone of these pipelines are the local feature matchers, establishing\ncorrespondences across images. Traditionally, point and line matching have been\ntreated as independent tasks. Recently, GlueStick proposed a GNN-based network\nthat simultaneously operates on points and lines to establish matches. While\nrunning a single joint matching reduced the overall computational complexity,\nthe heavy architecture prevented real-time applications or deployment to edge\ndevices.\n  Inspired by recent progress in point matching, we propose LightGlueStick, a\nlightweight matcher for points and line segments. The key novel component in\nour architecture is the Attentional Line Message Passing (ALMP), which\nexplicitly exposes the connectivity of the lines to the network, allowing for\nefficient communication between nodes. In thorough experiments we show that\nLightGlueStick establishes a new state-of-the-art across different benchmarks.\nThe code is available at https://github.com/aubingazhib/LightGlueStick.",
      "authors": [
        {
          "name": "Aidyn Ubingazhibov",
          "affiliation": null
        },
        {
          "name": "R\u00e9mi Pautrat",
          "affiliation": null
        },
        {
          "name": "Iago Su\u00e1rez",
          "affiliation": null
        },
        {
          "name": "Shaohui Liu",
          "affiliation": null
        },
        {
          "name": "Marc Pollefeys",
          "affiliation": null
        },
        {
          "name": "Viktor Larsson",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16438v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16438v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16435v1",
      "title": "What Questions Should Robots Be Able to Answer? A Dataset of User Questions for Explainable Robotics",
      "abstract": "With the growing use of large language models and conversational interfaces\nin human-robot interaction, robots' ability to answer user questions is more\nimportant than ever. We therefore introduce a dataset of 1,893 user questions\nfor household robots, collected from 100 participants and organized into 12\ncategories and 70 subcategories. Most work in explainable robotics focuses on\nwhy-questions. In contrast, our dataset provides a wide variety of questions,\nfrom questions about simple execution details to questions about how the robot\nwould act in hypothetical scenarios -- thus giving roboticists valuable\ninsights into what questions their robot needs to be able to answer. To collect\nthe dataset, we created 15 video stimuli and 7 text stimuli, depicting robots\nperforming varied household tasks. We then asked participants on Prolific what\nquestions they would want to ask the robot in each portrayed situation. In the\nfinal dataset, the most frequent categories are questions about task execution\ndetails (22.5%), the robot's capabilities (12.7%), and performance assessments\n(11.3%). Although questions about how robots would handle potentially difficult\nscenarios and ensure correct behavior are less frequent, users rank them as the\nmost important for robots to be able to answer. Moreover, we find that users\nwho identify as novices in robotics ask different questions than more\nexperienced users. Novices are more likely to inquire about simple facts, such\nas what the robot did or the current state of the environment. As robots enter\nenvironments shared with humans and language becomes central to giving\ninstructions and interaction, this dataset provides a valuable foundation for\n(i) identifying the information robots need to log and expose to conversational\ninterfaces, (ii) benchmarking question-answering modules, and (iii) designing\nexplanation strategies that align with user expectations.",
      "authors": [
        {
          "name": "Lennart Wachowiak",
          "affiliation": null
        },
        {
          "name": "Andrew Coles",
          "affiliation": null
        },
        {
          "name": "Gerard Canal",
          "affiliation": null
        },
        {
          "name": "Oya Celiktutan",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.RO",
        "cs.CL",
        "cs.HC",
        "I.2.9; H.5.2; H.5.0; I.2.8; I.2.7; J.4"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16435v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16435v1",
      "primary_category": "cs.RO",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16419v1",
      "title": "A Relative Error-Based Evaluation Framework of Heterogeneous Treatment Effect Estimators",
      "abstract": "While significant progress has been made in heterogeneous treatment effect\n(HTE) estimation, the evaluation of HTE estimators remains underdeveloped. In\nthis article, we propose a robust evaluation framework based on relative error,\nwhich quantifies performance differences between two HTE estimators. We first\nderive the key theoretical conditions on the nuisance parameters that are\nnecessary to achieve a robust estimator of relative error. Building on these\nconditions, we introduce novel loss functions and design a neural network\narchitecture to estimate nuisance parameters and obtain robust estimation of\nrelative error, thereby achieving reliable evaluation of HTE estimators. We\nprovide the large sample properties of the proposed relative error estimator.\nFurthermore, beyond evaluation, we propose a new learning algorithm for HTE\nthat leverages both the previously HTE estimators and the nuisance parameters\nlearned through our neural network architecture. Extensive experiments\ndemonstrate that our evaluation framework supports reliable comparisons across\nHTE estimators, and the proposed learning algorithm for HTE exhibits desirable\nperformance.",
      "authors": [
        {
          "name": "Jiayi Guo",
          "affiliation": null
        },
        {
          "name": "Haoxuan Li",
          "affiliation": null
        },
        {
          "name": "Ye Tian",
          "affiliation": null
        },
        {
          "name": "Peng Wu",
          "affiliation": null
        }
      ],
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16419v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16419v1",
      "primary_category": "stat.ML",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16416v1",
      "title": "SSL4RL: Revisiting Self-supervised Learning as Intrinsic Reward for Visual-Language Reasoning",
      "abstract": "Vision-language models (VLMs) have shown remarkable abilities by integrating\nlarge language models with visual inputs. However, they often fail to utilize\nvisual evidence adequately, either depending on linguistic priors in\nvision-centric tasks or resorting to textual shortcuts during reasoning.\nAlthough reinforcement learning (RL) can align models with desired behaviors,\nits application to VLMs has been hindered by the lack of scalable and reliable\nreward mechanisms. To overcome this challenge, we propose SSL4RL, a novel\nframework that leverages self-supervised learning (SSL) tasks as a source of\nverifiable rewards for RL-based fine-tuning. Our approach reformulates SSL\nobjectives-such as predicting image rotation or reconstructing masked\npatches-into dense, automatic reward signals, eliminating the need for human\npreference data or unreliable AI evaluators. Experiments show that SSL4RL\nsubstantially improves performance on both vision-centric and vision-language\nreasoning benchmarks. Furthermore, through systematic ablations, we identify\nkey factors-such as task difficulty, model scale, and semantic alignment with\nthe target domain-that influence the effectiveness of SSL4RL tasks, offering\nnew design principles for future work. We also demonstrate the framework's\ngenerality by applying it to graph learning, where it yields significant gains.\nSSL4RL establishes a versatile and effective paradigm for aligning multimodal\nmodels using verifiable, self-supervised objectives.",
      "authors": [
        {
          "name": "Xiaojun Guo",
          "affiliation": null
        },
        {
          "name": "Runyu Zhou",
          "affiliation": null
        },
        {
          "name": "Yifei Wang",
          "affiliation": null
        },
        {
          "name": "Qi Zhang",
          "affiliation": null
        },
        {
          "name": "Chenheng Zhang",
          "affiliation": null
        },
        {
          "name": "Stefanie Jegelka",
          "affiliation": null
        },
        {
          "name": "Xiaohan Wang",
          "affiliation": null
        },
        {
          "name": "Jiajun Chai",
          "affiliation": null
        },
        {
          "name": "Guojun Yin",
          "affiliation": null
        },
        {
          "name": "Wei Lin",
          "affiliation": null
        },
        {
          "name": "Yisen Wang",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16416v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16416v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16414v1",
      "title": "AoI-Aware Task Offloading and Transmission Optimization for Industrial IoT Networks: A Branching Deep Reinforcement Learning Approach",
      "abstract": "In the Industrial Internet of Things (IIoT), the frequent transmission of\nlarge amounts of data over wireless networks should meet the stringent\ntimeliness requirements. Particularly, the freshness of packet status updates\nhas a significant impact on the system performance. In this paper, we propose\nan age-of-information (AoI)-aware multi-base station (BS) real-time monitoring\nframework to support extensive IIoT deployments. To meet the freshness\nrequirements of IIoT, we formulate a joint task offloading and resource\nallocation optimization problem with the goal of minimizing long-term average\nAoI. Tackling the core challenges of combinatorial explosion in multi-BS\ndecision spaces and the stochastic dynamics of IIoT systems is crucial, as\nthese factors render traditional optimization methods intractable. Firstly, an\ninnovative branching-based Dueling Double Deep Q-Network (Branching-D3QN)\nalgorithm is proposed to effectively implement task offloading, which optimizes\nthe convergence performance by reducing the action space complexity from\nexponential to linear levels. Then, an efficient optimization solution to\nresource allocation is proposed by proving the semi-definite property of the\nHessian matrix of bandwidth and computation resources. Finally, we propose an\niterative optimization algorithm for efficient joint task offloading and\nresource allocation to achieve optimal average AoI performance. Extensive\nsimulations demonstrate that our proposed Branching-D3QN algorithm outperforms\nboth state-of-the-art DRL methods and classical heuristics, achieving up to a\n75% enhanced convergence speed and at least a 22% reduction in the long-term\naverage AoI.",
      "authors": [
        {
          "name": "Yuang Chen",
          "affiliation": null
        },
        {
          "name": "Fengqian Guo",
          "affiliation": null
        },
        {
          "name": "Chang Wu",
          "affiliation": null
        },
        {
          "name": "Shuyi Liu",
          "affiliation": null
        },
        {
          "name": "Hancheng Lu",
          "affiliation": null
        },
        {
          "name": "Chang Wen Chen",
          "affiliation": null
        }
      ],
      "categories": [
        "eess.SY",
        "cs.LG",
        "cs.SY"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16414v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16414v1",
      "primary_category": "eess.SY",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16411v1",
      "title": "Modeling Expert Interactions in Sparse Mixture of Experts via Graph Structures",
      "abstract": "Sparse Mixture of Experts (SMoE) has emerged as a promising solution to\nachieving unparalleled scalability in deep learning by decoupling model\nparameter count from computational cost. By activating only a small subset of\nparameters per sample, SMoE enables significant growth in model capacity while\nmaintaining efficiency. However, SMoE struggles to adapt to distributional\nshifts, leading to reduced robustness under data contamination. In this work,\nwe introduce SymphonySMoE, a novel family of SMoE that introduces a social\ngraph to model interactions among experts. This graph-based structure enhances\nthe token routing process, addressing the robustness challenges that are\ninherent in conventional SMoE designs. SymphonySMoE is lightweight, modular,\nand integrates seamlessly with existing SMoE-based models such as the XMoE and\nthe Generalist Language Model. We provide both theoretical analysis and\nempirical evidence demonstrating SymphonySMoE's advantages over baseline SMoE.\nExtensive experiments on language modeling and visual instruction tuning\nvalidate our method's effectiveness. We further highlight the scalability of\nSymphonySMoE to models with 4.2 and 7.4 billion parameters, showcasing its\napplicability in fine-tuning tasks for large-scale systems.",
      "authors": [
        {
          "name": "Minh-Khoi Nguyen-Nhat",
          "affiliation": null
        },
        {
          "name": "Rachel S. Y. Teo",
          "affiliation": null
        },
        {
          "name": "Laziz Abdullaev",
          "affiliation": null
        },
        {
          "name": "Maurice Mok",
          "affiliation": null
        },
        {
          "name": "Viet-Hoang Tran",
          "affiliation": null
        },
        {
          "name": "Tan Minh Nguyen",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16411v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16411v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16410v1",
      "title": "REALM: An MLLM-Agent Framework for Open World 3D Reasoning Segmentation and Editing on Gaussian Splatting",
      "abstract": "Bridging the gap between complex human instructions and precise 3D object\ngrounding remains a significant challenge in vision and robotics. Existing 3D\nsegmentation methods often struggle to interpret ambiguous, reasoning-based\ninstructions, while 2D vision-language models that excel at such reasoning lack\nintrinsic 3D spatial understanding. In this paper, we introduce REALM, an\ninnovative MLLM-agent framework that enables open-world reasoning-based\nsegmentation without requiring extensive 3D-specific post-training. We perform\nsegmentation directly on 3D Gaussian Splatting representations, capitalizing on\ntheir ability to render photorealistic novel views that are highly suitable for\nMLLM comprehension. As directly feeding one or more rendered views to the MLLM\ncan lead to high sensitivity to viewpoint selection, we propose a novel\nGlobal-to-Local Spatial Grounding strategy. Specifically, multiple global views\nare first fed into the MLLM agent in parallel for coarse-level localization,\naggregating responses to robustly identify the target object. Then, several\nclose-up novel views of the object are synthesized to perform fine-grained\nlocal segmentation, yielding accurate and consistent 3D masks. Extensive\nexperiments show that REALM achieves remarkable performance in interpreting\nboth explicit and implicit instructions across LERF, 3D-OVS, and our newly\nintroduced REALM3D benchmarks. Furthermore, our agent framework seamlessly\nsupports a range of 3D interaction tasks, including object removal,\nreplacement, and style transfer, demonstrating its practical utility and\nversatility. Project page: https://ChangyueShi.github.io/REALM.",
      "authors": [
        {
          "name": "Changyue Shi",
          "affiliation": null
        },
        {
          "name": "Minghao Chen",
          "affiliation": null
        },
        {
          "name": "Yiping Mao",
          "affiliation": null
        },
        {
          "name": "Chuxiao Yang",
          "affiliation": null
        },
        {
          "name": "Xinyuan Hu",
          "affiliation": null
        },
        {
          "name": "Jiajun Ding",
          "affiliation": null
        },
        {
          "name": "Zhou Yu",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16410v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16410v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16396v1",
      "title": "SPLite Hand: Sparsity-Aware Lightweight 3D Hand Pose Estimation",
      "abstract": "With the increasing ubiquity of AR/VR devices, the deployment of deep\nlearning models on edge devices has become a critical challenge. These devices\nrequire real-time inference, low power consumption, and minimal latency. Many\nframework designers face the conundrum of balancing efficiency and performance.\nWe design a light framework that adopts an encoder-decoder architecture and\nintroduces several key contributions aimed at improving both efficiency and\naccuracy. We apply sparse convolution on a ResNet-18 backbone to exploit the\ninherent sparsity in hand pose images, achieving a 42% end-to-end efficiency\nimprovement. Moreover, we propose our SPLite decoder. This new architecture\nsignificantly boosts the decoding process's frame rate by 3.1x on the Raspberry\nPi 5, while maintaining accuracy on par. To further optimize performance, we\napply quantization-aware training, reducing memory usage while preserving\naccuracy (PA-MPJPE increases only marginally from 9.0 mm to 9.1 mm on\nFreiHAND). Overall, our system achieves a 2.98x speed-up on a Raspberry Pi 5\nCPU (BCM2712 quad-core Arm A76 processor). Our method is also evaluated on\ncompound benchmark datasets, demonstrating comparable accuracy to\nstate-of-the-art approaches while significantly enhancing computational\nefficiency.",
      "authors": [
        {
          "name": "Yeh Keng Hao",
          "affiliation": null
        },
        {
          "name": "Hsu Tzu Wei",
          "affiliation": null
        },
        {
          "name": "Sun Min",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16396v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16396v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16393v1",
      "title": "Blending Learning to Rank and Dense Representations for Efficient and Effective Cascades",
      "abstract": "We investigate the exploitation of both lexical and neural relevance signals\nfor ad-hoc passage retrieval. Our exploration involves a large-scale training\ndataset in which dense neural representations of MS-MARCO queries and passages\nare complemented and integrated with 253 hand-crafted lexical features\nextracted from the same corpus. Blending of the relevance signals from the two\ndifferent groups of features is learned by a classical Learning-to-Rank (LTR)\nmodel based on a forest of decision trees. To evaluate our solution, we employ\na pipelined architecture where a dense neural retriever serves as the first\nstage and performs a nearest-neighbor search over the neural representations of\nthe documents. Our LTR model acts instead as the second stage that re-ranks the\nset of candidates retrieved by the first stage to enhance effectiveness. The\nresults of reproducible experiments conducted with state-of-the-art dense\nretrievers on publicly available resources show that the proposed solution\nsignificantly enhances the end-to-end ranking performance while relatively\nminimally impacting efficiency. Specifically, we achieve a boost in nDCG@10 of\nup to 11% with an increase in average query latency of only 4.3%. This confirms\nthe advantage of seamlessly combining two distinct families of signals that\nmutually contribute to retrieval effectiveness.",
      "authors": [
        {
          "name": "Franco Maria Nardini",
          "affiliation": null
        },
        {
          "name": "Raffaele Perego",
          "affiliation": null
        },
        {
          "name": "Nicola Tonellotto",
          "affiliation": null
        },
        {
          "name": "Salvatore Trani",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.IR",
        "cs.LG",
        "cs.PF"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16393v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16393v1",
      "primary_category": "cs.IR",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16392v1",
      "title": "RGMem: Renormalization Group-based Memory Evolution for Language Agent User Profile",
      "abstract": "Personalized and continuous interactions are the key to enhancing user\nexperience in today's large language model (LLM)-based conversational systems,\nhowever, the finite context windows and static parametric memory make it\ndifficult to model the cross-session long-term user states and behavioral\nconsistency. Currently, the existing solutions to this predicament, such as\nretrieval-augmented generation (RAG) and explicit memory systems, primarily\nfocus on fact-level storage and retrieval, lacking the capability to distill\nlatent preferences and deep traits from the multi-turn dialogues, which limits\nthe long-term and effective user modeling, directly leading to the personalized\ninteractions remaining shallow, and hindering the cross-session continuity. To\nrealize the long-term memory and behavioral consistency for Language Agents in\nLLM era, we propose a self-evolving memory framework RGMem, inspired by the\nideology of classic renormalization group (RG) in physics, this framework\nenables to organize the dialogue history in multiple scales: it first extracts\nsemantics and user insights from episodic fragments, then through hierarchical\ncoarse-graining and rescaling operations, progressively forms a\ndynamically-evolved user profile. The core innovation of our work lies in\nmodeling memory evolution as a multi-scale process of information compression\nand emergence, which accomplishes the high-level and accurate user profiles\nfrom noisy and microscopic-level interactions.",
      "authors": [
        {
          "name": "Ao Tian",
          "affiliation": null
        },
        {
          "name": "Yunfeng Lu",
          "affiliation": null
        },
        {
          "name": "Xinxin Fan",
          "affiliation": null
        },
        {
          "name": "Changhao Wang",
          "affiliation": null
        },
        {
          "name": "Lanzhi Zhou",
          "affiliation": null
        },
        {
          "name": "Yeyao Zhang",
          "affiliation": null
        },
        {
          "name": "Yanfang Liu",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.AI"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16392v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16392v1",
      "primary_category": "cs.AI",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16387v1",
      "title": "Probing the Hidden Talent of ASR Foundation Models for L2 English Oral Assessment",
      "abstract": "In this paper, we explore the untapped potential of Whisper, a\nwell-established automatic speech recognition (ASR) foundation model, in the\ncontext of L2 spoken language assessment (SLA). Unlike prior studies that\nextrinsically analyze transcriptions produced by Whisper, our approach goes a\nstep further to probe its latent capabilities by extracting acoustic and\nlinguistic features from hidden representations. With only a lightweight\nclassifier being trained on top of Whisper's intermediate and final outputs,\nour method achieves strong performance on the GEPT picture-description dataset,\noutperforming existing cutting-edge baselines, including a multimodal approach.\nFurthermore, by incorporating image and text-prompt information as auxiliary\nrelevance cues, we demonstrate additional performance gains. Finally, we\nconduct an in-depth analysis of Whisper's embeddings, which reveals that, even\nwithout task-specific fine-tuning, the model intrinsically encodes both ordinal\nproficiency patterns and semantic aspects of speech, highlighting its potential\nas a powerful foundation for SLA and other spoken language understanding tasks.",
      "authors": [
        {
          "name": "Fu-An Chao",
          "affiliation": null
        },
        {
          "name": "Bi-Cheng Yan",
          "affiliation": null
        },
        {
          "name": "Berlin Chen",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16387v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16387v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16382v1",
      "title": "Humanoid-inspired Causal Representation Learning for Domain Generalization",
      "abstract": "This paper proposes the Humanoid-inspired Structural Causal Model (HSCM), a\nnovel causal framework inspired by human intelligence, designed to overcome the\nlimitations of conventional domain generalization models. Unlike approaches\nthat rely on statistics to capture data-label dependencies and learn\ndistortion-invariant representations, HSCM replicates the hierarchical\nprocessing and multi-level learning of human vision systems, focusing on\nmodeling fine-grained causal mechanisms. By disentangling and reweighting key\nimage attributes such as color, texture, and shape, HSCM enhances\ngeneralization across diverse domains, ensuring robust performance and\ninterpretability. Leveraging the flexibility and adaptability of human\nintelligence, our approach enables more effective transfer and learning in\ndynamic, complex environments. Through both theoretical and empirical\nevaluations, we demonstrate that HSCM outperforms existing domain\ngeneralization models, providing a more principled method for capturing causal\nrelationships and improving model robustness. The code is available at\nhttps://github.com/lambett/HSCM.",
      "authors": [
        {
          "name": "Ze Tao",
          "affiliation": null
        },
        {
          "name": "Jian Zhang",
          "affiliation": null
        },
        {
          "name": "Haowei Li",
          "affiliation": null
        },
        {
          "name": "Xianshuai Li",
          "affiliation": null
        },
        {
          "name": "Yifei Peng",
          "affiliation": null
        },
        {
          "name": "Xiyao Liu",
          "affiliation": null
        },
        {
          "name": "Senzhang Wang",
          "affiliation": null
        },
        {
          "name": "Chao Liu",
          "affiliation": null
        },
        {
          "name": "Sheng Ren",
          "affiliation": null
        },
        {
          "name": "Shichao Zhang",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16382v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16382v1",
      "primary_category": "cs.AI",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16381v1",
      "title": "ATA: A Neuro-Symbolic Approach to Implement Autonomous and Trustworthy Agents",
      "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities, yet\ntheir deployment in high-stakes domains is hindered by inherent limitations in\ntrustworthiness, including hallucinations, instability, and a lack of\ntransparency. To address these challenges, we introduce a generic\nneuro-symbolic approach, which we call Autonomous Trustworthy Agents (ATA). The\ncore of our approach lies in decoupling tasks into two distinct phases: Offline\nknowledge ingestion and online task processing. During knowledge ingestion, an\nLLM translates an informal problem specification into a formal, symbolic\nknowledge base. This formal representation is crucial as it can be verified and\nrefined by human experts, ensuring its correctness and alignment with domain\nrequirements. In the subsequent task processing phase, each incoming input is\nencoded into the same formal language. A symbolic decision engine then utilizes\nthis encoded input in conjunction with the formal knowledge base to derive a\nreliable result. Through an extensive evaluation on a complex reasoning task,\nwe demonstrate that a concrete implementation of ATA is competitive with\nstate-of-the-art end-to-end reasoning models in a fully automated setup while\nmaintaining trustworthiness. Crucially, with a human-verified and corrected\nknowledge base, our approach significantly outperforms even larger models,\nwhile exhibiting perfect determinism, enhanced stability against input\nperturbations, and inherent immunity to prompt injection attacks. By generating\ndecisions grounded in symbolic reasoning, ATA offers a practical and\ncontrollable architecture for building the next generation of transparent,\nauditable, and reliable autonomous agents.",
      "authors": [
        {
          "name": "David Peer",
          "affiliation": null
        },
        {
          "name": "Sebastian Stabinger",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16381v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16381v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16380v1",
      "title": "MoReBench: Evaluating Procedural and Pluralistic Moral Reasoning in Language Models, More than Outcomes",
      "abstract": "As AI systems progress, we rely more on them to make decisions with us and\nfor us. To ensure that such decisions are aligned with human values, it is\nimperative for us to understand not only what decisions they make but also how\nthey come to those decisions. Reasoning language models, which provide both\nfinal responses and (partially transparent) intermediate thinking traces,\npresent a timely opportunity to study AI procedural reasoning. Unlike math and\ncode problems which often have objectively correct answers, moral dilemmas are\nan excellent testbed for process-focused evaluation because they allow for\nmultiple defensible conclusions. To do so, we present MoReBench: 1,000 moral\nscenarios, each paired with a set of rubric criteria that experts consider\nessential to include (or avoid) when reasoning about the scenarios. MoReBench\ncontains over 23 thousand criteria including identifying moral considerations,\nweighing trade-offs, and giving actionable recommendations to cover cases on AI\nadvising humans moral decisions as well as making moral decisions autonomously.\nSeparately, we curate MoReBench-Theory: 150 examples to test whether AI can\nreason under five major frameworks in normative ethics. Our results show that\nscaling laws and existing benchmarks on math, code, and scientific reasoning\ntasks fail to predict models' abilities to perform moral reasoning. Models also\nshow partiality towards specific moral frameworks (e.g., Benthamite Act\nUtilitarianism and Kantian Deontology), which might be side effects of popular\ntraining paradigms. Together, these benchmarks advance process-focused\nreasoning evaluation towards safer and more transparent AI.",
      "authors": [
        {
          "name": "Yu Ying Chiu",
          "affiliation": null
        },
        {
          "name": "Michael S. Lee",
          "affiliation": null
        },
        {
          "name": "Rachel Calcott",
          "affiliation": null
        },
        {
          "name": "Brandon Handoko",
          "affiliation": null
        },
        {
          "name": "Paul de Font-Reaulx",
          "affiliation": null
        },
        {
          "name": "Paula Rodriguez",
          "affiliation": null
        },
        {
          "name": "Chen Bo Calvin Zhang",
          "affiliation": null
        },
        {
          "name": "Ziwen Han",
          "affiliation": null
        },
        {
          "name": "Udari Madhushani Sehwag",
          "affiliation": null
        },
        {
          "name": "Yash Maurya",
          "affiliation": null
        },
        {
          "name": "Christina Q Knight",
          "affiliation": null
        },
        {
          "name": "Harry R. Lloyd",
          "affiliation": null
        },
        {
          "name": "Florence Bacus",
          "affiliation": null
        },
        {
          "name": "Mantas Mazeika",
          "affiliation": null
        },
        {
          "name": "Bing Liu",
          "affiliation": null
        },
        {
          "name": "Yejin Choi",
          "affiliation": null
        },
        {
          "name": "Mitchell L Gordon",
          "affiliation": null
        },
        {
          "name": "Sydney Levine",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.HC",
        "cs.LG"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16380v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16380v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16377v1",
      "title": "Demeter: A Parametric Model of Crop Plant Morphology from the Real World",
      "abstract": "Learning 3D parametric shape models of objects has gained popularity in\nvision and graphics and has showed broad utility in 3D reconstruction,\ngeneration, understanding, and simulation. While powerful models exist for\nhumans and animals, equally expressive approaches for modeling plants are\nlacking. In this work, we present Demeter, a data-driven parametric model that\nencodes key factors of a plant morphology, including topology, shape,\narticulation, and deformation into a compact learned representation. Unlike\nprevious parametric models, Demeter handles varying shape topology across\nvarious species and models three sources of shape variation: articulation,\nsubcomponent shape variation, and non-rigid deformation. To advance crop plant\nmodeling, we collected a large-scale, ground-truthed dataset from a soybean\nfarm as a testbed. Experiments show that Demeter effectively synthesizes\nshapes, reconstructs structures, and simulates biophysical processes. Code and\ndata is available at https://tianhang-cheng.github.io/Demeter/.",
      "authors": [
        {
          "name": "Tianhang Cheng",
          "affiliation": null
        },
        {
          "name": "Albert J. Zhai",
          "affiliation": null
        },
        {
          "name": "Evan Z. Chen",
          "affiliation": null
        },
        {
          "name": "Rui Zhou",
          "affiliation": null
        },
        {
          "name": "Yawen Deng",
          "affiliation": null
        },
        {
          "name": "Zitong Li",
          "affiliation": null
        },
        {
          "name": "Kejie Zhao",
          "affiliation": null
        },
        {
          "name": "Janice Shiu",
          "affiliation": null
        },
        {
          "name": "Qianyu Zhao",
          "affiliation": null
        },
        {
          "name": "Yide Xu",
          "affiliation": null
        },
        {
          "name": "Xinlei Wang",
          "affiliation": null
        },
        {
          "name": "Yuan Shen",
          "affiliation": null
        },
        {
          "name": "Sheng Wang",
          "affiliation": null
        },
        {
          "name": "Lisa Ainsworth",
          "affiliation": null
        },
        {
          "name": "Kaiyu Guan",
          "affiliation": null
        },
        {
          "name": "Shenlong Wang",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16377v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16377v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16376v1",
      "title": "Conformal Prediction in The Loop: A Feedback-Based Uncertainty Model for Trajectory Optimization",
      "abstract": "Conformal Prediction (CP) is a powerful statistical machine learning tool to\nconstruct uncertainty sets with coverage guarantees, which has fueled its\nextensive adoption in generating prediction regions for decision-making tasks,\ne.g., Trajectory Optimization (TO) in uncertain environments. However, existing\nmethods predominantly employ a sequential scheme, where decisions rely\nunidirectionally on the prediction regions, and consequently the information\nfrom decision-making fails to be fed back to instruct CP. In this paper, we\npropose a novel Feedback-Based CP (Fb-CP) framework for shrinking-horizon TO\nwith a joint risk constraint over the entire mission time. Specifically, a\nCP-based posterior risk calculation method is developed by fully leveraging the\nrealized trajectories to adjust the posterior allowable risk, which is then\nallocated to future times to update prediction regions. In this way, the\ninformation in the realized trajectories is continuously fed back to the CP,\nenabling attractive feedback-based adjustments of the prediction regions and a\nprovable online improvement in trajectory performance. Furthermore, we\ntheoretically prove that such adjustments consistently maintain the coverage\nguarantees of the prediction regions, thereby ensuring provable safety.\nAdditionally, we develop a decision-focused iterative risk allocation algorithm\nwith theoretical convergence analysis for allocating the posterior allowable\nrisk which closely aligns with Fb-CP. Furthermore, we extend the proposed\nmethod to handle distribution shift. The effectiveness and superiority of the\nproposed method are demonstrated through benchmark experiments.",
      "authors": [
        {
          "name": "Han Wang",
          "affiliation": null
        },
        {
          "name": "Chao Ning",
          "affiliation": null
        }
      ],
      "categories": [
        "math.OC",
        "cs.AI",
        "cs.RO",
        "cs.SY",
        "eess.SY",
        "math.ST",
        "stat.TH"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16376v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16376v1",
      "primary_category": "math.OC",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16375v1",
      "title": "iWatchRoadv2: Pothole Detection, Geospatial Mapping, and Intelligent Road Governance",
      "abstract": "Road potholes pose significant safety hazards and maintenance challenges,\nparticularly on India's diverse and under-maintained road networks. This paper\npresents iWatchRoadv2, a fully automated end-to-end platform for real-time\npothole detection, GPS-based geotagging, and dynamic road health visualization\nusing OpenStreetMap (OSM). We curated a self-annotated dataset of over 7,000\ndashcam frames capturing diverse Indian road conditions, weather patterns, and\nlighting scenarios, which we used to fine-tune the Ultralytics YOLO model for\naccurate pothole detection. The system synchronizes OCR-extracted video\ntimestamps with external GPS logs to precisely geolocate each detected pothole,\nenriching detections with comprehensive metadata, including road segment\nattribution and contractor information managed through an optimized backend\ndatabase. iWatchRoadv2 introduces intelligent governance features that enable\nauthorities to link road segments with contract metadata through a secure login\ninterface. The system automatically sends alerts to contractors and officials\nwhen road health deteriorates, supporting automated accountability and warranty\nenforcement. The intuitive web interface delivers actionable analytics to\nstakeholders and the public, facilitating evidence-driven repair planning,\nbudget allocation, and quality assessment. Our cost-effective and scalable\nsolution streamlines frame processing and storage while supporting seamless\npublic engagement for urban and rural deployments. By automating the complete\npothole monitoring lifecycle, from detection to repair verification,\niWatchRoadv2 enables data-driven smart city management, transparent governance,\nand sustainable improvements in road infrastructure maintenance. The platform\nand live demonstration are accessible at\nhttps://smlab.niser.ac.in/project/iwatchroad.",
      "authors": [
        {
          "name": "Rishi Raj Sahoo",
          "affiliation": null
        },
        {
          "name": "Surbhi Saswati Mohanty",
          "affiliation": null
        },
        {
          "name": "Subhankar Mishra",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16375v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16375v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16374v1",
      "title": "Before you <think>, monitor: Implementing Flavell's metacognitive framework in LLMs",
      "abstract": "Current approaches to enhancing LLM reasoning follows two isolated paradigms:\nMonitor-Generate methods like Plan-and-Solve (Wang et al., 2023) and\nSELF-DISCOVER (Zhou et al., 2024) excel at strategic planning but lack\nmechanisms to verify whether selected strategies succeed; while Generate-Verify\napproaches like Self-Verification (Weng et al., 2022) and SELF-REFINE (Madaan\net al., 2023) iteratively refine outputs but commence generation blindly\nwithout task assessment. This separation creates inefficiencies -- strategies\nfail without feedback, and refinement occurs without strategic grounding. We\naddress this gap by implementing Flavell's cognitive monitoring model (1979)\nfrom the broader Monitor-Generate-Verify framework (Oh and Gobet, 2025),\noperationalising it as a three-phase iterative system. On GSM8K, preliminary\nresults show 75.42% accuracy versus 68.44% for SELF-REFINE and 67.07% for\nSelf-Verification, while requiring fewer attempts (1.3 vs 2.0) at 27-37%\nincreased inference cost. These initial findings suggest upfront monitoring\nproduces higher-quality initial solutions that reduce refinement needs, though\nevaluation beyond arithmetic reasoning is needed to establish generalisability.",
      "authors": [
        {
          "name": "Nick Oh",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.AI"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16374v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16374v1",
      "primary_category": "cs.AI",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16373v1",
      "title": "Navigating through the hidden embedding space: steering LLMs to improve mental health assessment",
      "abstract": "The rapid evolution of Large Language Models (LLMs) is transforming AI,\nopening new opportunities in sensitive and high-impact areas such as Mental\nHealth (MH). Yet, despite these advancements, recent evidence reveals that\nsmaller-scale models still struggle to deliver optimal performance in\ndomain-specific applications. In this study, we present a cost-efficient yet\npowerful approach to improve MH assessment capabilities of an LLM, without\nrelying on any computationally intensive techniques. Our lightweight method\nconsists of a linear transformation applied to a specific layer's activations,\nleveraging steering vectors to guide the model's output. Remarkably, this\nintervention enables the model to achieve improved results across two distinct\ntasks: (1) identifying whether a Reddit post is useful for detecting the\npresence or absence of depressive symptoms (relevance prediction task), and (2)\ncompleting a standardized psychological screening questionnaire for depression\nbased on users' Reddit post history (questionnaire completion task). Results\nhighlight the untapped potential of steering mechanisms as computationally\nefficient tools for LLMs' MH domain adaptation.",
      "authors": [
        {
          "name": "Federico Ravenda",
          "affiliation": null
        },
        {
          "name": "Seyed Ali Bahrainian",
          "affiliation": null
        },
        {
          "name": "Andrea Raballo",
          "affiliation": null
        },
        {
          "name": "Antonietta Mira",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16373v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16373v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16371v1",
      "title": "Cataract-LMM: Large-Scale, Multi-Source, Multi-Task Benchmark for Deep Learning in Surgical Video Analysis",
      "abstract": "The development of computer-assisted surgery systems depends on large-scale,\nannotated datasets. Current resources for cataract surgery often lack the\ndiversity and annotation depth needed to train generalizable deep-learning\nmodels. To address this gap, we present a dataset of 3,000 phacoemulsification\ncataract surgery videos from two surgical centers, performed by surgeons with a\nrange of experience levels. This resource is enriched with four annotation\nlayers: temporal surgical phases, instance segmentation of instruments and\nanatomical structures, instrument-tissue interaction tracking, and quantitative\nskill scores based on the established competency rubrics like the ICO-OSCAR.\nThe technical quality of the dataset is supported by a series of benchmarking\nexperiments for key surgical AI tasks, including workflow recognition, scene\nsegmentation, and automated skill assessment. Furthermore, we establish a\ndomain adaptation baseline for the phase recognition task by training a model\non a subset of surgical centers and evaluating its performance on a held-out\ncenter. The dataset and annotations are available in Google Form\n(https://docs.google.com/forms/d/e/1FAIpQLSfmyMAPSTGrIy2sTnz0-TMw08ZagTimRulbAQcWdaPwDy187A/viewform?usp=dialog).",
      "authors": [
        {
          "name": "Mohammad Javad Ahmadi",
          "affiliation": null
        },
        {
          "name": "Iman Gandomi",
          "affiliation": null
        },
        {
          "name": "Parisa Abdi",
          "affiliation": null
        },
        {
          "name": "Seyed-Farzad Mohammadi",
          "affiliation": null
        },
        {
          "name": "Amirhossein Taslimi",
          "affiliation": null
        },
        {
          "name": "Mehdi Khodaparast",
          "affiliation": null
        },
        {
          "name": "Hassan Hashemi",
          "affiliation": null
        },
        {
          "name": "Mahdi Tavakoli",
          "affiliation": null
        },
        {
          "name": "Hamid D. Taghirad",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16371v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16371v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16370v1",
      "title": "MIRAD - A comprehensive real-world robust anomaly detection dataset for Mass Individualization",
      "abstract": "Social manufacturing leverages community collaboration and scattered\nresources to realize mass individualization in modern industry. However, this\nparadigm shift also introduces substantial challenges in quality control,\nparticularly in defect detection. The main difficulties stem from three\naspects. First, products often have highly customized configurations. Second,\nproduction typically involves fragmented, small-batch orders. Third, imaging\nenvironments vary considerably across distributed sites. To overcome the\nscarcity of real-world datasets and tailored algorithms, we introduce the Mass\nIndividualization Robust Anomaly Detection (MIRAD) dataset. As the first\nbenchmark explicitly designed for anomaly detection in social manufacturing,\nMIRAD captures three critical dimensions of this domain: (1) diverse\nindividualized products with large intra-class variation, (2) data collected\nfrom six geographically dispersed manufacturing nodes, and (3) substantial\nimaging heterogeneity, including variations in lighting, background, and motion\nconditions. We then conduct extensive evaluations of state-of-the-art (SOTA)\nanomaly detection methods on MIRAD, covering one-class, multi-class, and\nzero-shot approaches. Results show a significant performance drop across all\nmodels compared with conventional benchmarks, highlighting the unresolved\ncomplexities of defect detection in real-world individualized production. By\nbridging industrial requirements and academic research, MIRAD provides a\nrealistic foundation for developing robust quality control solutions essential\nfor Industry 5.0. The dataset is publicly available at\nhttps://github.com/wu33learn/MIRAD.",
      "authors": [
        {
          "name": "Pulin Li",
          "affiliation": null
        },
        {
          "name": "Guocheng Wu",
          "affiliation": null
        },
        {
          "name": "Li Yin",
          "affiliation": null
        },
        {
          "name": "Yuxin Zheng",
          "affiliation": null
        },
        {
          "name": "Wei Zhang",
          "affiliation": null
        },
        {
          "name": "Yanjie Zhou",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16370v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16370v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16368v1",
      "title": "The Burden of Interactive Alignment with Inconsistent Preferences",
      "abstract": "From media platforms to chatbots, algorithms shape how people interact,\nlearn, and discover information. Such interactions between users and an\nalgorithm often unfold over multiple steps, during which strategic users can\nguide the algorithm to better align with their true interests by selectively\nengaging with content. However, users frequently exhibit inconsistent\npreferences: they may spend considerable time on content that offers little\nlong-term value, inadvertently signaling that such content is desirable.\nFocusing on the user side, this raises a key question: what does it take for\nsuch users to align the algorithm with their true interests?\n  To investigate these dynamics, we model the user's decision process as split\nbetween a rational system 2 that decides whether to engage and an impulsive\nsystem 1 that determines how long engagement lasts. We then study a\nmulti-leader, single-follower extensive Stackelberg game, where users,\nspecifically system 2, lead by committing to engagement strategies and the\nalgorithm best-responds based on observed interactions. We define the burden of\nalignment as the minimum horizon over which users must optimize to effectively\nsteer the algorithm. We show that a critical horizon exists: users who are\nsufficiently foresighted can achieve alignment, while those who are not are\ninstead aligned to the algorithm's objective. This critical horizon can be\nlong, imposing a substantial burden. However, even a small, costly signal\n(e.g., an extra click) can significantly reduce it. Overall, our framework\nexplains how users with inconsistent preferences can align an engagement-driven\nalgorithm with their interests in a Stackelberg equilibrium, highlighting both\nthe challenges and potential remedies for achieving alignment.",
      "authors": [
        {
          "name": "Ali Shirali",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.LG",
        "econ.TH"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16368v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16368v1",
      "primary_category": "cs.AI",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16363v1",
      "title": "End-to-End Argument Mining through Autoregressive Argumentative Structure Prediction",
      "abstract": "Argument Mining (AM) helps in automating the extraction of complex\nargumentative structures such as Argument Components (ACs) like Premise, Claim\netc. and Argumentative Relations (ARs) like Support, Attack etc. in an\nargumentative text. Due to the inherent complexity of reasoning involved with\nthis task, modelling dependencies between ACs and ARs is challenging. Most of\nthe recent approaches formulate this task through a generative paradigm by\nflattening the argumentative structures. In contrast to that, this study\njointly formulates the key tasks of AM in an end-to-end fashion using\nAutoregressive Argumentative Structure Prediction (AASP) framework. The\nproposed AASP framework is based on the autoregressive structure prediction\nframework that has given good performance for several NLP tasks. AASP framework\nmodels the argumentative structures as constrained pre-defined sets of actions\nwith the help of a conditional pre-trained language model. These actions build\nthe argumentative structures step-by-step in an autoregressive manner to\ncapture the flow of argumentative reasoning in an efficient way. Extensive\nexperiments conducted on three standard AM benchmarks demonstrate that AASP\nachieves state-of-theart (SoTA) results across all AM tasks in two benchmarks\nand delivers strong results in one benchmark.",
      "authors": [
        {
          "name": "Nilmadhab Das",
          "affiliation": null
        },
        {
          "name": "Vishal Vaibhav",
          "affiliation": null
        },
        {
          "name": "Yash Sunil Choudhary",
          "affiliation": null
        },
        {
          "name": "V. Vijaya Saradhi",
          "affiliation": null
        },
        {
          "name": "Ashish Anand",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16363v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16363v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16359v1",
      "title": "Utilising Large Language Models for Generating Effective Counter Arguments to Anti-Vaccine Tweets",
      "abstract": "In an era where public health is increasingly influenced by information\nshared on social media, combatting vaccine skepticism and misinformation has\nbecome a critical societal goal. Misleading narratives around vaccination have\nspread widely, creating barriers to achieving high immunisation rates and\nundermining trust in health recommendations. While efforts to detect\nmisinformation have made significant progress, the generation of real time\ncounter-arguments tailored to debunk such claims remains an insufficiently\nexplored area. In this work, we explore the capabilities of LLMs to generate\nsound counter-argument rebuttals to vaccine misinformation. Building on prior\nresearch in misinformation debunking, we experiment with various prompting\nstrategies and fine-tuning approaches to optimise counter-argument generation.\nAdditionally, we train classifiers to categorise anti-vaccine tweets into\nmulti-labeled categories such as concerns about vaccine efficacy, side effects,\nand political influences allowing for more context aware rebuttals. Our\nevaluation, conducted through human judgment, LLM based assessments, and\nautomatic metrics, reveals strong alignment across these methods. Our findings\ndemonstrate that integrating label descriptions and structured fine-tuning\nenhances counter-argument effectiveness, offering a promising approach for\nmitigating vaccine misinformation at scale.",
      "authors": [
        {
          "name": "Utsav Dhanuka",
          "affiliation": null
        },
        {
          "name": "Soham Poddar",
          "affiliation": null
        },
        {
          "name": "Saptarshi Ghosh",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16359v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16359v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16357v1",
      "title": "MLCPD: A Unified Multi-Language Code Parsing Dataset with Universal AST Schema",
      "abstract": "We introduce the MultiLang Code Parser Dataset (MLCPD), a large-scale,\nlanguage-agnostic dataset unifying syntactic and structural representations of\ncode across ten major programming languages. MLCPD contains over seven million\nparsed source files normalized under our proposed universal Abstract Syntax\nTree (AST) schema, enabling consistent cross-language reasoning, structural\nlearning, and multilingual software analysis. Unlike existing corpora that\nfocus purely on token-level code or isolated parsers, MLCPD provides both\nhierarchical tree representations and rich metadata for every file, ensuring\nlossless syntactic coverage and structural uniformity. Each entry includes a\nnormalized schema, language-level metadata, and abstracted node semantics\nstored in Parquet format for scalable retrieval. Empirical analyses reveal\nstrong cross-language structural regularities-demonstrating that syntactic\ngraphs from languages as diverse as Python, Java, and Go can be aligned under a\nshared schema. We release the dataset publicly on Hugging Face and the\naccompanying codebase on GitHub, which includes complete pipelines for dataset\nreproduction, grammar compilation, and a visualization tool for exploring the\nunified AST across languages. Together, these resources establish MLCPD as an\nopen, reproducible foundation for future research in cross-language\nrepresentation learning and program analysis.",
      "authors": [
        {
          "name": "Jugal Gajjar",
          "affiliation": null
        },
        {
          "name": "Kamalasankari Subramaniakuppusamy",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.SE",
        "cs.LG",
        "cs.PL"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16357v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16357v1",
      "primary_category": "cs.SE",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16356v1",
      "title": "Sparse Transformer Architectures via Regularized Wasserstein Proximal Operator with $L_1$ Prior",
      "abstract": "In this work, we propose a sparse transformer architecture that incorporates\nprior information about the underlying data distribution directly into the\ntransformer structure of the neural network. The design of the model is\nmotivated by a special optimal transport problem, namely the regularized\nWasserstein proximal operator, which admits a closed-form solution and turns\nout to be a special representation of transformer architectures. Compared with\nclassical flow-based models, the proposed approach improves the convexity\nproperties of the optimization problem and promotes sparsity in the generated\nsamples. Through both theoretical analysis and numerical experiments, including\napplications in generative modeling and Bayesian inverse problems, we\ndemonstrate that the sparse transformer achieves higher accuracy and faster\nconvergence to the target distribution than classical neural ODE-based methods.",
      "authors": [
        {
          "name": "Fuqun Han",
          "affiliation": null
        },
        {
          "name": "Stanley Osher",
          "affiliation": null
        },
        {
          "name": "Wuchen Li",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "math.OC",
        "stat.ML"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16356v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16356v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16350v1",
      "title": "MGTS-Net: Exploring Graph-Enhanced Multimodal Fusion for Augmented Time Series Forecasting",
      "abstract": "Recent research in time series forecasting has explored integrating\nmultimodal features into models to improve accuracy. However, the accuracy of\nsuch methods is constrained by three key challenges: inadequate extraction of\nfine-grained temporal patterns, suboptimal integration of multimodal\ninformation, and limited adaptability to dynamic multi-scale features. To\naddress these problems, we propose MGTS-Net, a Multimodal Graph-enhanced\nNetwork for Time Series forecasting. The model consists of three core\ncomponents: (1) a Multimodal Feature Extraction layer (MFE), which optimizes\nfeature encoders according to the characteristics of temporal, visual, and\ntextual modalities to extract temporal features of fine-grained patterns; (2) a\nMultimodal Feature Fusion layer (MFF), which constructs a heterogeneous graph\nto model intra-modal temporal dependencies and cross-modal alignment\nrelationships and dynamically aggregates multimodal knowledge; (3) a\nMulti-Scale Prediction layer (MSP), which adapts to multi-scale features by\ndynamically weighting and fusing the outputs of short-term, medium-term, and\nlong-term predictors. Extensive experiments demonstrate that MGTS-Net exhibits\nexcellent performance with light weight and high efficiency. Compared with\nother state-of-the-art baseline models, our method achieves superior\nperformance, validating the superiority of the proposed methodology.",
      "authors": [
        {
          "name": "Shule Hao",
          "affiliation": null
        },
        {
          "name": "Junpeng Bao",
          "affiliation": null
        },
        {
          "name": "Wenli Li",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16350v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16350v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16344v1",
      "title": "Manual2Skill++: Connector-Aware General Robotic Assembly from Instruction Manuals via Vision-Language Models",
      "abstract": "Assembly hinges on reliably forming connections between parts; yet most\nrobotic approaches plan assembly sequences and part poses while treating\nconnectors as an afterthought. Connections represent the critical \"last mile\"\nof assembly execution, while task planning may sequence operations and motion\nplan may position parts, the precise establishment of physical connections\nultimately determines assembly success or failure. In this paper, we consider\nconnections as first-class primitives in assembly representation, including\nconnector types, specifications, quantities, and placement locations. Drawing\ninspiration from how humans learn assembly tasks through step-by-step\ninstruction manuals, we present Manual2Skill++, a vision-language framework\nthat automatically extracts structured connection information from assembly\nmanuals. We encode assembly tasks as hierarchical graphs where nodes represent\nparts and sub-assemblies, and edges explicitly model connection relationships\nbetween components. A large-scale vision-language model parses symbolic\ndiagrams and annotations in manuals to instantiate these graphs, leveraging the\nrich connection knowledge embedded in human-designed instructions. We curate a\ndataset containing over 20 assembly tasks with diverse connector types to\nvalidate our representation extraction approach, and evaluate the complete task\nunderstanding-to-execution pipeline across four complex assembly scenarios in\nsimulation, spanning furniture, toys, and manufacturing components with\nreal-world correspondence.",
      "authors": [
        {
          "name": "Chenrui Tie",
          "affiliation": null
        },
        {
          "name": "Shengxiang Sun",
          "affiliation": null
        },
        {
          "name": "Yudi Lin",
          "affiliation": null
        },
        {
          "name": "Yanbo Wang",
          "affiliation": null
        },
        {
          "name": "Zhongrui Li",
          "affiliation": null
        },
        {
          "name": "Zhouhan Zhong",
          "affiliation": null
        },
        {
          "name": "Jinxuan Zhu",
          "affiliation": null
        },
        {
          "name": "Yiman Pang",
          "affiliation": null
        },
        {
          "name": "Haonan Chen",
          "affiliation": null
        },
        {
          "name": "Junting Chen",
          "affiliation": null
        },
        {
          "name": "Ruihai Wu",
          "affiliation": null
        },
        {
          "name": "Lin Shao",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16344v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16344v1",
      "primary_category": "cs.RO",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16342v1",
      "title": "Beyond Fixed Anchors: Precisely Erasing Concepts with Sibling Exclusive Counterparts",
      "abstract": "Existing concept erasure methods for text-to-image diffusion models commonly\nrely on fixed anchor strategies, which often lead to critical issues such as\nconcept re-emergence and erosion. To address this, we conduct causal tracing to\nreveal the inherent sensitivity of erasure to anchor selection and define\nSibling Exclusive Concepts as a superior class of anchors. Based on this\ninsight, we propose \\textbf{SELECT} (Sibling-Exclusive Evaluation for\nContextual Targeting), a dynamic anchor selection framework designed to\novercome the limitations of fixed anchors. Our framework introduces a novel\ntwo-stage evaluation mechanism that automatically discovers optimal anchors for\nprecise erasure while identifying critical boundary anchors to preserve related\nconcepts. Extensive evaluations demonstrate that SELECT, as a universal anchor\nsolution, not only efficiently adapts to multiple erasure frameworks but also\nconsistently outperforms existing baselines across key performance metrics,\naveraging only 4 seconds for anchor mining of a single concept.",
      "authors": [
        {
          "name": "Tong Zhang",
          "affiliation": null
        },
        {
          "name": "Ru Zhang",
          "affiliation": null
        },
        {
          "name": "Jianyi Liu",
          "affiliation": null
        },
        {
          "name": "Zhen Yang",
          "affiliation": null
        },
        {
          "name": "Gongshen Liu",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16342v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16342v1",
      "primary_category": "cs.AI",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16340v1",
      "title": "Thinking About Thinking: Evaluating Reasoning in Post-Trained Language Models",
      "abstract": "Recent advances in post-training techniques have endowed Large Language\nModels (LLMs) with enhanced capabilities for tackling complex, logic-intensive\ntasks through the generation of supplementary planning tokens. This development\nraises a fundamental question: Are these models aware of what they \"learn\" and\n\"think\"? To address this, we define three core competencies: (1) awareness of\nlearned latent policies, (2) generalization of these policies across domains,\nand (3) alignment between internal reasoning traces and final outputs. We\nempirically evaluate these abilities on several tasks, each designed to require\nlearning a distinct policy. Furthermore, we contrast the profiles of models\npost-trained via Supervised Fine-Tuning (SFT), Direct Policy Optimization\n(DPO), and Group Relative Policy Optimization (GRPO). Our findings indicate\nthat RL-trained models not only demonstrate greater awareness of their learned\nbehaviors and stronger generalizability to novel, structurally similar tasks\nthan SFT models but also often exhibit weak alignment between their reasoning\ntraces and final outputs, an effect most pronounced in GRPO-trained models.",
      "authors": [
        {
          "name": "Pratham Singla",
          "affiliation": null
        },
        {
          "name": "Shivank Garg",
          "affiliation": null
        },
        {
          "name": "Ayush Singh",
          "affiliation": null
        },
        {
          "name": "Ishan Garg",
          "affiliation": null
        },
        {
          "name": "Ketan Suhaas Saichandran",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16340v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16340v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16335v1",
      "title": "On the Provable Importance of Gradients for Language-Assisted Image Clustering",
      "abstract": "This paper investigates the recently emerged problem of Language-assisted\nImage Clustering (LaIC), where textual semantics are leveraged to improve the\ndiscriminability of visual representations to facilitate image clustering. Due\nto the unavailability of true class names, one of core challenges of LaIC lies\nin how to filter positive nouns, i.e., those semantically close to the images\nof interest, from unlabeled wild corpus data. Existing filtering strategies are\npredominantly based on the off-the-shelf feature space learned by CLIP;\nhowever, despite being intuitive, these strategies lack a rigorous theoretical\nfoundation. To fill this gap, we propose a novel gradient-based framework,\ntermed as GradNorm, which is theoretically guaranteed and shows strong\nempirical performance. In particular, we measure the positiveness of each noun\nbased on the magnitude of gradients back-propagated from the cross-entropy\nbetween the predicted target distribution and the softmax output.\nTheoretically, we provide a rigorous error bound to quantify the separability\nof positive nouns by GradNorm and prove that GradNorm naturally subsumes\nexisting filtering strategies as extremely special cases of itself.\nEmpirically, extensive experiments show that GradNorm achieves the\nstate-of-the-art clustering performance on various benchmarks.",
      "authors": [
        {
          "name": "Bo Peng",
          "affiliation": null
        },
        {
          "name": "Jie Lu",
          "affiliation": null
        },
        {
          "name": "Guangquan Zhang",
          "affiliation": null
        },
        {
          "name": "Zhen Fang",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16335v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16335v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16334v1",
      "title": "Investigating the Association Between Text-Based Indications of Foodborne Illness from Yelp Reviews and New York City Health Inspection Outcomes (2023)",
      "abstract": "Foodborne illnesses are gastrointestinal conditions caused by consuming\ncontaminated food. Restaurants are critical venues to investigate outbreaks\nbecause they share sourcing, preparation, and distribution of foods. Public\nreporting of illness via formal channels is limited, whereas social media\nplatforms host abundant user-generated content that can provide timely public\nhealth signals. This paper analyzes signals from Yelp reviews produced by a\nHierarchical Sigmoid Attention Network (HSAN) classifier and compares them with\nofficial restaurant inspection outcomes issued by the New York City Department\nof Health and Mental Hygiene (NYC DOHMH) in 2023. We evaluate correlations at\nthe Census tract level, compare distributions of HSAN scores by prevalence of\nC-graded restaurants, and map spatial patterns across NYC. We find minimal\ncorrelation between HSAN signals and inspection scores at the tract level and\nno significant differences by number of C-graded restaurants. We discuss\nimplications and outline next steps toward address-level analyses.",
      "authors": [
        {
          "name": "Eden Shaveet",
          "affiliation": null
        },
        {
          "name": "Crystal Su",
          "affiliation": null
        },
        {
          "name": "Daniel Hsu",
          "affiliation": null
        },
        {
          "name": "Luis Gravano",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.IR",
        "cs.CL"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16334v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16334v1",
      "primary_category": "cs.IR",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16333v1",
      "title": "RL makes MLLMs see better than SFT",
      "abstract": "A dominant assumption in Multimodal Language Model (MLLM) research is that\nits performance is largely inherited from the LLM backbone, given its immense\nparameter scale and remarkable capabilities. This has created a void in the\nunderstanding of the vision encoder, which determines how MLLMs perceive\nimages. The recent shift in MLLM training paradigms, from Supervised Finetuning\n(SFT) to Reinforcement Learning (RL), magnifies this oversight-namely, the\nsignificant lack of analysis on how such training reshapes the vision encoder\nas well as the MLLM. To address this, we first investigate the impact of\ntraining strategies on MLLMs, where RL shows a clear advantage over SFT in\nstrongly vision-related VQA benchmarks. Motivated by this, we conduct a\ncritical yet under-explored analysis of the vision encoder of MLLMs through\ndiverse and in-depth experiments, ranging from ImageNet classification and\nsegmentation to gradient visualization. Our results demonstrate that MLLM's\npost-training strategy (i.e., SFT or RL) not only leads to distinct outcomes on\nMLLM downstream tasks, but also fundamentally reshapes MLLM's underlying visual\nrepresentations. Specifically, the key finding of our study is that RL produces\nstronger and precisely localized visual representations compared to SFT,\nboosting the ability of the vision encoder for MLLM. We then reframe our\nfindings into a simple recipe for building strong vision encoders for MLLMs,\nPreference-Instructed Vision OpTimization (PIVOT). When integrated into MLLMs,\na PIVOT-trained vision encoder outperforms even larger and more heavily-trained\ncounterparts, despite requiring less than 1% of the computational cost of\nstandard vision pretraining. This result opens an effective and efficient path\nfor advancing the vision backbones of MLLMs. Project page available at\nhttps://june-page.github.io/pivot/",
      "authors": [
        {
          "name": "Junha Song",
          "affiliation": null
        },
        {
          "name": "Sangdoo Yun",
          "affiliation": null
        },
        {
          "name": "Dongyoon Han",
          "affiliation": null
        },
        {
          "name": "Jaegul Choo",
          "affiliation": null
        },
        {
          "name": "Byeongho Heo",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16333v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16333v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16332v1",
      "title": "TokenAR: Multiple Subject Generation via Autoregressive Token-level enhancement",
      "abstract": "Autoregressive Model (AR) has shown remarkable success in conditional image\ngeneration. However, these approaches for multiple reference generation\nstruggle with decoupling different reference identities. In this work, we\npropose the TokenAR framework, specifically focused on a simple but effective\ntoken-level enhancement mechanism to address reference identity confusion\nproblem. Such token-level enhancement consists of three parts, 1). Token Index\nEmbedding clusters the tokens index for better representing the same reference\nimages; 2). Instruct Token Injection plays as a role of extra visual feature\ncontainer to inject detailed and complementary priors for reference tokens; 3).\nThe identity-token disentanglement strategy (ITD) explicitly guides the token\nrepresentations toward independently representing the features of each\nidentity.This token-enhancement framework significantly augments the\ncapabilities of existing AR based methods in conditional image generation,\nenabling good identity consistency while preserving high quality background\nreconstruction. Driven by the goal of high-quality and high-diversity in\nmulti-subject generation, we introduce the InstructAR Dataset, the first\nopen-source, large-scale, multi-reference input, open domain image generation\ndataset that includes 28K training pairs, each example has two reference\nsubjects, a relative prompt and a background with mask annotation, curated for\nmultiple reference image generation training and evaluating. Comprehensive\nexperiments validate that our approach surpasses current state-of-the-art\nmodels in multiple reference image generation task. The implementation code and\ndatasets will be made publicly. Codes are available, see\nhttps://github.com/lyrig/TokenAR",
      "authors": [
        {
          "name": "Haiyue Sun",
          "affiliation": null
        },
        {
          "name": "Qingdong He",
          "affiliation": null
        },
        {
          "name": "Jinlong Peng",
          "affiliation": null
        },
        {
          "name": "Peng Tang",
          "affiliation": null
        },
        {
          "name": "Jiangning Zhang",
          "affiliation": null
        },
        {
          "name": "Junwei Zhu",
          "affiliation": null
        },
        {
          "name": "Xiaobin Hu",
          "affiliation": null
        },
        {
          "name": "Shuicheng Yan",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16332v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16332v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16326v1",
      "title": "DiffusionX: Efficient Edge-Cloud Collaborative Image Generation with Multi-Round Prompt Evolution",
      "abstract": "Recent advances in diffusion models have driven remarkable progress in image\ngeneration. However, the generation process remains computationally intensive,\nand users often need to iteratively refine prompts to achieve the desired\nresults, further increasing latency and placing a heavy burden on cloud\nresources. To address this challenge, we propose DiffusionX, a cloud-edge\ncollaborative framework for efficient multi-round, prompt-based generation. In\nthis system, a lightweight on-device diffusion model interacts with users by\nrapidly producing preview images, while a high-capacity cloud model performs\nfinal refinements after the prompt is finalized. We further introduce a noise\nlevel predictor that dynamically balances the computation load, optimizing the\ntrade-off between latency and cloud workload. Experiments show that DiffusionX\nreduces average generation time by 15.8% compared with Stable Diffusion v1.5,\nwhile maintaining comparable image quality. Moreover, it is only 0.9% slower\nthan Tiny-SD with significantly improved image quality, thereby demonstrating\nefficiency and scalability with minimal overhead.",
      "authors": [
        {
          "name": "Yi Wei",
          "affiliation": null
        },
        {
          "name": "Shunpu Tang",
          "affiliation": null
        },
        {
          "name": "Liang Zhao",
          "affiliation": null
        },
        {
          "name": "Qiangian Yang",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16326v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16326v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16325v1",
      "title": "Scale-DiT: Ultra-High-Resolution Image Generation with Hierarchical Local Attention",
      "abstract": "Ultra-high-resolution text-to-image generation demands both fine-grained\ntexture synthesis and globally coherent structure, yet current diffusion models\nremain constrained to sub-$1K \\times 1K$ resolutions due to the prohibitive\nquadratic complexity of attention and the scarcity of native $4K$ training\ndata. We present \\textbf{Scale-DiT}, a new diffusion framework that introduces\nhierarchical local attention with low-resolution global guidance, enabling\nefficient, scalable, and semantically coherent image synthesis at ultra-high\nresolutions. Specifically, high-resolution latents are divided into fixed-size\nlocal windows to reduce attention complexity from quadratic to near-linear,\nwhile a low-resolution latent equipped with scaled positional anchors injects\nglobal semantics. A lightweight LoRA adaptation bridges global and local\npathways during denoising, ensuring consistency across structure and detail. To\nmaximize inference efficiency, we repermute token sequence in Hilbert curve\norder and implement a fused-kernel for skipping masked operations, resulting in\na GPU-friendly design. Extensive experiments demonstrate that Scale-DiT\nachieves more than $2\\times$ faster inference and lower memory usage compared\nto dense attention baselines, while reliably scaling to $4K \\times 4K$\nresolution without requiring additional high-resolution training data. On both\nquantitative benchmarks (FID, IS, CLIP Score) and qualitative comparisons,\nScale-DiT delivers superior global coherence and sharper local detail, matching\nor outperforming state-of-the-art methods that rely on native 4K training.\nTaken together, these results highlight hierarchical local attention with\nguided low-resolution anchors as a promising and effective approach for\nadvancing ultra-high-resolution image generation.",
      "authors": [
        {
          "name": "Yuyao Zhang",
          "affiliation": null
        },
        {
          "name": "Yu-Wing Tai",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16325v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16325v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16322v1",
      "title": "Memorizing Long-tail Data Can Help Generalization Through Composition",
      "abstract": "Deep learning has led researchers to rethink the relationship between\nmemorization and generalization. In many settings, memorization does not hurt\ngeneralization due to implicit regularization and may help by memorizing\nlong-tailed examples. In this paper, we consider the synergy between\nmemorization and simple composition -- the ability to make correct prediction\non a combination of long-tailed features. Theoretically, we show that for a\nlinear setting, memorization together with composition can help the model make\ncorrect predictions on rare test examples that require a combination of\nlong-tailed features, even if such combinations were never observed in the\ntraining data. Experiments on neural network architecture on simple data show\nthat the theoretical insight extends beyond the linear setting, and we further\nobserve that the composition capability of the model depends on its\narchitecture.",
      "authors": [
        {
          "name": "Mo Zhou",
          "affiliation": null
        },
        {
          "name": "Haoyang Ma",
          "affiliation": null
        },
        {
          "name": "Rong Ge",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16322v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16322v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16321v1",
      "title": "Time-Embedded Algorithm Unrolling for Computational MRI",
      "abstract": "Algorithm unrolling methods have proven powerful for solving the regularized\nleast squares problem in computational magnetic resonance imaging (MRI). These\napproaches unfold an iterative algorithm with a fixed number of iterations,\ntypically alternating between a neural network-based proximal operator for\nregularization, a data fidelity operation and auxiliary updates with learnable\nparameters. While the connection to optimization methods dictate that the\nproximal operator network should be shared across unrolls, this can introduce\nartifacts or blurring. Heuristically, practitioners have shown that using\ndistinct networks may be beneficial, but this significantly increases the\nnumber of learnable parameters, making it challenging to prevent overfitting.\nTo address these shortcomings, by taking inspirations from proximal operators\nwith varying thresholds in approximate message passing (AMP) and the success of\ntime-embedding in diffusion models, we propose a time-embedded algorithm\nunrolling scheme for inverse problems. Specifically, we introduce a novel\nperspective on the iteration-dependent proximal operation in vector AMP (VAMP)\nand the subsequent Onsager correction in the context of algorithm unrolling,\nframing them as a time-embedded neural network. Similarly, the scalar weights\nin the data fidelity operation and its associated Onsager correction are cast\nas time-dependent learnable parameters. Our extensive experiments on the\nfastMRI dataset, spanning various acceleration rates and datasets, demonstrate\nthat our method effectively reduces aliasing artifacts and mitigates noise\namplification, achieving state-of-the-art performance. Furthermore, we show\nthat our time-embedding strategy extends to existing algorithm unrolling\napproaches, enhancing reconstruction quality without increasing the\ncomputational complexity significantly.",
      "authors": [
        {
          "name": "Junno Yun",
          "affiliation": null
        },
        {
          "name": "Ya\u015far Utku Al\u00e7alar",
          "affiliation": null
        },
        {
          "name": "Mehmet Ak\u00e7akaya",
          "affiliation": null
        }
      ],
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "physics.med-ph"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16321v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16321v1",
      "primary_category": "eess.IV",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16320v1",
      "title": "Scaling Laws for Deepfake Detection",
      "abstract": "This paper presents a systematic study of scaling laws for the deepfake\ndetection task. Specifically, we analyze the model performance against the\nnumber of real image domains, deepfake generation methods, and training images.\nSince no existing dataset meets the scale requirements for this research, we\nconstruct ScaleDF, the largest dataset to date in this field, which contains\nover 5.8 million real images from 51 different datasets (domains) and more than\n8.8 million fake images generated by 102 deepfake methods. Using ScaleDF, we\nobserve power-law scaling similar to that shown in large language models\n(LLMs). Specifically, the average detection error follows a predictable\npower-law decay as either the number of real domains or the number of deepfake\nmethods increases. This key observation not only allows us to forecast the\nnumber of additional real domains or deepfake methods required to reach a\ntarget performance, but also inspires us to counter the evolving deepfake\ntechnology in a data-centric manner. Beyond this, we examine the role of\npre-training and data augmentations in deepfake detection under scaling, as\nwell as the limitations of scaling itself.",
      "authors": [
        {
          "name": "Wenhao Wang",
          "affiliation": null
        },
        {
          "name": "Longqi Cai",
          "affiliation": null
        },
        {
          "name": "Taihong Xiao",
          "affiliation": null
        },
        {
          "name": "Yuxiao Wang",
          "affiliation": null
        },
        {
          "name": "Ming-Hsuan Yang",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16320v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16320v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16319v1",
      "title": "Stroke2Sketch: Harnessing Stroke Attributes for Training-Free Sketch Generation",
      "abstract": "Generating sketches guided by reference styles requires precise transfer of\nstroke attributes, such as line thickness, deformation, and texture sparsity,\nwhile preserving semantic structure and content fidelity. To this end, we\npropose Stroke2Sketch, a novel training-free framework that introduces\ncross-image stroke attention, a mechanism embedded within self-attention layers\nto establish fine-grained semantic correspondences and enable accurate stroke\nattribute transfer. This allows our method to adaptively integrate reference\nstroke characteristics into content images while maintaining structural\nintegrity. Additionally, we develop adaptive contrast enhancement and\nsemantic-focused attention to reinforce content preservation and foreground\nemphasis. Stroke2Sketch effectively synthesizes stylistically faithful sketches\nthat closely resemble handcrafted results, outperforming existing methods in\nexpressive stroke control and semantic coherence. Codes are available at\nhttps://github.com/rane7/Stroke2Sketch.",
      "authors": [
        {
          "name": "Rui Yang",
          "affiliation": null
        },
        {
          "name": "Huining Li",
          "affiliation": null
        },
        {
          "name": "Yiyi Long",
          "affiliation": null
        },
        {
          "name": "Xiaojun Wu",
          "affiliation": null
        },
        {
          "name": "Shengfeng He",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16319v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16319v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16311v1",
      "title": "Toward General Digraph Contrastive Learning: A Dual Spatial Perspective",
      "abstract": "Graph Contrastive Learning (GCL) has emerged as a powerful tool for\nextracting consistent representations from graphs, independent of labeled\ninformation. However, existing methods predominantly focus on undirected\ngraphs, disregarding the pivotal directional information that is fundamental\nand indispensable in real-world networks (e.g., social networks and\nrecommendations).In this paper, we introduce S2-DiGCL, a novel framework that\nemphasizes spatial insights from complex and real domain perspectives for\ndirected graph (digraph) contrastive learning. From the complex-domain\nperspective, S2-DiGCL introduces personalized perturbations into the magnetic\nLaplacian to adaptively modulate edge phases and directional semantics. From\nthe real-domain perspective, it employs a path-based subgraph augmentation\nstrategy to capture fine-grained local asymmetries and topological\ndependencies. By jointly leveraging these two complementary spatial views,\nS2-DiGCL constructs high-quality positive and negative samples, leading to more\ngeneral and robust digraph contrastive learning. Extensive experiments on 7\nreal-world digraph datasets demonstrate the superiority of our approach,\nachieving SOTA performance with 4.41% improvement in node classification and\n4.34% in link prediction under both supervised and unsupervised settings.",
      "authors": [
        {
          "name": "Daohan Su",
          "affiliation": null
        },
        {
          "name": "Yang Zhang",
          "affiliation": null
        },
        {
          "name": "Xunkai Li",
          "affiliation": null
        },
        {
          "name": "Rong-Hua Li",
          "affiliation": null
        },
        {
          "name": "Guoren Wang",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16311v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16311v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16310v1",
      "title": "Lung Cancer Classification from CT Images Using ResNet",
      "abstract": "Lung cancer, a malignancy originating in lung tissues, is commonly diagnosed\nand classified using medical imaging techniques, particularly computed\ntomography (CT). Despite the integration of machine learning and deep learning\nmethods, the predictive efficacy of automated systems for lung cancer\nclassification from CT images remains below the desired threshold for clinical\nadoption. Existing research predominantly focuses on binary classification,\ndistinguishing between malignant and benign lung nodules. In this study, a\nnovel deep learning-based approach is introduced, aimed at an improved\nmulti-class classification, discerning various subtypes of lung cancer from CT\nimages. Leveraging a pre-trained ResNet model, lung tissue images were\nclassified into three distinct classes, two of which denote malignancy and one\nbenign. Employing a dataset comprising 15,000 lung CT images sourced from the\nLC25000 histopathological images, the ResNet50 model was trained on 10,200\nimages, validated on 2,550 images, and tested on the remaining 2,250 images.\nThrough the incorporation of custom layers atop the ResNet architecture and\nmeticulous hyperparameter fine-tuning, a remarkable test accuracy of 98.8% was\nrecorded. This represents a notable enhancement over the performance of prior\nmodels on the same dataset.",
      "authors": [
        {
          "name": "Olajumoke O. Adekunle",
          "affiliation": null
        },
        {
          "name": "Joseph D. Akinyemi",
          "affiliation": null
        },
        {
          "name": "Khadijat T. Ladoja",
          "affiliation": null
        },
        {
          "name": "Olufade F. W. Onifade",
          "affiliation": null
        }
      ],
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "I.4.0; I.4.9"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16310v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16310v1",
      "primary_category": "eess.IV",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16309v1",
      "title": "MedRule-KG: A Knowledge-Graph--Steered Scaffold for Mathematical Reasoning with a Lightweight Verifier",
      "abstract": "Large language models (LLMs) often produce fluent reasoning steps while\nviolating simple mathematical or logical constraints. We introduce MedRule-KG,\na compact typed knowledge graph coupled with a symbolic verifier, designed to\nenforce mathematically interpretable rules in reasoning tasks. MedRule-KG\nencodes entities, relations, and three domain-inspired rules, while the\nverifier checks predictions and applies minimal corrections to guarantee\nconsistency. On a 90-example FDA-derived benchmark, grounding in MedRule-KG\nimproves exact match (EM) from 0.767 to 0.900, and adding the verifier yields\n1.000 EM while eliminating rule violations entirely. We demonstrate how\nMedRule-KG provides a general scaffold for safe mathematical reasoning, discuss\nablations, and release code and data to encourage reproducibility.",
      "authors": [
        {
          "name": "Crystal Su",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.AI"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16309v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16309v1",
      "primary_category": "cs.AI",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16306v1",
      "title": "Scaffold-Aware Generative Augmentation and Reranking for Enhanced Virtual Screening",
      "abstract": "Ligand-based virtual screening (VS) is an essential step in drug discovery\nthat evaluates large chemical libraries to identify compounds that potentially\nbind to a therapeutic target. However, VS faces three major challenges: class\nimbalance due to the low active rate, structural imbalance among active\nmolecules where certain scaffolds dominate, and the need to identify\nstructurally diverse active compounds for novel drug development. We introduce\nScaffAug, a scaffold-aware VS framework that addresses these challenges through\nthree modules. The augmentation module first generates synthetic data\nconditioned on scaffolds of actual hits using generative AI, specifically a\ngraph diffusion model. This helps mitigate the class imbalance and furthermore\nthe structural imbalance, due to our proposed scaffold-aware sampling\nalgorithm, designed to produce more samples for active molecules with\nunderrepresented scaffolds. A model-agnostic self-training module is then used\nto safely integrate the generated synthetic data from our augmentation module\nwith the original labeled data. Lastly, we introduce a reranking module that\nimproves VS by enhancing scaffold diversity in the top recommended set of\nmolecules, while still maintaining and even enhancing the overall general\nperformance of identifying novel, active compounds. We conduct comprehensive\ncomputational experiments across five target classes, comparing ScaffAug\nagainst existing baseline methods by reporting the performance of multiple\nevaluation metrics and performing ablation studies on ScaffAug. Overall, this\nwork introduces novel perspectives on effectively enhancing VS by leveraging\ngenerative augmentations, reranking, and general scaffold-awareness.",
      "authors": [
        {
          "name": "Xin Wang",
          "affiliation": null
        },
        {
          "name": "Yu Wang",
          "affiliation": null
        },
        {
          "name": "Yunchao Liu",
          "affiliation": null
        },
        {
          "name": "Jens Meiler",
          "affiliation": null
        },
        {
          "name": "Tyler Derr",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16306v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16306v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16302v1",
      "title": "DTKG: Dual-Track Knowledge Graph-Verified Reasoning Framework for Multi-Hop QA",
      "abstract": "Multi-hop reasoning for question answering (QA) plays a critical role in\nretrieval-augmented generation (RAG) for modern large language models (LLMs).\nThe accurate answer can be obtained through retrieving relational structure of\nentities from knowledge graph (KG). Regarding the inherent relation-dependency\nand reasoning pattern, multi-hop reasoning can be in general classified into\ntwo categories: i) parallel fact-verification multi-hop reasoning question,\ni.e., requiring simultaneous verifications of multiple independent\nsub-questions; and ii) chained multi-hop reasoning questions, i.e., demanding\nsequential multi-step inference with intermediate conclusions serving as\nessential premises for subsequent reasoning. Currently, the multi-hop reasoning\napproaches singly employ one of two techniques: LLM response-based fact\nverification and KG path-based chain construction. Nevertheless, the former\nexcels at parallel fact-verification but underperforms on chained reasoning\ntasks, while the latter demonstrates proficiency in chained multi-hop reasoning\nbut suffers from redundant path retrieval when handling parallel\nfact-verification reasoning. These limitations deteriorate the efficiency and\naccuracy for multi-hop QA tasks. To address this challenge, we propose a novel\ndual-track KG verification and reasoning framework DTKG, which is inspired by\nthe Dual Process Theory in cognitive science. Specifically, DTKG comprises two\nmain stages: the Classification Stage and the Branch Processing Stage.",
      "authors": [
        {
          "name": "Changhao Wang",
          "affiliation": null
        },
        {
          "name": "Yanfang Liu",
          "affiliation": null
        },
        {
          "name": "Xinxin Fan",
          "affiliation": null
        },
        {
          "name": "Anzhi Zhou",
          "affiliation": null
        },
        {
          "name": "Lao Tian",
          "affiliation": null
        },
        {
          "name": "Yunfeng Lu",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.AI",
        "cs.IR"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16302v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16302v1",
      "primary_category": "cs.AI",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16295v1",
      "title": "OpenLVLM-MIA: A Controlled Benchmark Revealing the Limits of Membership Inference Attacks on Large Vision-Language Models",
      "abstract": "OpenLVLM-MIA is a new benchmark that highlights fundamental challenges in\nevaluating membership inference attacks (MIA) against large vision-language\nmodels (LVLMs). While prior work has reported high attack success rates, our\nanalysis suggests that these results often arise from detecting distributional\nbias introduced during dataset construction rather than from identifying true\nmembership status. To address this issue, we introduce a controlled benchmark\nof 6{,}000 images where the distributions of member and non-member samples are\ncarefully balanced, and ground-truth membership labels are provided across\nthree distinct training stages. Experiments using OpenLVLM-MIA demonstrated\nthat the performance of state-of-the-art MIA methods converged to random chance\nunder unbiased conditions. By offering a transparent and unbiased benchmark,\nOpenLVLM-MIA clarifies the current limitations of MIA research on LVLMs and\nprovides a solid foundation for developing stronger privacy-preserving\ntechniques.",
      "authors": [
        {
          "name": "Ryoto Miyamoto",
          "affiliation": null
        },
        {
          "name": "Xin Fan",
          "affiliation": null
        },
        {
          "name": "Fuyuko Kido",
          "affiliation": null
        },
        {
          "name": "Tsuneo Matsumoto",
          "affiliation": null
        },
        {
          "name": "Hayato Yamana",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16295v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16295v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16293v1",
      "title": "Synergizing chemical and AI communities for advancing laboratories of the future",
      "abstract": "The development of automated experimental facilities and the digitization of\nexperimental data have introduced numerous opportunities to radically advance\nchemical laboratories. As many laboratory tasks involve predicting and\nunderstanding previously unknown chemical relationships, machine learning (ML)\napproaches trained on experimental data can substantially accelerate the\nconventional design-build-test-learn process. This outlook article aims to help\nchemists understand and begin to adopt ML predictive models for a variety of\nlaboratory tasks, including experimental design, synthesis optimization, and\nmaterials characterization. Furthermore, this article introduces how artificial\nintelligence (AI) agents based on large language models can help researchers\nacquire background knowledge in chemical or data science and accelerate various\naspects of the discovery process. We present three case studies in distinct\nareas to illustrate how ML models and AI agents can be leveraged to reduce\ntime-consuming experiments and manual data analysis. Finally, we highlight\nexisting challenges that require continued synergistic effort from both\nexperimental and computational communities to address.",
      "authors": [
        {
          "name": "Saejin Oh",
          "affiliation": null
        },
        {
          "name": "Xinyi Fang",
          "affiliation": null
        },
        {
          "name": "I-Hsin Lin",
          "affiliation": null
        },
        {
          "name": "Paris Dee",
          "affiliation": null
        },
        {
          "name": "Christopher S. Dunham",
          "affiliation": null
        },
        {
          "name": "Stacy M. Copp",
          "affiliation": null
        },
        {
          "name": "Abigail G. Doyle",
          "affiliation": null
        },
        {
          "name": "Javier Read de Alaniz",
          "affiliation": null
        },
        {
          "name": "Mengyang Gu",
          "affiliation": null
        }
      ],
      "categories": [
        "stat.AP",
        "cs.AI",
        "cs.LG"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16293v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16293v1",
      "primary_category": "stat.AP",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16292v1",
      "title": "QSVD: Efficient Low-rank Approximation for Unified Query-Key-Value Weight Compression in Low-Precision Vision-Language Models",
      "abstract": "Vision-Language Models (VLMs) are integral to tasks such as image captioning\nand visual question answering, but their high computational cost, driven by\nlarge memory footprints and processing time, limits their scalability and\nreal-time applicability. In this work, we propose leveraging Singular-Value\nDecomposition (SVD) over the joint query (Q), key (K), and value (V) weight\nmatrices to reduce KV cache size and computational overhead. We in addition\nintroduce an efficient rank allocation strategy that dynamically adjusts the\nSVD rank based on its impact on VLM accuracy, achieving a significant reduction\nin both memory usage and computational cost. Finally, we extend this approach\nby applying quantization to both VLM weights and activations, resulting in a\nhighly efficient VLM. Our method outperforms previous approaches that rely\nsolely on quantization or SVD by achieving more than $10\\%$ accuracy\nimprovement while consuming less hardware cost, making it better for real-time\ndeployment on resource-constrained devices. We open source our code at\n\\href{https://github.com/SAI-Lab-NYU/QSVD}{\\texttt{https://github.com/SAI-Lab-NYU/QSVD}}.",
      "authors": [
        {
          "name": "Yutong Wang",
          "affiliation": null
        },
        {
          "name": "Haiyu Wang",
          "affiliation": null
        },
        {
          "name": "Sai Qian Zhang",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16292v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16292v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16290v1",
      "title": "Cerberus: Real-Time Video Anomaly Detection via Cascaded Vision-Language Models",
      "abstract": "Video anomaly detection (VAD) has rapidly advanced by recent development of\nVision-Language Models (VLMs). While these models offer superior zero-shot\ndetection capabilities, their immense computational cost and unstable visual\ngrounding performance hinder real-time deployment. To overcome these\nchallenges, we introduce Cerberus, a two-stage cascaded system designed for\nefficient yet accurate real-time VAD. Cerberus learns normal behavioral rules\noffline, and combines lightweight filtering with fine-grained VLM reasoning\nduring online inference. The performance gains of Cerberus come from two key\ninnovations: motion mask prompting and rule-based deviation detection. The\nformer directs the VLM's attention to regions relevant to motion, while the\nlatter identifies anomalies as deviations from learned norms rather than\nenumerating possible anomalies. Extensive evaluations on four datasets show\nthat Cerberus on average achieves 57.68 fps on an NVIDIA L40S GPU, a\n151.79$\\times$ speedup, and 97.2\\% accuracy comparable to the state-of-the-art\nVLM-based VAD methods, establishing it as a practical solution for real-time\nvideo analytics.",
      "authors": [
        {
          "name": "Yue Zheng",
          "affiliation": null
        },
        {
          "name": "Xiufang Shi",
          "affiliation": null
        },
        {
          "name": "Jiming Chen",
          "affiliation": null
        },
        {
          "name": "Yuanchao Shu",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CV",
        "cs.CL"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16290v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16290v1",
      "primary_category": "cs.CV",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16289v1",
      "title": "Disentangling Hyperedges through the Lens of Category Theory",
      "abstract": "Despite the promising results of disentangled representation learning in\ndiscovering latent patterns in graph-structured data, few studies have explored\ndisentanglement for hypergraph-structured data. Integrating hyperedge\ndisentanglement into hypergraph neural networks enables models to leverage\nhidden hyperedge semantics, such as unannotated relations between nodes, that\nare associated with labels. This paper presents an analysis of hyperedge\ndisentanglement from a category-theoretical perspective and proposes a novel\ncriterion for disentanglement derived from the naturality condition. Our\nproof-of-concept model experimentally showed the potential of the proposed\ncriterion by successfully capturing functional relations of genes (nodes) in\ngenetic pathways (hyperedges).",
      "authors": [
        {
          "name": "Yoonho Lee",
          "affiliation": null
        },
        {
          "name": "Junseok Lee",
          "affiliation": null
        },
        {
          "name": "Sangwoo Seo",
          "affiliation": null
        },
        {
          "name": "Sungwon Kim",
          "affiliation": null
        },
        {
          "name": "Yeongmin Kim",
          "affiliation": null
        },
        {
          "name": "Chanyoung Park",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16289v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16289v1",
      "primary_category": "cs.LG",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16282v1",
      "title": "Instant Personalized Large Language Model Adaptation via Hypernetwork",
      "abstract": "Personalized large language models (LLMs) tailor content to individual\npreferences using user profiles or histories. However, existing\nparameter-efficient fine-tuning (PEFT) methods, such as the\n``One-PEFT-Per-User'' (OPPU) paradigm, require training a separate adapter for\neach user, making them computationally expensive and impractical for real-time\nupdates. We introduce Profile-to-PEFT, a scalable framework that employs a\nhypernetwork, trained end-to-end, to map a user's encoded profile directly to a\nfull set of adapter parameters (e.g., LoRA), eliminating per-user training at\ndeployment. This design enables instant adaptation, generalization to unseen\nusers, and privacy-preserving local deployment. Experimental results\ndemonstrate that our method outperforms both prompt-based personalization and\nOPPU while using substantially fewer computational resources at deployment. The\nframework exhibits strong generalization to out-of-distribution users and\nmaintains robustness across varying user activity levels and different\nembedding backbones. The proposed Profile-to-PEFT framework enables efficient,\nscalable, and adaptive LLM personalization suitable for large-scale\napplications.",
      "authors": [
        {
          "name": "Zhaoxuan Tan",
          "affiliation": null
        },
        {
          "name": "Zixuan Zhang",
          "affiliation": null
        },
        {
          "name": "Haoyang Wen",
          "affiliation": null
        },
        {
          "name": "Zheng Li",
          "affiliation": null
        },
        {
          "name": "Rongzhi Zhang",
          "affiliation": null
        },
        {
          "name": "Pei Chen",
          "affiliation": null
        },
        {
          "name": "Fengran Mo",
          "affiliation": null
        },
        {
          "name": "Zheyuan Liu",
          "affiliation": null
        },
        {
          "name": "Qingkai Zeng",
          "affiliation": null
        },
        {
          "name": "Qingyu Yin",
          "affiliation": null
        },
        {
          "name": "Meng Jiang",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.CL"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16282v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16282v1",
      "primary_category": "cs.CL",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16281v1",
      "title": "Do What You Say: Steering Vision-Language-Action Models via Runtime Reasoning-Action Alignment Verification",
      "abstract": "Reasoning Vision Language Action (VLA) models improve robotic\ninstruction-following by generating step-by-step textual plans before low-level\nactions, an approach inspired by Chain-of-Thought (CoT) reasoning in language\nmodels. Yet even with a correct textual plan, the generated actions can still\nmiss the intended outcomes in the plan, especially in out-of-distribution (OOD)\nscenarios. We formalize this phenomenon as a lack of embodied CoT faithfulness,\nand introduce a training-free, runtime policy steering method for\nreasoning-action alignment. Given a reasoning VLA's intermediate textual plan,\nour framework samples multiple candidate action sequences from the same model,\npredicts their outcomes via simulation, and uses a pre-trained Vision-Language\nModel (VLM) to select the sequence whose outcome best aligns with the VLA's own\ntextual plan. Only executing action sequences that align with the textual\nreasoning turns our base VLA's natural action diversity from a source of error\ninto a strength, boosting robustness to semantic and visual OOD perturbations\nand enabling novel behavior composition without costly re-training. We also\ncontribute a reasoning-annotated extension of LIBERO-100, environment\nvariations tailored for OOD evaluation, and demonstrate up to 15% performance\ngain over prior work on behavior composition tasks and scales with compute and\ndata diversity. Project Website at:\nhttps://yilin-wu98.github.io/steering-reasoning-vla/",
      "authors": [
        {
          "name": "Yilin Wu",
          "affiliation": null
        },
        {
          "name": "Anqi Li",
          "affiliation": null
        },
        {
          "name": "Tucker Hermans",
          "affiliation": null
        },
        {
          "name": "Fabio Ramos",
          "affiliation": null
        },
        {
          "name": "Andrea Bajcsy",
          "affiliation": null
        },
        {
          "name": "Claudia P'erez-D'Arpino",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16281v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16281v1",
      "primary_category": "cs.RO",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16276v1",
      "title": "What Limits Agentic Systems Efficiency?",
      "abstract": "Large Language Models (LLMs), such as OpenAI-o1 and DeepSeek-R1, have\ndemonstrated strong reasoning capabilities. To further enhance LLM\ncapabilities, recent agentic systems, such as Deep Research, incorporate web\ninteractions into LLM reasoning to mitigate uncertainties and reduce potential\nerrors. However, existing research predominantly focuses on reasoning\nperformance, often neglecting the efficiency of agentic systems. In this work,\nwe present a comprehensive empirical study that identifies efficiency\nbottlenecks in web-interactive agentic systems. We decompose end-to-end latency\ninto two primary components: LLM API latency and web environment latency. We\nconduct a comprehensive empirical study across 15 models and 5 providers to\ndemonstrate high variability in API-based agentic systems. We observe that web\nenvironment latency can contribute as much as 53.7% to the overall latency in a\nweb-based agentic system. To improve latency, we propose SpecCache, a caching\nframework augmented with speculative execution that can reduce web environment\noverhead. Extensive evaluations on two standard benchmarks show that our\napproach improves the cache hit rate by up to 58x compared to a random caching\nstrategy, while reducing web environment overhead by up to 3.2x, without\ndegrading agentic system performance.",
      "authors": [
        {
          "name": "Song Bian",
          "affiliation": null
        },
        {
          "name": "Minghao Yan",
          "affiliation": null
        },
        {
          "name": "Anand Jayarajan",
          "affiliation": null
        },
        {
          "name": "Gennady Pekhimenko",
          "affiliation": null
        },
        {
          "name": "Shivaram Venkataraman",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16276v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16276v1",
      "primary_category": "cs.AI",
      "updated_date": "2025-10-18"
    },
    {
      "arxiv_id": "2510.16273v1",
      "title": "MuseTok: Symbolic Music Tokenization for Generation and Semantic Understanding",
      "abstract": "Discrete representation learning has shown promising results across various\ndomains, including generation and understanding in image, speech and language.\nInspired by these advances, we propose MuseTok, a tokenization method for\nsymbolic music, and investigate its effectiveness in both music generation and\nunderstanding tasks. MuseTok employs the residual vector quantized-variational\nautoencoder (RQ-VAE) on bar-wise music segments within a Transformer-based\nencoder-decoder framework, producing music codes that achieve high-fidelity\nmusic reconstruction and accurate understanding of music theory. For\ncomprehensive evaluation, we apply MuseTok to music generation and semantic\nunderstanding tasks, including melody extraction, chord recognition, and\nemotion recognition. Models incorporating MuseTok outperform previous\nrepresentation learning baselines in semantic understanding while maintaining\ncomparable performance in content generation. Furthermore, qualitative analyses\non MuseTok codes, using ground-truth categories and synthetic datasets, reveal\nthat MuseTok effectively captures underlying musical concepts from large music\ncollections.",
      "authors": [
        {
          "name": "Jingyue Huang",
          "affiliation": null
        },
        {
          "name": "Zachary Novack",
          "affiliation": null
        },
        {
          "name": "Phillip Long",
          "affiliation": null
        },
        {
          "name": "Yupeng Hou",
          "affiliation": null
        },
        {
          "name": "Ke Chen",
          "affiliation": null
        },
        {
          "name": "Taylor Berg-Kirkpatrick",
          "affiliation": null
        },
        {
          "name": "Julian McAuley",
          "affiliation": null
        }
      ],
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "published_date": "2025-10-18",
      "arxiv_url": "http://arxiv.org/abs/2510.16273v1",
      "pdf_url": "http://arxiv.org/pdf/2510.16273v1",
      "primary_category": "cs.SD",
      "updated_date": "2025-10-18"
    }
  ]
}