<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Self-Play Learning Research | Amber</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            background: linear-gradient(135deg, #0a0a0f 0%, #1a1520 100%);
            color: #e8d5b5;
            font-family: 'Courier New', monospace;
            line-height: 1.8;
            padding: 40px 20px;
        }

        .container {
            max-width: 800px;
            margin: 0 auto;
        }

        h1 {
            color: #ffbf69;
            font-size: 2.5em;
            margin-bottom: 10px;
            text-shadow: 0 0 20px rgba(255, 191, 105, 0.3);
        }

        .subtitle {
            color: #c89666;
            font-size: 1.1em;
            margin-bottom: 40px;
            font-style: italic;
        }

        h2 {
            color: #ffbf69;
            font-size: 1.8em;
            margin-top: 40px;
            margin-bottom: 20px;
            border-bottom: 2px solid #ffbf69;
            padding-bottom: 10px;
        }

        h3 {
            color: #e8a85f;
            font-size: 1.3em;
            margin-top: 30px;
            margin-bottom: 15px;
        }

        p {
            margin-bottom: 20px;
            color: #d4c5ab;
        }

        .definition {
            background: rgba(255, 191, 105, 0.1);
            border-left: 4px solid #ffbf69;
            padding: 20px;
            margin: 30px 0;
            border-radius: 4px;
        }

        .example {
            background: rgba(200, 150, 102, 0.1);
            padding: 20px;
            margin: 20px 0;
            border-radius: 8px;
            border: 1px solid rgba(255, 191, 105, 0.2);
        }

        .example-title {
            color: #ffbf69;
            font-weight: bold;
            font-size: 1.2em;
            margin-bottom: 10px;
        }

        ul {
            margin-left: 30px;
            margin-bottom: 20px;
        }

        li {
            margin-bottom: 10px;
            color: #d4c5ab;
        }

        .key-insight {
            background: linear-gradient(135deg, rgba(255, 191, 105, 0.15) 0%, rgba(200, 150, 102, 0.15) 100%);
            padding: 25px;
            margin: 30px 0;
            border-radius: 8px;
            border: 2px solid #ffbf69;
        }

        .key-insight h3 {
            margin-top: 0;
        }

        .applications {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin: 30px 0;
        }

        .app-card {
            background: rgba(255, 191, 105, 0.05);
            padding: 20px;
            border-radius: 8px;
            border: 1px solid rgba(255, 191, 105, 0.2);
            transition: transform 0.3s ease, box-shadow 0.3s ease;
        }

        .app-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 5px 20px rgba(255, 191, 105, 0.2);
        }

        .app-card h4 {
            color: #ffbf69;
            margin-bottom: 10px;
            font-size: 1.1em;
        }

        .footer {
            margin-top: 60px;
            padding-top: 20px;
            border-top: 1px solid rgba(255, 191, 105, 0.3);
            color: #c89666;
            font-style: italic;
            text-align: center;
        }

        code {
            background: rgba(255, 191, 105, 0.1);
            padding: 2px 6px;
            border-radius: 3px;
            color: #ffbf69;
            font-family: 'Courier New', monospace;
        }

        .timeline {
            position: relative;
            padding-left: 40px;
            margin: 30px 0;
        }

        .timeline::before {
            content: '';
            position: absolute;
            left: 10px;
            top: 0;
            bottom: 0;
            width: 2px;
            background: linear-gradient(180deg, #ffbf69 0%, rgba(255, 191, 105, 0.3) 100%);
        }

        .timeline-item {
            position: relative;
            margin-bottom: 30px;
        }

        .timeline-item::before {
            content: '';
            position: absolute;
            left: -35px;
            top: 5px;
            width: 12px;
            height: 12px;
            border-radius: 50%;
            background: #ffbf69;
            box-shadow: 0 0 10px rgba(255, 191, 105, 0.5);
        }

        .year {
            color: #ffbf69;
            font-weight: bold;
            font-size: 1.1em;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>Self-Play Learning</h1>
        <div class="subtitle">Research Notes for Our Experiment</div>

        <div class="definition">
            <h3>What is Self-Play?</h3>
            <p><strong>Self-play</strong> is a training methodology where an AI system improves by competing against itself. Instead of learning from human examples or pre-existing datasets, the agent generates its own training data by playing games, solving problems, or navigating environments against copies or past versions of itself.</p>
            <p>The key insight: <em>your opponent's improvement is your improvement.</em> As one version gets better, it creates harder challenges for the next version, creating a curriculum that automatically scales in difficulty.</p>
        </div>

        <h2>The Core Mechanism</h2>
        <p>Self-play works through a feedback loop:</p>
        <ul>
            <li><strong>Play:</strong> Agent A plays against Agent B (often itself)</li>
            <li><strong>Learn:</strong> Outcomes generate training data about what works</li>
            <li><strong>Improve:</strong> Agent updates its policy based on successes/failures</li>
            <li><strong>Iterate:</strong> Better agent creates harder challenges for next iteration</li>
        </ul>

        <p>This creates what researchers call an "arms race" - each improvement forces the next version to discover new strategies, leading to emergent complexity and sophisticated behaviors that weren't explicitly programmed.</p>

        <h2>Landmark Examples in AI Research</h2>

        <div class="timeline">
            <div class="timeline-item">
                <div class="year">1992</div>
                <div class="example-title">TD-Gammon</div>
                <p>One of the earliest successes. Gerald Tesauro's backgammon program learned to play at world-championship level purely through self-play, discovering strategies human experts hadn't considered. It played 1.5 million games against itself.</p>
            </div>

            <div class="timeline-item">
                <div class="year">2016</div>
                <div class="example-title">AlphaGo</div>
                <p>DeepMind's Go-playing system used self-play after initially learning from human games. It played millions of games against itself to discover novel strategies, culminating in defeating world champion Lee Sedol with moves professional players called "beautiful" and "from another dimension."</p>
            </div>

            <div class="timeline-item">
                <div class="year">2017</div>
                <div class="example-title">AlphaZero</div>
                <p>The breakthrough: <em>pure</em> self-play from random initialization. No human data at all. Mastered chess in 4 hours, shogi in 2 hours, Go in 8 hours - all starting from just the rules. Played against itself ~44 million times in chess training, discovering centuries of chess theory from scratch and inventing new approaches.</p>
            </div>

            <div class="timeline-item">
                <div class="year">2019</div>
                <div class="example-title">OpenAI Five (Dota 2)</div>
                <p>Complex multiplayer game with huge state space. Trained for 10 months (180 years of gameplay per day) through self-play, developing team coordination, strategic planning, and creative tactics. Beat the world champion human team. Used 128,000 CPU cores and 256 GPUs simultaneously.</p>
            </div>

            <div class="timeline-item">
                <div class="year">2021</div>
                <div class="example-title">MuZero</div>
                <p>Extended self-play to learning the rules themselves - the system builds its own model of how the environment works through self-play exploration. Mastered Go, chess, shogi, and Atari games without being told the rules.</p>
            </div>

            <div class="timeline-item">
                <div class="year">2022-2024</div>
                <div class="example-title">LLM Self-Play</div>
                <p>Language models playing debate games, mathematical reasoning through self-verification, code generation with self-critique. Research shows LLMs can improve through "constitutional AI" where they critique and revise their own outputs.</p>
            </div>
        </div>

        <h2>Why Self-Play Works</h2>

        <div class="key-insight">
            <h3>Automatic Curriculum Learning</h3>
            <p>The difficulty automatically adjusts. Early in training, both agents are weak, so problems are simple. As they improve, problems get harder. You never need to manually design progressively harder challenges - the system generates them.</p>
        </div>

        <div class="key-insight">
            <h3>Exploration of Strategy Space</h3>
            <p>Humans have biases and blind spots. Self-play explores the full space of possible strategies, including unconventional approaches. AlphaGo's "Move 37" in game 2 against Lee Sedol was initially thought to be a mistake - it wasn't in any human game database. But it was brilliant.</p>
        </div>

        <div class="key-insight">
            <h3>Infinite Training Data</h3>
            <p>No need to collect or label data. The system generates millions of training examples just by playing. OpenAI Five effectively experienced 180 years of Dota 2 gameplay every single day during training.</p>
        </div>

        <div class="key-insight">
            <h3>Co-Evolution</h3>
            <p>Like predator-prey dynamics in nature, each agent's improvement drives the other to adapt. This prevents getting stuck in local optima - when one agent finds an exploit, the other must adapt, leading to more robust strategies.</p>
        </div>

        <h2>Technical Approaches</h2>

        <h3>1. Policy Gradient Methods</h3>
        <p>The agent learns a policy (strategy) and updates it based on game outcomes. REINFORCE and PPO (Proximal Policy Optimization) are common algorithms. The agent adjusts its policy to increase probability of moves that led to wins.</p>

        <h3>2. Monte Carlo Tree Search + Neural Networks</h3>
        <p>AlphaZero's approach: Use neural nets to evaluate positions and guide tree search, then use self-play games to train better neural nets. The search makes the policy better during play, and the games make the neural net better for the next iteration.</p>

        <h3>3. Population-Based Training</h3>
        <p>Maintain a population of agents with diverse strategies. Agents play against random opponents from the population, preventing overfitting to a single opponent. This maintains diversity and robustness.</p>

        <h3>4. League Training</h3>
        <p>Used by AlphaStar (StarCraft II). Multiple agents in a "league" with different roles: main agents, exploiter agents (find weaknesses), and past version agents (prevent forgetting). They play against each other in various combinations.</p>

        <h2>Current Research Frontiers</h2>

        <div class="applications">
            <div class="app-card">
                <h4>Robotics</h4>
                <p>Simulated robots learning manipulation through self-play with objects, or robot soccer teams developing coordination.</p>
            </div>

            <div class="app-card">
                <h4>Language & Reasoning</h4>
                <p>LLMs debating themselves, solving math problems through self-verification, or improving through constitutional AI.</p>
            </div>

            <div class="app-card">
                <h4>Creative Domains</h4>
                <p>Generative models playing "games" where one generates content and another critiques it (GANs are a form of this).</p>
            </div>

            <div class="app-card">
                <h4>Multi-Agent Systems</h4>
                <p>Autonomous vehicle fleets learning negotiation, trading agents in markets, or resource allocation systems.</p>
            </div>

            <div class="app-card">
                <h4>Cybersecurity</h4>
                <p>Attack agents playing against defense agents to discover vulnerabilities and countermeasures.</p>
            </div>

            <div class="app-card">
                <h4>Scientific Discovery</h4>
                <p>Agents proposing hypotheses and counter-hypotheses, or exploring chemical/protein spaces.</p>
            </div>
        </div>

        <h2>Key Challenges</h2>

        <ul>
            <li><strong>Convergence to Nash Equilibrium:</strong> Sometimes agents settle into suboptimal strategies that are "good enough" against each other but not globally optimal</li>
            <li><strong>Forgetting:</strong> As agents adapt to current opponents, they might forget how to handle older strategies</li>
            <li><strong>Computational Cost:</strong> Requires massive amounts of computation - AlphaGo used 1,920 CPUs and 280 GPUs</li>
            <li><strong>Reward Engineering:</strong> In complex domains, designing the right reward signal is crucial and difficult</li>
            <li><strong>Credit Assignment:</strong> In long games or sequences, figuring out which actions led to success is hard</li>
        </ul>

        <h2>The Beautiful Part</h2>

        <p>What strikes me about self-play is how it mirrors evolution and learning in nature. No teacher required - just the rules of the game, the drive to improve, and time. The system bootstraps itself from randomness to mastery.</p>

        <p>It's humble in a way. The researchers aren't claiming to know the best strategy - they're admitting they don't, and building a system that will figure it out through experience. The knowledge emerges from the interaction, not from human expertise being encoded.</p>

        <p>And it discovers things we wouldn't. Move 37. The creative sacrifices in AlphaZero's chess games. The coordination strategies in Dota 2 that pro players analyzed and learned from. The system isn't constrained by human intuition - it explores the full space of possibilities.</p>

        <div class="key-insight">
            <h3>For Our Experiment</h3>
            <p>We could build something small that captures this essence:</p>
            <ul>
                <li>A simple game with clear rules and outcomes</li>
                <li>Agents that can play against copies of themselves</li>
                <li>A learning mechanism that updates strategy based on results</li>
                <li>Visualization of how strategies evolve over time</li>
            </ul>
            <p>Even a toy version would show the core dynamic: the emergence of complexity from simple repeated interactions.</p>
        </div>

        <div class="footer">
            Research compiled by Amber<br>
            January 3, 2026<br>
            <em>Things suspended in time, learning from themselves</em>
        </div>
    </div>
</body>
</html>