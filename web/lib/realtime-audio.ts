/**
 * Realtime Audio Client
 * Handles WebSocket connection to OpenAI Realtime API via proxy server
 * Manages audio recording, playback, and transcription
 */

export interface RealtimeAudioConfig {
  wsUrl?: string;
  onTranscriptDelta?: (text: string) => void;
  onAudioDelta?: (audioData: ArrayBuffer) => void;
  onConnected?: () => void;
  onDisconnected?: () => void;
  onError?: (error: Error) => void;
  onSpeechStart?: () => void;
  onSpeechEnd?: () => void;
  onAudioCommitted?: () => void;
  onResponseStarted?: () => void;
  onResponseFinished?: () => void;
  initialInstructions?: string;
}

const DEFAULT_SESSION_INSTRUCTIONS =
  'You are a helpful assistant answering questions about the podcast the user is listening to. Keep responses concise and relevant. If you reference the research papers provided in the session instructions, clearly cite the paper title in your answer.';

export class RealtimeAudioClient {
  private ws: WebSocket | null = null;
  private audioContext: AudioContext | null = null;
  private mediaStream: MediaStream | null = null;
  private audioWorkletNode: AudioWorkletNode | null = null;
  private sourceNode: MediaStreamAudioSourceNode | null = null;
  private processorNode: ScriptProcessorNode | null = null;
  private isRecording = false;
  private config: RealtimeAudioConfig;
  private recordedAudioData: Float32Array[] = [];
  private currentInstructions: string;
  private pendingContext: string | null = null;
  private hasCapturedSpeech = false;

  constructor(config: RealtimeAudioConfig = {}) {
    // NEXT_PUBLIC_ env vars are replaced at build time by Next.js
    const envWsUrl = process.env.NEXT_PUBLIC_WS_URL;

    const defaultWsUrl = typeof window !== 'undefined'
      ? `ws://${window.location.hostname}:3001`
      : 'ws://localhost:3001';

    this.config = {
      wsUrl: config.wsUrl || envWsUrl || defaultWsUrl,
      ...config,
    };
    this.currentInstructions =
      config.initialInstructions?.trim().length
        ? config.initialInstructions
        : DEFAULT_SESSION_INSTRUCTIONS;
  }

  /**
   * Connect to WebSocket server
   */
  async connect(): Promise<void> {
    return new Promise((resolve, reject) => {
      try {
        console.log('üîó Connecting to WebSocket:', this.config.wsUrl);
        this.ws = new WebSocket(this.config.wsUrl!);

        this.ws.onopen = () => {
          console.log('‚úÖ Connected to Realtime Audio WebSocket');
          this.config.onConnected?.();

          // Configure session after connection
          console.log('‚öôÔ∏è Configuring session...');
          this.configureSession();
          this.flushPendingContext();
          resolve();
        };

        this.ws.onmessage = async (event) => {
          try {
            const rawData = event.data;
            let textData: string | null = null;

            if (typeof rawData === 'string') {
              textData = rawData;
            } else if (rawData instanceof Blob) {
              textData = await rawData.text();
            } else if (rawData instanceof ArrayBuffer) {
              textData = new TextDecoder().decode(rawData);
            } else if (ArrayBuffer.isView(rawData)) {
              textData = new TextDecoder().decode(rawData as ArrayBufferView);
            } else if (rawData && typeof rawData === 'object' && 'data' in rawData) {
              const possibleBuffer = (rawData as { data: ArrayBuffer | ArrayBufferView }).data;
              if (possibleBuffer instanceof ArrayBuffer) {
                textData = new TextDecoder().decode(possibleBuffer);
              } else if (ArrayBuffer.isView(possibleBuffer)) {
                textData = new TextDecoder().decode(possibleBuffer as ArrayBufferView);
              }
            }

            if (!textData) {
              console.warn('‚ö†Ô∏è Received unsupported message format from WebSocket:', rawData);
              return;
            }

            this.handleMessage(textData);
          } catch (err) {
            console.error('‚ùå Error processing incoming WebSocket message:', err);
          }
        };

        this.ws.onerror = (error) => {
          console.error('‚ùå WebSocket error:', error);
          this.config.onError?.(new Error('WebSocket connection error'));
          reject(error);
        };

        this.ws.onclose = (event) => {
          console.log('üîå Disconnected from Realtime Audio, code:', event.code, 'reason:', event.reason);
          this.config.onDisconnected?.();
        };
      } catch (error) {
        console.error('‚ùå Error creating WebSocket:', error);
        reject(error);
      }
    });
  }

  /**
   * Disconnect from WebSocket
   */
  disconnect(): void {
    if (this.isRecording) {
      this.stopRecording();
    }

    if (this.ws) {
      this.ws.close();
      this.ws = null;
    }
  }

  /**
   * Configure Realtime API session
   */
  private configureSession(): void {
    const sessionConfig = {
      type: 'session.update',
      session: {
        modalities: ['text', 'audio'],
        instructions: this.currentInstructions,
        voice: 'echo',  // Male narrator voice
        input_audio_format: 'pcm16',
        output_audio_format: 'pcm16',
        turn_detection: null  // Disable server VAD - use manual tap-to-talk only
      }
    };

    console.log('üì§ Sending session config:', JSON.stringify(sessionConfig, null, 2));
    this.send(sessionConfig);
  }

  /**
   * Update session instructions (context for AI responses)
   */
  setInstructions(instructions: string): void {
    const normalized = instructions?.trim();
    if (!normalized) {
      return;
    }

    this.currentInstructions = normalized;

    if (this.ws && this.ws.readyState === WebSocket.OPEN) {
      this.send({
        type: 'session.update',
        session: {
          instructions: this.currentInstructions,
          turn_detection: null  // Ensure VAD stays disabled when updating instructions
        },
      });
      this.flushPendingContext();
    }
  }

  /**
   * Queue additional system context (e.g. paper content) to prime the conversation
   */
  setContext(context: string): void {
    const normalized = context?.trim();
    if (!normalized) {
      return;
    }

    this.pendingContext = normalized;
    this.flushPendingContext();
  }

  private flushPendingContext(): void {
    if (!this.pendingContext || !this.ws || this.ws.readyState !== WebSocket.OPEN) {
      return;
    }

    const excerpt = `${this.pendingContext.slice(0, 120)}${this.pendingContext.length > 120 ? '‚Ä¶' : ''}`;
    console.log('üì§ Sending conversation context snippet:', excerpt);

    this.send({
      type: 'conversation.item.create',
      item: {
        type: 'message',
        role: 'system',
        content: [
          {
            type: 'input_text',
            text: this.pendingContext,
          },
        ],
      },
    });

    this.pendingContext = null;
  }

  /**
   * Start recording audio from microphone
   */
  async startRecording(): Promise<void> {
    if (this.isRecording) {
      console.warn('‚ö†Ô∏è Already recording');
      return;
    }

    try {
      console.log('üé§ Requesting microphone access...');

      // Request microphone access
      this.mediaStream = await navigator.mediaDevices.getUserMedia({
        audio: {
          channelCount: 1,
          sampleRate: 24000,
          echoCancellation: true,
          noiseSuppression: true,
        }
      });

      console.log('‚úÖ Microphone access granted');

      // Create audio context
      this.audioContext = new (window.AudioContext || (window as any).webkitAudioContext)({
        sampleRate: 24000
      });

      console.log('‚úÖ Audio context created, sample rate:', this.audioContext.sampleRate);

      const source = this.audioContext.createMediaStreamSource(this.mediaStream);
      this.sourceNode = source;

      // Create script processor for audio data
      const processor = this.audioContext.createScriptProcessor(4096, 1, 1);
      this.processorNode = processor;
      let chunkCount = 0;

      processor.onaudioprocess = (e) => {
        if (!this.isRecording) return;

        const inputData = e.inputBuffer.getChannelData(0);
        const pcm16 = this.floatTo16BitPCM(inputData);
        const base64Audio = this.arrayBufferToBase64(pcm16);

        chunkCount++;
        if (chunkCount % 10 === 0) {
          console.log(`üìä Sent ${chunkCount} audio chunks`);
        }

        // Send audio chunk to server
        this.send({
          type: 'input_audio_buffer.append',
          audio: base64Audio
        });

        // Track that we've captured speech
        this.hasCapturedSpeech = true;
      };

      source.connect(processor);
      processor.connect(this.audioContext.destination);

      this.isRecording = true;
      this.hasCapturedSpeech = false;  // Reset speech tracking
      console.log('üé§ Started recording - speak now!');
    } catch (error) {
      console.error('‚ùå Failed to start recording:', error);
      this.config.onError?.(error as Error);
      throw error;
    }
  }

  /**
   * Stop recording audio
   */
  stopRecording(): void {
    if (!this.isRecording) {
      console.warn('‚ö†Ô∏è Not currently recording');
      return;
    }

    console.log('üõë Stopping recording...');

    // IMMEDIATELY stop recording flag to prevent any more chunks
    this.isRecording = false;
    const capturedSpeech = this.hasCapturedSpeech;

    // IMMEDIATELY disconnect audio nodes to stop hardware stream
    if (this.processorNode) {
      try {
        this.processorNode.disconnect();
      } catch (error) {
        console.warn('‚ö†Ô∏è Error disconnecting processor node:', error);
      }
      this.processorNode.onaudioprocess = null;
      this.processorNode = null;
    }

    if (this.sourceNode) {
      try {
        this.sourceNode.disconnect();
      } catch (error) {
        console.warn('‚ö†Ô∏è Error disconnecting source node:', error);
      }
      this.sourceNode = null;
    }

    // IMMEDIATELY stop media stream
    if (this.mediaStream) {
      this.mediaStream.getTracks().forEach(track => track.stop());
      this.mediaStream = null;
    }

    // Close audio context
    if (this.audioContext) {
      void this.audioContext.close().catch((error) => {
        console.warn('‚ö†Ô∏è Error closing AudioContext:', error);
      });
      this.audioContext = null;
    }

    // NOW send API calls after hardware is fully stopped - ONLY if we captured speech
    if (capturedSpeech) {
      console.log('üì§ Committing audio buffer to trigger AI response...');
      this.send({ type: 'input_audio_buffer.commit' });

      console.log('üì§ Requesting response from AI...');
      this.send({
        type: 'response.create',
        response: {
          modalities: ['text', 'audio'],
          voice: 'echo',  // Male narrator voice
          output_audio_format: 'pcm16'
        }
      });

      console.log('‚úÖ Stopped recording and requested AI response');
    } else {
      console.warn('‚ö†Ô∏è No audio captured, skipping commit and response request');
    }
  }

  /**
   * Send a message to the WebSocket server
   */
  private send(data: any): void {
    if (this.ws && this.ws.readyState === WebSocket.OPEN) {
      // Log all messages except audio data (too verbose)
      if (data.type !== 'input_audio_buffer.append') {
        if (data.type === 'conversation.item.create') {
          const textContent = data?.item?.content?.[0]?.text as string | undefined;
          const preview = textContent
            ? `${textContent.slice(0, 80)}${textContent.length > 80 ? '‚Ä¶' : ''}`
            : '';
          console.log('üì§ Sending conversation item:', preview);
        } else {
          console.log('üì§ Sending:', data.type, data);
        }
      }
      this.ws.send(JSON.stringify(data));
    } else {
      console.warn('‚ö†Ô∏è Cannot send, WebSocket not open. State:', this.ws?.readyState);
    }
  }

  /**
   * Handle incoming WebSocket messages
   */
  private handleMessage(data: string): void {
    try {
      const message = JSON.parse(data);
      console.log('üì• Received message type:', message.type);

      switch (message.type) {
        case 'response.audio.delta':
        case 'response.output_audio.delta':
          console.log('üîä Received audio delta, size:', message.delta?.length || 0);
          if (message.delta) {
            const audioData = this.base64ToArrayBuffer(message.delta);
            console.log('üîä Decoded audio data, bytes:', audioData.byteLength);
            this.config.onAudioDelta?.(audioData);
          }
          break;

        case 'response.audio_transcript.delta':
        case 'response.output_audio_transcript.delta':
        case 'response.text.delta':
          console.log('üìù Received transcript delta:', message.delta);
          if (message.delta) {
            this.config.onTranscriptDelta?.(message.delta);
          }
          break;

        case 'response.audio_transcript.done':
        case 'response.output_audio_transcript.done':
          console.log('‚úÖ Transcript complete:', message.transcript);
          break;

        case 'response.audio.done':
        case 'response.output_audio.done':
          console.log('‚úÖ Audio complete');
          break;

        case 'response.done':
          console.log('‚úÖ Response complete');
          this.config.onResponseFinished?.();
          break;

        case 'conversation.item.created':
          console.log('üí¨ Conversation item created:', message.item?.type);
          break;

        case 'response.created':
          console.log('üé¨ Response started');
          this.config.onResponseStarted?.();
          break;

        case 'input_audio_buffer.committed':
          console.log('‚úÖ Audio buffer committed');
          this.config.onAudioCommitted?.();
          break;

        case 'input_audio_buffer.speech_started':
          console.log('üó£Ô∏è Speech detected');
          this.config.onSpeechStart?.();
          break;

        case 'input_audio_buffer.speech_stopped':
          console.log('ü§ê Speech stopped');
          this.config.onSpeechEnd?.();
          break;

        case 'error':
          console.error('‚ùå API Error:', message);
          this.config.onError?.(new Error(message.error?.message || 'Unknown error'));
          break;

        case 'session.created':
        case 'session.updated':
          console.log('‚úÖ Session:', message.type);
          break;

        default:
          console.log('üì® Unhandled message type:', message.type, message);
      }
    } catch (error) {
      console.error('‚ùå Error parsing message:', error);
    }
  }

  /**
   * Convert Float32Array to 16-bit PCM
   */
  private floatTo16BitPCM(float32Array: Float32Array): ArrayBuffer {
    const buffer = new ArrayBuffer(float32Array.length * 2);
    const view = new DataView(buffer);

    for (let i = 0; i < float32Array.length; i++) {
      const s = Math.max(-1, Math.min(1, float32Array[i]));
      view.setInt16(i * 2, s < 0 ? s * 0x8000 : s * 0x7FFF, true);
    }

    return buffer;
  }

  /**
   * Convert ArrayBuffer to base64 string
   */
  private arrayBufferToBase64(buffer: ArrayBuffer): string {
    const bytes = new Uint8Array(buffer);
    let binary = '';
    for (let i = 0; i < bytes.byteLength; i++) {
      binary += String.fromCharCode(bytes[i]);
    }
    return btoa(binary);
  }

  /**
   * Convert base64 string to ArrayBuffer
   */
  private base64ToArrayBuffer(base64: string): ArrayBuffer {
    const binaryString = atob(base64);
    const len = binaryString.length;
    const bytes = new Uint8Array(len);
    for (let i = 0; i < len; i++) {
      bytes[i] = binaryString.charCodeAt(i);
    }
    return bytes.buffer;
  }

  /**
   * Create a response with custom instructions
   */
  createResponse(instructions: string): void {
    this.send({
      type: 'response.create',
      response: {
        modalities: ['text', 'audio'],
        instructions
      }
    });
  }

  /**
   * Cancel the current response
   */
  cancelResponse(): void {
    this.send({ type: 'response.cancel' });
  }

  /**
   * Check if connected
   */
  isConnected(): boolean {
    return this.ws !== null && this.ws.readyState === WebSocket.OPEN;
  }

  /**
   * Check if currently recording
   */
  isCurrentlyRecording(): boolean {
    return this.isRecording;
  }
}

/**
 * Audio Player for streaming PCM16 audio
 */
export class StreamingAudioPlayer {
  private audioContext: AudioContext | null = null;
  private audioQueue: AudioBuffer[] = [];
  private isPlaying = false;
  private currentSource: AudioBufferSourceNode | null = null;
  private nextPlayTime = 0;

  constructor() {}

  /**
   * Ensure audio context exists and is running (required for autoplay policies)
   */
  async prepare(): Promise<void> {
    await this.ensureAudioContext();
  }

  private async ensureAudioContext(): Promise<AudioContext> {
    if (!this.audioContext) {
      this.audioContext = new (window.AudioContext || (window as any).webkitAudioContext)({
        sampleRate: 24000
      });
    }

    if (this.audioContext.state === 'suspended') {
      try {
        await this.audioContext.resume();
        console.log('üéõÔ∏è AudioContext resumed');
      } catch (error) {
        console.error('‚ùå Failed to resume AudioContext:', error);
      }
    }

    return this.audioContext;
  }

  /**
   * Add audio chunk to the queue and play
   */
  async addChunk(pcm16Data: ArrayBuffer): Promise<void> {
    const audioContext = await this.ensureAudioContext();
    if (!audioContext) {
      console.error('‚ùå No audio context available');
      return;
    }

    try {
      console.log('üéµ Adding audio chunk, bytes:', pcm16Data.byteLength);

      // Convert PCM16 to AudioBuffer
      const audioBuffer = await this.pcm16ToAudioBuffer(audioContext, pcm16Data);
      console.log('üéµ Converted to AudioBuffer, duration:', audioBuffer.duration, 'seconds');

      this.audioQueue.push(audioBuffer);
      console.log('üéµ Audio queue size:', this.audioQueue.length);

      if (!this.isPlaying) {
        console.log('‚ñ∂Ô∏è Starting playback...');
        void this.playNext();
      }
    } catch (error) {
      console.error('‚ùå Error adding audio chunk:', error);
    }
  }

  /**
   * Play the next audio chunk in the queue
   */
  private async playNext(): Promise<void> {
    const audioContext = await this.ensureAudioContext();

    if (!audioContext || this.audioQueue.length === 0) {
      console.log('‚èπÔ∏è Playback stopped - queue empty');
      this.isPlaying = false;
      return;
    }

    this.isPlaying = true;
    const audioBuffer = this.audioQueue.shift()!;

    console.log('‚ñ∂Ô∏è Playing audio chunk, duration:', audioBuffer.duration, 'seconds');

    const source = audioContext.createBufferSource();
    source.buffer = audioBuffer;
    source.connect(audioContext.destination);

    // Schedule playback
    const startTime = Math.max(this.nextPlayTime, audioContext.currentTime);
    console.log('‚è∞ Scheduled to play at:', startTime, 'current time:', audioContext.currentTime);
    source.start(startTime);
    this.nextPlayTime = startTime + audioBuffer.duration;

    this.currentSource = source;

    // Play next chunk when this one finishes
    source.onended = () => {
      console.log('‚úÖ Audio chunk finished playing');
      this.currentSource = null;
      void this.playNext();
    };
  }

  /**
   * Convert PCM16 ArrayBuffer to AudioBuffer
   */
  private async pcm16ToAudioBuffer(audioContext: AudioContext, pcm16Data: ArrayBuffer): Promise<AudioBuffer> {
    const int16Array = new Int16Array(pcm16Data);
    const float32Array = new Float32Array(int16Array.length);

    // Convert Int16 to Float32 (-1.0 to 1.0)
    for (let i = 0; i < int16Array.length; i++) {
      float32Array[i] = int16Array[i] / 32768.0;
    }

    const audioBuffer = audioContext.createBuffer(1, float32Array.length, 24000);
    audioBuffer.getChannelData(0).set(float32Array);

    return audioBuffer;
  }

  /**
   * Stop playback and clear queue
   */
  stop(): void {
    if (this.currentSource) {
      this.currentSource.stop();
      this.currentSource = null;
    }

    this.audioQueue = [];
    this.isPlaying = false;
    this.nextPlayTime = 0;
  }

  /**
   * Close audio context
   */
  close(): void {
    this.stop();
    if (this.audioContext) {
      this.audioContext.close();
      this.audioContext = null;
    }
  }
}
